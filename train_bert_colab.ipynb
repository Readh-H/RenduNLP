{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdKkmoDjRQGdINe8aNlnPP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Readh-H/NLP/blob/main/train_bert_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFO6Luux07Zs"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets seqeval -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import CamembertTokenizerFast, CamembertForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
        "from datasets import Dataset\n",
        "from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "7ITA07Cs1NXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # SÃ©lectionnez les 3 fichiers : slot-filling.in, slot-filling.out, intentlabels\n"
      ],
      "metadata": {
        "id": "Xg_8xbjE1Rjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(filename):\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        return [line.strip().split() for line in f]\n",
        "\n",
        "sentences = read_file(\"slot-filling.in\")\n",
        "slots = read_file(\"slot-filling.out\")\n",
        "with open(\"intentlabels\", \"r\", encoding=\"utf-8\") as f:\n",
        "    intents = [line.strip() for line in f]\n",
        "\n",
        "assert len(sentences) == len(slots) == len(intents)\n",
        "\n",
        "data = [{\"tokens\": s, \"slots\": sl, \"intent\": i} for s, sl, i in zip(sentences, slots, intents)]\n",
        "dataset = Dataset.from_list(data).train_test_split(test_size=0.2)\n"
      ],
      "metadata": {
        "id": "Ja0Wt1EB1TMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = CamembertTokenizerFast.from_pretrained(\"camembert-base\")\n",
        "\n",
        "unique_slots = sorted({label for ex in dataset[\"train\"] for label in ex[\"slots\"]})\n",
        "slot2id = {label: idx for idx, label in enumerate(unique_slots)}\n",
        "id2slot = {v: k for k, v in slot2id.items()}\n",
        "\n",
        "intent2id = {label: idx for idx, label in enumerate(sorted(set(dataset[\"train\"][\"intent\"])))}\n",
        "id2intent = {v: k for k, v in intent2id.items()}\n"
      ],
      "metadata": {
        "id": "wcMhem6q14JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_example(example):\n",
        "    tokenized = tokenizer(example[\"tokens\"], is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=64)\n",
        "    word_ids = tokenized.word_ids()\n",
        "\n",
        "    labels = []\n",
        "    previous_word_idx = None\n",
        "    for word_idx in word_ids:\n",
        "        if word_idx is None:\n",
        "            labels.append(-100)\n",
        "        else:\n",
        "            labels.append(slot2id[example[\"slots\"][word_idx]])\n",
        "        previous_word_idx = word_idx\n",
        "\n",
        "    tokenized[\"labels\"] = labels\n",
        "    tokenized[\"intent_label\"] = intent2id[example[\"intent\"]]\n",
        "    return tokenized\n",
        "\n",
        "encoded_dataset = dataset.map(encode_example)\n"
      ],
      "metadata": {
        "id": "_QgXElvp18_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CamembertForTokenClassification.from_pretrained(\"camembert-base\", num_labels=len(slot2id))\n"
      ],
      "metadata": {
        "id": "03NrE98E1-kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    for pred, lab in zip(predictions, labels):\n",
        "        true_seq = []\n",
        "        pred_seq = []\n",
        "        for p, l in zip(pred, lab):\n",
        "            if l != -100:\n",
        "                true_seq.append(id2slot[l])\n",
        "                pred_seq.append(id2slot[p])\n",
        "        true_labels.append(true_seq)\n",
        "        pred_labels.append(pred_seq)\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision_score(true_labels, pred_labels),\n",
        "        \"recall\": recall_score(true_labels, pred_labels),\n",
        "        \"f1\": f1_score(true_labels, pred_labels),\n",
        "        \"accuracy\": accuracy_score(true_labels, pred_labels),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "5dtNgVY12s_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"./camembert-nlu\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "ZX1UWFSB2v-_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}