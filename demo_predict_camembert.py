import torch
from transformers import CamembertTokenizerFast
import json
from models.model_camembert import CamembertNLU
import sys

# === Chargement des mappings ===
try:
    with open("data/processed/train_camembert.json", encoding="utf-8") as f:
        raw_data = json.load(f)

    all_slots = sorted({label for sample in raw_data for label in sample["slots"]})
    all_intents = sorted({sample["intent"] for sample in raw_data})

    slot2idx = {label: idx for idx, label in enumerate(all_slots)}
    intent2idx = {label: idx for idx, label in enumerate(all_intents)}
    idx2slot = {v: k for k, v in slot2idx.items()}
    idx2intent = {v: k for k, v in intent2idx.items()}
except Exception as e:
    print(f"‚ùå Erreur lors du chargement des mappings: {e}")
    print("‚ö†Ô∏è V√©rifiez que le fichier train_camembert.json existe dans le dossier data/processed/")
    sys.exit(1)

# === Chargement du mod√®le ===
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CamembertNLU(slot_size=len(slot2idx), intent_size=len(intent2idx))

# Essayer diff√©rentes m√©thodes de chargement
try:
    # 1. Essayer safetensors (format recommand√©)
    try:
        from safetensors.torch import load_file
        state_dict = load_file("models/camembert_final/model.safetensors")
        model.load_state_dict(state_dict)
        print("‚úÖ Mod√®le charg√© depuis safetensors avec succ√®s!")
    except Exception as e:
        print(f"‚ÑπÔ∏è Impossible de charger depuis safetensors: {e}")
        raise Exception("Essayer m√©thode suivante")
        
except Exception:
    # 2. Essayer pytorch_model.bin
    try:
        model.load_state_dict(torch.load("models/camembert_final/pytorch_model.bin", map_location=device))
        print("‚úÖ Mod√®le charg√© depuis pytorch_model.bin avec succ√®s!")
    except Exception as e:
        print(f"‚ÑπÔ∏è Impossible de charger depuis pytorch_model.bin: {e}")
        
        # 3. Essayer from_pretrained
        try:
            from transformers import CamembertForTokenClassification
            print("Chargement du mod√®le avec from_pretrained...")
            encoder = CamembertForTokenClassification.from_pretrained("models/camembert_final/").roberta
            model.encoder = encoder
            print("‚úÖ Mod√®le charg√© avec from_pretrained avec succ√®s!")
        except Exception as e2:
            print(f"‚ùå Toutes les m√©thodes de chargement ont √©chou√©: {e2}")
            print("‚ö†Ô∏è Veuillez r√©entra√Æner le mod√®le et sauvegarder explicitement les poids.")
            sys.exit(1)

model.to(device)
model.eval()

# === Tokenizer ===
tokenizer = CamembertTokenizerFast.from_pretrained("camembert-base")

# === Fonction de pr√©diction ===
def predict(model, sentence):
    tokens = sentence.split()
    
    # Tokenisation avec is_split_into_words=True
    # IMPORTANT: Garder l'objet encod√© original pour word_ids
    encoded = tokenizer(tokens, is_split_into_words=True, return_tensors="pt", padding=True)
    
    # R√©cup√©rer les word_ids avant de convertir en tenseurs pour le GPU <-- echec
    word_ids = encoded.word_ids(batch_index=0)
    
    # Pr√©parer les entr√©es pour le mod√®le
    inputs = {k: v.to(device) for k, v in encoded.items()}
    
    with torch.no_grad():
        outputs = model(**inputs)
    
    slot_logits, intent_logits = outputs
    
    # Intent
    intent_pred = torch.argmax(intent_logits, dim=1).item()
    intent = idx2intent[intent_pred]
    
    # Slots
    slot_preds = torch.argmax(slot_logits, dim=2)[0].cpu().numpy()
    
    # Aligner les pr√©dictions avec les tokens
    previous_word_idx = None
    slot_labels = []
    for i, word_idx in enumerate(word_ids):
        if word_idx is None or word_idx == previous_word_idx:
            continue
        slot_labels.append(idx2slot[slot_preds[i]])
        previous_word_idx = word_idx
    
    print("\nüß™ R√©sultat de la pr√©diction :")
    print("Phrase :", sentence)
    print("Intent :", intent)
    print("Slots  :")
    for token, slot in zip(tokens, slot_labels):
        print(f"  {token:<15} ‚Üí {slot}")

# === Lancement interactif ===
if __name__ == "__main__":
    print("ü§ñ D√©mo du mod√®le CamemBERT NLU")
    print("üí° Tapez une phrase pour obtenir l'intention et les slots d√©tect√©s")
    print(f"üìä Mod√®le charg√© sur {device}")
    
    while True:
        try:
            sentence = input("\nTapez une phrase (ou 'exit' pour quitter) : ")
            if sentence.lower() in ["exit", "quit", "q", "stop", "fin", "stopper"]:
                break
            predict(model, sentence)
        except Exception as e:
            print(f"Une erreur est survenue : {e}")
            print(f"D√©tails : {type(e).__name__}")
