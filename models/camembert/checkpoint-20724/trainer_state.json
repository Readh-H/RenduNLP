{
  "best_global_step": 20724,
  "best_metric": 0.6379383185466836,
  "best_model_checkpoint": "models/camembert\\checkpoint-20724",
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 20724,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0019301293186643506,
      "grad_norm": 7.518535137176514,
      "learning_rate": 1.999131441806601e-05,
      "loss": 8.6475,
      "step": 10
    },
    {
      "epoch": 0.003860258637328701,
      "grad_norm": 6.546566963195801,
      "learning_rate": 1.998166377147269e-05,
      "loss": 8.4067,
      "step": 20
    },
    {
      "epoch": 0.005790387955993051,
      "grad_norm": 6.92575740814209,
      "learning_rate": 1.997201312487937e-05,
      "loss": 8.0665,
      "step": 30
    },
    {
      "epoch": 0.007720517274657402,
      "grad_norm": 9.000710487365723,
      "learning_rate": 1.9962362478286047e-05,
      "loss": 7.7167,
      "step": 40
    },
    {
      "epoch": 0.009650646593321753,
      "grad_norm": 6.370423316955566,
      "learning_rate": 1.9952711831692726e-05,
      "loss": 7.476,
      "step": 50
    },
    {
      "epoch": 0.011580775911986103,
      "grad_norm": 6.9139299392700195,
      "learning_rate": 1.9943061185099405e-05,
      "loss": 7.3893,
      "step": 60
    },
    {
      "epoch": 0.013510905230650454,
      "grad_norm": 8.953699111938477,
      "learning_rate": 1.993341053850608e-05,
      "loss": 7.1207,
      "step": 70
    },
    {
      "epoch": 0.015441034549314805,
      "grad_norm": 7.634289741516113,
      "learning_rate": 1.992375989191276e-05,
      "loss": 7.0192,
      "step": 80
    },
    {
      "epoch": 0.017371163867979156,
      "grad_norm": 6.379313945770264,
      "learning_rate": 1.9914109245319438e-05,
      "loss": 7.0731,
      "step": 90
    },
    {
      "epoch": 0.019301293186643507,
      "grad_norm": 6.03101110458374,
      "learning_rate": 1.9904458598726114e-05,
      "loss": 6.8584,
      "step": 100
    },
    {
      "epoch": 0.021231422505307854,
      "grad_norm": 6.21621036529541,
      "learning_rate": 1.9894807952132796e-05,
      "loss": 6.9324,
      "step": 110
    },
    {
      "epoch": 0.023161551823972205,
      "grad_norm": 5.588554859161377,
      "learning_rate": 1.9885157305539475e-05,
      "loss": 6.9109,
      "step": 120
    },
    {
      "epoch": 0.025091681142636556,
      "grad_norm": 5.517746448516846,
      "learning_rate": 1.987550665894615e-05,
      "loss": 6.7862,
      "step": 130
    },
    {
      "epoch": 0.027021810461300907,
      "grad_norm": 4.764743328094482,
      "learning_rate": 1.986585601235283e-05,
      "loss": 6.8326,
      "step": 140
    },
    {
      "epoch": 0.02895193977996526,
      "grad_norm": 6.521220684051514,
      "learning_rate": 1.9856205365759508e-05,
      "loss": 6.8508,
      "step": 150
    },
    {
      "epoch": 0.03088206909862961,
      "grad_norm": 7.527667045593262,
      "learning_rate": 1.9846554719166187e-05,
      "loss": 6.6598,
      "step": 160
    },
    {
      "epoch": 0.03281219841729396,
      "grad_norm": 7.6186981201171875,
      "learning_rate": 1.9836904072572865e-05,
      "loss": 6.7593,
      "step": 170
    },
    {
      "epoch": 0.03474232773595831,
      "grad_norm": 7.116814613342285,
      "learning_rate": 1.982725342597954e-05,
      "loss": 6.4273,
      "step": 180
    },
    {
      "epoch": 0.03667245705462266,
      "grad_norm": 5.3662590980529785,
      "learning_rate": 1.981760277938622e-05,
      "loss": 6.7494,
      "step": 190
    },
    {
      "epoch": 0.038602586373287014,
      "grad_norm": 7.409459590911865,
      "learning_rate": 1.98079521327929e-05,
      "loss": 6.6839,
      "step": 200
    },
    {
      "epoch": 0.04053271569195136,
      "grad_norm": 7.038153171539307,
      "learning_rate": 1.9798301486199577e-05,
      "loss": 6.6341,
      "step": 210
    },
    {
      "epoch": 0.04246284501061571,
      "grad_norm": 7.710297584533691,
      "learning_rate": 1.9788650839606256e-05,
      "loss": 6.5537,
      "step": 220
    },
    {
      "epoch": 0.04439297432928006,
      "grad_norm": 7.332005023956299,
      "learning_rate": 1.9779000193012935e-05,
      "loss": 6.4844,
      "step": 230
    },
    {
      "epoch": 0.04632310364794441,
      "grad_norm": 7.194823265075684,
      "learning_rate": 1.976934954641961e-05,
      "loss": 6.5442,
      "step": 240
    },
    {
      "epoch": 0.048253232966608765,
      "grad_norm": 7.366460800170898,
      "learning_rate": 1.975969889982629e-05,
      "loss": 6.2968,
      "step": 250
    },
    {
      "epoch": 0.05018336228527311,
      "grad_norm": 5.508444786071777,
      "learning_rate": 1.9750048253232968e-05,
      "loss": 6.3492,
      "step": 260
    },
    {
      "epoch": 0.05211349160393746,
      "grad_norm": 5.800264835357666,
      "learning_rate": 1.9740397606639647e-05,
      "loss": 6.4209,
      "step": 270
    },
    {
      "epoch": 0.054043620922601815,
      "grad_norm": 5.725395679473877,
      "learning_rate": 1.9730746960046326e-05,
      "loss": 6.4509,
      "step": 280
    },
    {
      "epoch": 0.05597375024126616,
      "grad_norm": 6.545402526855469,
      "learning_rate": 1.9721096313453005e-05,
      "loss": 6.6348,
      "step": 290
    },
    {
      "epoch": 0.05790387955993052,
      "grad_norm": 5.906174182891846,
      "learning_rate": 1.971144566685968e-05,
      "loss": 6.129,
      "step": 300
    },
    {
      "epoch": 0.059834008878594865,
      "grad_norm": 5.990317344665527,
      "learning_rate": 1.970179502026636e-05,
      "loss": 6.3002,
      "step": 310
    },
    {
      "epoch": 0.06176413819725922,
      "grad_norm": 7.422779560089111,
      "learning_rate": 1.9692144373673038e-05,
      "loss": 6.1495,
      "step": 320
    },
    {
      "epoch": 0.06369426751592357,
      "grad_norm": 6.3987298011779785,
      "learning_rate": 1.9682493727079717e-05,
      "loss": 6.3652,
      "step": 330
    },
    {
      "epoch": 0.06562439683458791,
      "grad_norm": 7.092891216278076,
      "learning_rate": 1.9672843080486395e-05,
      "loss": 6.1564,
      "step": 340
    },
    {
      "epoch": 0.06755452615325226,
      "grad_norm": 5.971985816955566,
      "learning_rate": 1.966319243389307e-05,
      "loss": 6.2326,
      "step": 350
    },
    {
      "epoch": 0.06948465547191662,
      "grad_norm": 7.07247257232666,
      "learning_rate": 1.965354178729975e-05,
      "loss": 6.3538,
      "step": 360
    },
    {
      "epoch": 0.07141478479058097,
      "grad_norm": 6.777222633361816,
      "learning_rate": 1.964389114070643e-05,
      "loss": 5.9659,
      "step": 370
    },
    {
      "epoch": 0.07334491410924532,
      "grad_norm": 7.574714660644531,
      "learning_rate": 1.9634240494113107e-05,
      "loss": 5.9631,
      "step": 380
    },
    {
      "epoch": 0.07527504342790967,
      "grad_norm": 7.125153541564941,
      "learning_rate": 1.9624589847519786e-05,
      "loss": 6.5203,
      "step": 390
    },
    {
      "epoch": 0.07720517274657403,
      "grad_norm": 7.5586066246032715,
      "learning_rate": 1.9614939200926465e-05,
      "loss": 5.9242,
      "step": 400
    },
    {
      "epoch": 0.07913530206523837,
      "grad_norm": 6.09491491317749,
      "learning_rate": 1.960528855433314e-05,
      "loss": 6.1868,
      "step": 410
    },
    {
      "epoch": 0.08106543138390272,
      "grad_norm": 6.865261077880859,
      "learning_rate": 1.959563790773982e-05,
      "loss": 6.1337,
      "step": 420
    },
    {
      "epoch": 0.08299556070256707,
      "grad_norm": 6.937936305999756,
      "learning_rate": 1.9585987261146498e-05,
      "loss": 5.9606,
      "step": 430
    },
    {
      "epoch": 0.08492569002123142,
      "grad_norm": 6.523399829864502,
      "learning_rate": 1.9576336614553177e-05,
      "loss": 6.1374,
      "step": 440
    },
    {
      "epoch": 0.08685581933989578,
      "grad_norm": 7.5763936042785645,
      "learning_rate": 1.9566685967959856e-05,
      "loss": 6.0991,
      "step": 450
    },
    {
      "epoch": 0.08878594865856013,
      "grad_norm": 6.747790336608887,
      "learning_rate": 1.9557035321366535e-05,
      "loss": 5.9686,
      "step": 460
    },
    {
      "epoch": 0.09071607797722447,
      "grad_norm": 6.129618167877197,
      "learning_rate": 1.954738467477321e-05,
      "loss": 5.7346,
      "step": 470
    },
    {
      "epoch": 0.09264620729588882,
      "grad_norm": 6.6857075691223145,
      "learning_rate": 1.953773402817989e-05,
      "loss": 5.9706,
      "step": 480
    },
    {
      "epoch": 0.09457633661455317,
      "grad_norm": 7.060307025909424,
      "learning_rate": 1.9528083381586568e-05,
      "loss": 5.8804,
      "step": 490
    },
    {
      "epoch": 0.09650646593321753,
      "grad_norm": 7.01095724105835,
      "learning_rate": 1.9518432734993247e-05,
      "loss": 5.9007,
      "step": 500
    },
    {
      "epoch": 0.09843659525188188,
      "grad_norm": 6.054405689239502,
      "learning_rate": 1.9508782088399925e-05,
      "loss": 6.0432,
      "step": 510
    },
    {
      "epoch": 0.10036672457054623,
      "grad_norm": 6.300296783447266,
      "learning_rate": 1.94991314418066e-05,
      "loss": 5.715,
      "step": 520
    },
    {
      "epoch": 0.10229685388921057,
      "grad_norm": 6.655271530151367,
      "learning_rate": 1.948948079521328e-05,
      "loss": 5.7488,
      "step": 530
    },
    {
      "epoch": 0.10422698320787492,
      "grad_norm": 6.283161640167236,
      "learning_rate": 1.947983014861996e-05,
      "loss": 5.5467,
      "step": 540
    },
    {
      "epoch": 0.10615711252653928,
      "grad_norm": 6.6152472496032715,
      "learning_rate": 1.9470179502026637e-05,
      "loss": 5.8977,
      "step": 550
    },
    {
      "epoch": 0.10808724184520363,
      "grad_norm": 7.001309871673584,
      "learning_rate": 1.9460528855433316e-05,
      "loss": 5.801,
      "step": 560
    },
    {
      "epoch": 0.11001737116386798,
      "grad_norm": 6.9102044105529785,
      "learning_rate": 1.9450878208839995e-05,
      "loss": 5.5789,
      "step": 570
    },
    {
      "epoch": 0.11194750048253232,
      "grad_norm": 6.582967281341553,
      "learning_rate": 1.944122756224667e-05,
      "loss": 5.6269,
      "step": 580
    },
    {
      "epoch": 0.11387762980119669,
      "grad_norm": 6.651410102844238,
      "learning_rate": 1.943157691565335e-05,
      "loss": 5.2945,
      "step": 590
    },
    {
      "epoch": 0.11580775911986103,
      "grad_norm": 6.399775981903076,
      "learning_rate": 1.9421926269060028e-05,
      "loss": 5.7786,
      "step": 600
    },
    {
      "epoch": 0.11773788843852538,
      "grad_norm": 7.359265327453613,
      "learning_rate": 1.9412275622466707e-05,
      "loss": 5.7339,
      "step": 610
    },
    {
      "epoch": 0.11966801775718973,
      "grad_norm": 7.114992141723633,
      "learning_rate": 1.9402624975873386e-05,
      "loss": 5.314,
      "step": 620
    },
    {
      "epoch": 0.12159814707585408,
      "grad_norm": 6.591903209686279,
      "learning_rate": 1.9392974329280065e-05,
      "loss": 5.8873,
      "step": 630
    },
    {
      "epoch": 0.12352827639451844,
      "grad_norm": 6.976163864135742,
      "learning_rate": 1.938332368268674e-05,
      "loss": 5.6807,
      "step": 640
    },
    {
      "epoch": 0.12545840571318279,
      "grad_norm": 7.623526573181152,
      "learning_rate": 1.937367303609342e-05,
      "loss": 5.5254,
      "step": 650
    },
    {
      "epoch": 0.12738853503184713,
      "grad_norm": 6.750922679901123,
      "learning_rate": 1.9364022389500098e-05,
      "loss": 5.1901,
      "step": 660
    },
    {
      "epoch": 0.12931866435051148,
      "grad_norm": 6.812376499176025,
      "learning_rate": 1.9354371742906777e-05,
      "loss": 5.7353,
      "step": 670
    },
    {
      "epoch": 0.13124879366917583,
      "grad_norm": 6.888833045959473,
      "learning_rate": 1.9344721096313455e-05,
      "loss": 5.362,
      "step": 680
    },
    {
      "epoch": 0.13317892298784018,
      "grad_norm": 6.5015950202941895,
      "learning_rate": 1.933507044972013e-05,
      "loss": 5.6534,
      "step": 690
    },
    {
      "epoch": 0.13510905230650452,
      "grad_norm": 6.404411315917969,
      "learning_rate": 1.932541980312681e-05,
      "loss": 5.654,
      "step": 700
    },
    {
      "epoch": 0.1370391816251689,
      "grad_norm": 7.112742900848389,
      "learning_rate": 1.9315769156533492e-05,
      "loss": 5.4811,
      "step": 710
    },
    {
      "epoch": 0.13896931094383325,
      "grad_norm": 7.2324628829956055,
      "learning_rate": 1.9306118509940167e-05,
      "loss": 5.6282,
      "step": 720
    },
    {
      "epoch": 0.1408994402624976,
      "grad_norm": 6.951450824737549,
      "learning_rate": 1.9296467863346846e-05,
      "loss": 5.5273,
      "step": 730
    },
    {
      "epoch": 0.14282956958116194,
      "grad_norm": 6.531373977661133,
      "learning_rate": 1.9286817216753525e-05,
      "loss": 5.6161,
      "step": 740
    },
    {
      "epoch": 0.1447596988998263,
      "grad_norm": 6.88507080078125,
      "learning_rate": 1.92771665701602e-05,
      "loss": 5.4013,
      "step": 750
    },
    {
      "epoch": 0.14668982821849064,
      "grad_norm": 5.914280414581299,
      "learning_rate": 1.926751592356688e-05,
      "loss": 5.3011,
      "step": 760
    },
    {
      "epoch": 0.14861995753715498,
      "grad_norm": 5.880892753601074,
      "learning_rate": 1.9257865276973558e-05,
      "loss": 5.541,
      "step": 770
    },
    {
      "epoch": 0.15055008685581933,
      "grad_norm": 9.116085052490234,
      "learning_rate": 1.9248214630380237e-05,
      "loss": 5.0234,
      "step": 780
    },
    {
      "epoch": 0.15248021617448368,
      "grad_norm": 6.923271179199219,
      "learning_rate": 1.9238563983786916e-05,
      "loss": 5.2519,
      "step": 790
    },
    {
      "epoch": 0.15441034549314805,
      "grad_norm": 9.510721206665039,
      "learning_rate": 1.9228913337193595e-05,
      "loss": 5.516,
      "step": 800
    },
    {
      "epoch": 0.1563404748118124,
      "grad_norm": 6.6395063400268555,
      "learning_rate": 1.921926269060027e-05,
      "loss": 5.7905,
      "step": 810
    },
    {
      "epoch": 0.15827060413047675,
      "grad_norm": 6.295666217803955,
      "learning_rate": 1.9209612044006952e-05,
      "loss": 5.3214,
      "step": 820
    },
    {
      "epoch": 0.1602007334491411,
      "grad_norm": 6.657223224639893,
      "learning_rate": 1.9199961397413628e-05,
      "loss": 5.2354,
      "step": 830
    },
    {
      "epoch": 0.16213086276780544,
      "grad_norm": 7.10333776473999,
      "learning_rate": 1.9190310750820307e-05,
      "loss": 5.5876,
      "step": 840
    },
    {
      "epoch": 0.1640609920864698,
      "grad_norm": 6.172407627105713,
      "learning_rate": 1.9180660104226985e-05,
      "loss": 5.2716,
      "step": 850
    },
    {
      "epoch": 0.16599112140513414,
      "grad_norm": 6.81936502456665,
      "learning_rate": 1.917100945763366e-05,
      "loss": 5.3333,
      "step": 860
    },
    {
      "epoch": 0.1679212507237985,
      "grad_norm": 6.958003997802734,
      "learning_rate": 1.916135881104034e-05,
      "loss": 5.2147,
      "step": 870
    },
    {
      "epoch": 0.16985138004246284,
      "grad_norm": 6.611533164978027,
      "learning_rate": 1.9151708164447022e-05,
      "loss": 5.2679,
      "step": 880
    },
    {
      "epoch": 0.17178150936112718,
      "grad_norm": 6.918147087097168,
      "learning_rate": 1.9142057517853697e-05,
      "loss": 5.2123,
      "step": 890
    },
    {
      "epoch": 0.17371163867979156,
      "grad_norm": 6.142858028411865,
      "learning_rate": 1.9132406871260376e-05,
      "loss": 5.5161,
      "step": 900
    },
    {
      "epoch": 0.1756417679984559,
      "grad_norm": 6.380001544952393,
      "learning_rate": 1.9122756224667055e-05,
      "loss": 5.4812,
      "step": 910
    },
    {
      "epoch": 0.17757189731712025,
      "grad_norm": 8.679652214050293,
      "learning_rate": 1.911310557807373e-05,
      "loss": 5.0152,
      "step": 920
    },
    {
      "epoch": 0.1795020266357846,
      "grad_norm": 7.090809345245361,
      "learning_rate": 1.9103454931480413e-05,
      "loss": 5.5699,
      "step": 930
    },
    {
      "epoch": 0.18143215595444895,
      "grad_norm": 7.020040035247803,
      "learning_rate": 1.9093804284887088e-05,
      "loss": 5.0741,
      "step": 940
    },
    {
      "epoch": 0.1833622852731133,
      "grad_norm": 6.59566593170166,
      "learning_rate": 1.9084153638293767e-05,
      "loss": 5.1929,
      "step": 950
    },
    {
      "epoch": 0.18529241459177764,
      "grad_norm": 7.1251606941223145,
      "learning_rate": 1.9074502991700446e-05,
      "loss": 5.2144,
      "step": 960
    },
    {
      "epoch": 0.187222543910442,
      "grad_norm": 7.51411771774292,
      "learning_rate": 1.9064852345107125e-05,
      "loss": 5.1835,
      "step": 970
    },
    {
      "epoch": 0.18915267322910634,
      "grad_norm": 6.821052551269531,
      "learning_rate": 1.90552016985138e-05,
      "loss": 5.2508,
      "step": 980
    },
    {
      "epoch": 0.1910828025477707,
      "grad_norm": 6.766191005706787,
      "learning_rate": 1.9045551051920482e-05,
      "loss": 5.45,
      "step": 990
    },
    {
      "epoch": 0.19301293186643506,
      "grad_norm": 7.344473361968994,
      "learning_rate": 1.9035900405327158e-05,
      "loss": 5.2945,
      "step": 1000
    },
    {
      "epoch": 0.1949430611850994,
      "grad_norm": 6.995244979858398,
      "learning_rate": 1.9026249758733837e-05,
      "loss": 4.777,
      "step": 1010
    },
    {
      "epoch": 0.19687319050376376,
      "grad_norm": 7.153800010681152,
      "learning_rate": 1.9016599112140515e-05,
      "loss": 5.265,
      "step": 1020
    },
    {
      "epoch": 0.1988033198224281,
      "grad_norm": 7.09705114364624,
      "learning_rate": 1.900694846554719e-05,
      "loss": 5.4332,
      "step": 1030
    },
    {
      "epoch": 0.20073344914109245,
      "grad_norm": 6.812826633453369,
      "learning_rate": 1.899729781895387e-05,
      "loss": 5.5461,
      "step": 1040
    },
    {
      "epoch": 0.2026635784597568,
      "grad_norm": 6.269583702087402,
      "learning_rate": 1.8987647172360552e-05,
      "loss": 5.2225,
      "step": 1050
    },
    {
      "epoch": 0.20459370777842115,
      "grad_norm": 6.821446418762207,
      "learning_rate": 1.8977996525767227e-05,
      "loss": 5.2681,
      "step": 1060
    },
    {
      "epoch": 0.2065238370970855,
      "grad_norm": 6.7503533363342285,
      "learning_rate": 1.8968345879173906e-05,
      "loss": 5.0575,
      "step": 1070
    },
    {
      "epoch": 0.20845396641574984,
      "grad_norm": 7.394402503967285,
      "learning_rate": 1.8958695232580585e-05,
      "loss": 5.1755,
      "step": 1080
    },
    {
      "epoch": 0.21038409573441422,
      "grad_norm": 6.566605091094971,
      "learning_rate": 1.894904458598726e-05,
      "loss": 5.1652,
      "step": 1090
    },
    {
      "epoch": 0.21231422505307856,
      "grad_norm": 11.819528579711914,
      "learning_rate": 1.8939393939393943e-05,
      "loss": 5.4003,
      "step": 1100
    },
    {
      "epoch": 0.2142443543717429,
      "grad_norm": 6.671020030975342,
      "learning_rate": 1.8929743292800618e-05,
      "loss": 5.6528,
      "step": 1110
    },
    {
      "epoch": 0.21617448369040726,
      "grad_norm": 6.229356288909912,
      "learning_rate": 1.8920092646207297e-05,
      "loss": 5.2723,
      "step": 1120
    },
    {
      "epoch": 0.2181046130090716,
      "grad_norm": 7.266167163848877,
      "learning_rate": 1.8910441999613976e-05,
      "loss": 4.799,
      "step": 1130
    },
    {
      "epoch": 0.22003474232773595,
      "grad_norm": 7.086863994598389,
      "learning_rate": 1.8900791353020655e-05,
      "loss": 4.9505,
      "step": 1140
    },
    {
      "epoch": 0.2219648716464003,
      "grad_norm": 6.5901079177856445,
      "learning_rate": 1.889114070642733e-05,
      "loss": 5.0447,
      "step": 1150
    },
    {
      "epoch": 0.22389500096506465,
      "grad_norm": 11.356246948242188,
      "learning_rate": 1.8881490059834012e-05,
      "loss": 5.2006,
      "step": 1160
    },
    {
      "epoch": 0.225825130283729,
      "grad_norm": 11.532161712646484,
      "learning_rate": 1.8871839413240688e-05,
      "loss": 5.0361,
      "step": 1170
    },
    {
      "epoch": 0.22775525960239337,
      "grad_norm": 8.33015251159668,
      "learning_rate": 1.8862188766647367e-05,
      "loss": 5.0401,
      "step": 1180
    },
    {
      "epoch": 0.22968538892105772,
      "grad_norm": 6.704700469970703,
      "learning_rate": 1.8852538120054045e-05,
      "loss": 4.3921,
      "step": 1190
    },
    {
      "epoch": 0.23161551823972207,
      "grad_norm": 7.000170707702637,
      "learning_rate": 1.884288747346072e-05,
      "loss": 4.866,
      "step": 1200
    },
    {
      "epoch": 0.23354564755838642,
      "grad_norm": 8.353026390075684,
      "learning_rate": 1.8833236826867403e-05,
      "loss": 4.9575,
      "step": 1210
    },
    {
      "epoch": 0.23547577687705076,
      "grad_norm": 11.721516609191895,
      "learning_rate": 1.8823586180274082e-05,
      "loss": 5.3655,
      "step": 1220
    },
    {
      "epoch": 0.2374059061957151,
      "grad_norm": 7.1019182205200195,
      "learning_rate": 1.8813935533680757e-05,
      "loss": 5.6029,
      "step": 1230
    },
    {
      "epoch": 0.23933603551437946,
      "grad_norm": 6.136909008026123,
      "learning_rate": 1.8804284887087436e-05,
      "loss": 4.9323,
      "step": 1240
    },
    {
      "epoch": 0.2412661648330438,
      "grad_norm": 6.271228790283203,
      "learning_rate": 1.8794634240494115e-05,
      "loss": 4.9765,
      "step": 1250
    },
    {
      "epoch": 0.24319629415170815,
      "grad_norm": 7.585427761077881,
      "learning_rate": 1.878498359390079e-05,
      "loss": 5.2525,
      "step": 1260
    },
    {
      "epoch": 0.24512642347037253,
      "grad_norm": 10.053956985473633,
      "learning_rate": 1.8775332947307473e-05,
      "loss": 5.0213,
      "step": 1270
    },
    {
      "epoch": 0.24705655278903688,
      "grad_norm": 13.246166229248047,
      "learning_rate": 1.8765682300714148e-05,
      "loss": 5.3379,
      "step": 1280
    },
    {
      "epoch": 0.24898668210770122,
      "grad_norm": 6.756868362426758,
      "learning_rate": 1.8756031654120827e-05,
      "loss": 5.3188,
      "step": 1290
    },
    {
      "epoch": 0.25091681142636557,
      "grad_norm": 7.071446418762207,
      "learning_rate": 1.8746381007527506e-05,
      "loss": 4.9145,
      "step": 1300
    },
    {
      "epoch": 0.2528469407450299,
      "grad_norm": 6.196412563323975,
      "learning_rate": 1.8736730360934185e-05,
      "loss": 4.6793,
      "step": 1310
    },
    {
      "epoch": 0.25477707006369427,
      "grad_norm": 7.335787296295166,
      "learning_rate": 1.8727079714340864e-05,
      "loss": 5.2582,
      "step": 1320
    },
    {
      "epoch": 0.25670719938235864,
      "grad_norm": 7.065436840057373,
      "learning_rate": 1.8717429067747542e-05,
      "loss": 5.4644,
      "step": 1330
    },
    {
      "epoch": 0.25863732870102296,
      "grad_norm": 6.808575630187988,
      "learning_rate": 1.8707778421154218e-05,
      "loss": 5.3685,
      "step": 1340
    },
    {
      "epoch": 0.26056745801968734,
      "grad_norm": 5.8527727127075195,
      "learning_rate": 1.8698127774560897e-05,
      "loss": 5.1827,
      "step": 1350
    },
    {
      "epoch": 0.26249758733835166,
      "grad_norm": 8.89516544342041,
      "learning_rate": 1.8688477127967575e-05,
      "loss": 4.6836,
      "step": 1360
    },
    {
      "epoch": 0.26442771665701603,
      "grad_norm": 10.179606437683105,
      "learning_rate": 1.867882648137425e-05,
      "loss": 5.1961,
      "step": 1370
    },
    {
      "epoch": 0.26635784597568035,
      "grad_norm": 7.513137340545654,
      "learning_rate": 1.8669175834780933e-05,
      "loss": 5.2341,
      "step": 1380
    },
    {
      "epoch": 0.2682879752943447,
      "grad_norm": 17.915586471557617,
      "learning_rate": 1.8659525188187612e-05,
      "loss": 4.9661,
      "step": 1390
    },
    {
      "epoch": 0.27021810461300905,
      "grad_norm": 7.9802565574646,
      "learning_rate": 1.8649874541594287e-05,
      "loss": 4.5326,
      "step": 1400
    },
    {
      "epoch": 0.2721482339316734,
      "grad_norm": 7.728894233703613,
      "learning_rate": 1.8640223895000966e-05,
      "loss": 4.6519,
      "step": 1410
    },
    {
      "epoch": 0.2740783632503378,
      "grad_norm": 9.24466323852539,
      "learning_rate": 1.8630573248407645e-05,
      "loss": 5.279,
      "step": 1420
    },
    {
      "epoch": 0.2760084925690021,
      "grad_norm": 9.440627098083496,
      "learning_rate": 1.8620922601814324e-05,
      "loss": 5.0724,
      "step": 1430
    },
    {
      "epoch": 0.2779386218876665,
      "grad_norm": 7.240311145782471,
      "learning_rate": 1.8611271955221003e-05,
      "loss": 4.5161,
      "step": 1440
    },
    {
      "epoch": 0.2798687512063308,
      "grad_norm": 7.718796730041504,
      "learning_rate": 1.8601621308627678e-05,
      "loss": 4.6153,
      "step": 1450
    },
    {
      "epoch": 0.2817988805249952,
      "grad_norm": 8.262622833251953,
      "learning_rate": 1.8591970662034357e-05,
      "loss": 4.6647,
      "step": 1460
    },
    {
      "epoch": 0.2837290098436595,
      "grad_norm": 13.026451110839844,
      "learning_rate": 1.8582320015441036e-05,
      "loss": 4.8471,
      "step": 1470
    },
    {
      "epoch": 0.2856591391623239,
      "grad_norm": 17.622150421142578,
      "learning_rate": 1.8572669368847715e-05,
      "loss": 4.7103,
      "step": 1480
    },
    {
      "epoch": 0.2875892684809882,
      "grad_norm": 6.581427097320557,
      "learning_rate": 1.8563018722254394e-05,
      "loss": 4.5086,
      "step": 1490
    },
    {
      "epoch": 0.2895193977996526,
      "grad_norm": 6.910327911376953,
      "learning_rate": 1.8553368075661072e-05,
      "loss": 4.659,
      "step": 1500
    },
    {
      "epoch": 0.29144952711831695,
      "grad_norm": 9.367227554321289,
      "learning_rate": 1.8543717429067748e-05,
      "loss": 5.0316,
      "step": 1510
    },
    {
      "epoch": 0.2933796564369813,
      "grad_norm": 6.840023517608643,
      "learning_rate": 1.8534066782474427e-05,
      "loss": 4.5193,
      "step": 1520
    },
    {
      "epoch": 0.29530978575564565,
      "grad_norm": 44.29435729980469,
      "learning_rate": 1.8524416135881106e-05,
      "loss": 4.4861,
      "step": 1530
    },
    {
      "epoch": 0.29723991507430997,
      "grad_norm": 13.47384262084961,
      "learning_rate": 1.8514765489287784e-05,
      "loss": 4.7819,
      "step": 1540
    },
    {
      "epoch": 0.29917004439297434,
      "grad_norm": 9.829700469970703,
      "learning_rate": 1.8505114842694463e-05,
      "loss": 4.8112,
      "step": 1550
    },
    {
      "epoch": 0.30110017371163866,
      "grad_norm": 5.92144250869751,
      "learning_rate": 1.8495464196101142e-05,
      "loss": 4.8173,
      "step": 1560
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 6.405904293060303,
      "learning_rate": 1.8485813549507817e-05,
      "loss": 4.8833,
      "step": 1570
    },
    {
      "epoch": 0.30496043234896736,
      "grad_norm": 7.908579349517822,
      "learning_rate": 1.8476162902914496e-05,
      "loss": 4.5424,
      "step": 1580
    },
    {
      "epoch": 0.30689056166763173,
      "grad_norm": 7.976463317871094,
      "learning_rate": 1.8466512256321175e-05,
      "loss": 4.523,
      "step": 1590
    },
    {
      "epoch": 0.3088206909862961,
      "grad_norm": 8.508816719055176,
      "learning_rate": 1.8456861609727854e-05,
      "loss": 4.8165,
      "step": 1600
    },
    {
      "epoch": 0.31075082030496043,
      "grad_norm": 7.340943813323975,
      "learning_rate": 1.8447210963134533e-05,
      "loss": 4.5409,
      "step": 1610
    },
    {
      "epoch": 0.3126809496236248,
      "grad_norm": 14.297039031982422,
      "learning_rate": 1.8437560316541208e-05,
      "loss": 4.332,
      "step": 1620
    },
    {
      "epoch": 0.3146110789422891,
      "grad_norm": 5.998317718505859,
      "learning_rate": 1.8427909669947887e-05,
      "loss": 4.4518,
      "step": 1630
    },
    {
      "epoch": 0.3165412082609535,
      "grad_norm": 6.78845739364624,
      "learning_rate": 1.8418259023354566e-05,
      "loss": 4.7349,
      "step": 1640
    },
    {
      "epoch": 0.3184713375796178,
      "grad_norm": 5.646640777587891,
      "learning_rate": 1.8408608376761245e-05,
      "loss": 4.7158,
      "step": 1650
    },
    {
      "epoch": 0.3204014668982822,
      "grad_norm": 6.006961822509766,
      "learning_rate": 1.8398957730167924e-05,
      "loss": 4.9652,
      "step": 1660
    },
    {
      "epoch": 0.3223315962169465,
      "grad_norm": 7.731565952301025,
      "learning_rate": 1.8389307083574602e-05,
      "loss": 4.5353,
      "step": 1670
    },
    {
      "epoch": 0.3242617255356109,
      "grad_norm": 7.489681243896484,
      "learning_rate": 1.8379656436981278e-05,
      "loss": 3.8659,
      "step": 1680
    },
    {
      "epoch": 0.3261918548542752,
      "grad_norm": 20.51139259338379,
      "learning_rate": 1.8370005790387957e-05,
      "loss": 4.6245,
      "step": 1690
    },
    {
      "epoch": 0.3281219841729396,
      "grad_norm": 14.164704322814941,
      "learning_rate": 1.8360355143794636e-05,
      "loss": 4.5258,
      "step": 1700
    },
    {
      "epoch": 0.33005211349160396,
      "grad_norm": 17.19072723388672,
      "learning_rate": 1.8350704497201314e-05,
      "loss": 4.6691,
      "step": 1710
    },
    {
      "epoch": 0.3319822428102683,
      "grad_norm": 6.005631923675537,
      "learning_rate": 1.8341053850607993e-05,
      "loss": 4.2377,
      "step": 1720
    },
    {
      "epoch": 0.33391237212893266,
      "grad_norm": 10.288466453552246,
      "learning_rate": 1.8331403204014672e-05,
      "loss": 4.8125,
      "step": 1730
    },
    {
      "epoch": 0.335842501447597,
      "grad_norm": 6.736964702606201,
      "learning_rate": 1.8321752557421347e-05,
      "loss": 4.5864,
      "step": 1740
    },
    {
      "epoch": 0.33777263076626135,
      "grad_norm": 10.288248062133789,
      "learning_rate": 1.8312101910828026e-05,
      "loss": 4.2699,
      "step": 1750
    },
    {
      "epoch": 0.33970276008492567,
      "grad_norm": 36.46757507324219,
      "learning_rate": 1.8302451264234705e-05,
      "loss": 4.5609,
      "step": 1760
    },
    {
      "epoch": 0.34163288940359005,
      "grad_norm": 7.099705219268799,
      "learning_rate": 1.8292800617641384e-05,
      "loss": 5.0214,
      "step": 1770
    },
    {
      "epoch": 0.34356301872225437,
      "grad_norm": 13.897612571716309,
      "learning_rate": 1.8283149971048063e-05,
      "loss": 4.3219,
      "step": 1780
    },
    {
      "epoch": 0.34549314804091874,
      "grad_norm": 9.98244571685791,
      "learning_rate": 1.8273499324454738e-05,
      "loss": 5.1523,
      "step": 1790
    },
    {
      "epoch": 0.3474232773595831,
      "grad_norm": 4.977921009063721,
      "learning_rate": 1.8263848677861417e-05,
      "loss": 4.2971,
      "step": 1800
    },
    {
      "epoch": 0.34935340667824744,
      "grad_norm": 8.000617980957031,
      "learning_rate": 1.8254198031268096e-05,
      "loss": 4.5084,
      "step": 1810
    },
    {
      "epoch": 0.3512835359969118,
      "grad_norm": 11.901802062988281,
      "learning_rate": 1.8244547384674775e-05,
      "loss": 3.9076,
      "step": 1820
    },
    {
      "epoch": 0.35321366531557613,
      "grad_norm": 6.572952747344971,
      "learning_rate": 1.8234896738081454e-05,
      "loss": 4.9256,
      "step": 1830
    },
    {
      "epoch": 0.3551437946342405,
      "grad_norm": 8.71635627746582,
      "learning_rate": 1.8225246091488132e-05,
      "loss": 4.8433,
      "step": 1840
    },
    {
      "epoch": 0.3570739239529048,
      "grad_norm": 10.929906845092773,
      "learning_rate": 1.8215595444894808e-05,
      "loss": 4.7728,
      "step": 1850
    },
    {
      "epoch": 0.3590040532715692,
      "grad_norm": 13.467706680297852,
      "learning_rate": 1.8205944798301487e-05,
      "loss": 4.3251,
      "step": 1860
    },
    {
      "epoch": 0.3609341825902335,
      "grad_norm": 20.57788848876953,
      "learning_rate": 1.8196294151708166e-05,
      "loss": 4.3356,
      "step": 1870
    },
    {
      "epoch": 0.3628643119088979,
      "grad_norm": 7.400430679321289,
      "learning_rate": 1.8186643505114844e-05,
      "loss": 4.4647,
      "step": 1880
    },
    {
      "epoch": 0.36479444122756227,
      "grad_norm": 6.432956218719482,
      "learning_rate": 1.8176992858521523e-05,
      "loss": 4.38,
      "step": 1890
    },
    {
      "epoch": 0.3667245705462266,
      "grad_norm": 8.644408226013184,
      "learning_rate": 1.8167342211928202e-05,
      "loss": 4.0643,
      "step": 1900
    },
    {
      "epoch": 0.36865469986489097,
      "grad_norm": 6.9160027503967285,
      "learning_rate": 1.8157691565334877e-05,
      "loss": 4.4685,
      "step": 1910
    },
    {
      "epoch": 0.3705848291835553,
      "grad_norm": 6.216663837432861,
      "learning_rate": 1.8148040918741556e-05,
      "loss": 4.8441,
      "step": 1920
    },
    {
      "epoch": 0.37251495850221966,
      "grad_norm": 7.194680690765381,
      "learning_rate": 1.8138390272148235e-05,
      "loss": 4.2127,
      "step": 1930
    },
    {
      "epoch": 0.374445087820884,
      "grad_norm": 21.59941864013672,
      "learning_rate": 1.8128739625554914e-05,
      "loss": 4.1862,
      "step": 1940
    },
    {
      "epoch": 0.37637521713954836,
      "grad_norm": 9.869810104370117,
      "learning_rate": 1.8119088978961593e-05,
      "loss": 4.0836,
      "step": 1950
    },
    {
      "epoch": 0.3783053464582127,
      "grad_norm": 7.340236186981201,
      "learning_rate": 1.8109438332368268e-05,
      "loss": 4.9146,
      "step": 1960
    },
    {
      "epoch": 0.38023547577687705,
      "grad_norm": 15.042229652404785,
      "learning_rate": 1.8099787685774947e-05,
      "loss": 4.0563,
      "step": 1970
    },
    {
      "epoch": 0.3821656050955414,
      "grad_norm": 11.849980354309082,
      "learning_rate": 1.8090137039181626e-05,
      "loss": 4.0834,
      "step": 1980
    },
    {
      "epoch": 0.38409573441420575,
      "grad_norm": 6.275345802307129,
      "learning_rate": 1.8080486392588305e-05,
      "loss": 3.6484,
      "step": 1990
    },
    {
      "epoch": 0.3860258637328701,
      "grad_norm": 10.22683048248291,
      "learning_rate": 1.8070835745994984e-05,
      "loss": 3.8958,
      "step": 2000
    },
    {
      "epoch": 0.38795599305153444,
      "grad_norm": 8.626140594482422,
      "learning_rate": 1.8061185099401662e-05,
      "loss": 4.4357,
      "step": 2010
    },
    {
      "epoch": 0.3898861223701988,
      "grad_norm": 8.039149284362793,
      "learning_rate": 1.8051534452808338e-05,
      "loss": 4.0127,
      "step": 2020
    },
    {
      "epoch": 0.39181625168886314,
      "grad_norm": 8.368335723876953,
      "learning_rate": 1.8041883806215017e-05,
      "loss": 4.434,
      "step": 2030
    },
    {
      "epoch": 0.3937463810075275,
      "grad_norm": 9.196917533874512,
      "learning_rate": 1.8032233159621696e-05,
      "loss": 3.7758,
      "step": 2040
    },
    {
      "epoch": 0.39567651032619183,
      "grad_norm": 7.111783981323242,
      "learning_rate": 1.8022582513028374e-05,
      "loss": 4.1655,
      "step": 2050
    },
    {
      "epoch": 0.3976066396448562,
      "grad_norm": 7.0398969650268555,
      "learning_rate": 1.8012931866435053e-05,
      "loss": 3.9069,
      "step": 2060
    },
    {
      "epoch": 0.3995367689635206,
      "grad_norm": 9.334893226623535,
      "learning_rate": 1.8003281219841732e-05,
      "loss": 4.2246,
      "step": 2070
    },
    {
      "epoch": 0.4014668982821849,
      "grad_norm": 6.667873859405518,
      "learning_rate": 1.7993630573248407e-05,
      "loss": 4.4796,
      "step": 2080
    },
    {
      "epoch": 0.4033970276008493,
      "grad_norm": 7.998379230499268,
      "learning_rate": 1.7983979926655086e-05,
      "loss": 4.2609,
      "step": 2090
    },
    {
      "epoch": 0.4053271569195136,
      "grad_norm": 10.943913459777832,
      "learning_rate": 1.7974329280061765e-05,
      "loss": 4.0185,
      "step": 2100
    },
    {
      "epoch": 0.407257286238178,
      "grad_norm": 11.85797119140625,
      "learning_rate": 1.7964678633468444e-05,
      "loss": 4.2561,
      "step": 2110
    },
    {
      "epoch": 0.4091874155568423,
      "grad_norm": 19.273155212402344,
      "learning_rate": 1.7955027986875123e-05,
      "loss": 4.2336,
      "step": 2120
    },
    {
      "epoch": 0.41111754487550667,
      "grad_norm": 7.099930286407471,
      "learning_rate": 1.7945377340281798e-05,
      "loss": 3.9275,
      "step": 2130
    },
    {
      "epoch": 0.413047674194171,
      "grad_norm": 8.716395378112793,
      "learning_rate": 1.7935726693688477e-05,
      "loss": 4.1054,
      "step": 2140
    },
    {
      "epoch": 0.41497780351283536,
      "grad_norm": 10.443559646606445,
      "learning_rate": 1.792607604709516e-05,
      "loss": 4.2174,
      "step": 2150
    },
    {
      "epoch": 0.4169079328314997,
      "grad_norm": 14.193965911865234,
      "learning_rate": 1.7916425400501835e-05,
      "loss": 4.3227,
      "step": 2160
    },
    {
      "epoch": 0.41883806215016406,
      "grad_norm": 6.098630905151367,
      "learning_rate": 1.7906774753908514e-05,
      "loss": 3.9534,
      "step": 2170
    },
    {
      "epoch": 0.42076819146882843,
      "grad_norm": 25.9865779876709,
      "learning_rate": 1.7897124107315192e-05,
      "loss": 4.2371,
      "step": 2180
    },
    {
      "epoch": 0.42269832078749275,
      "grad_norm": 32.110660552978516,
      "learning_rate": 1.7887473460721868e-05,
      "loss": 3.9489,
      "step": 2190
    },
    {
      "epoch": 0.42462845010615713,
      "grad_norm": 11.571063995361328,
      "learning_rate": 1.7877822814128547e-05,
      "loss": 4.0034,
      "step": 2200
    },
    {
      "epoch": 0.42655857942482145,
      "grad_norm": 7.472753524780273,
      "learning_rate": 1.7868172167535226e-05,
      "loss": 4.0219,
      "step": 2210
    },
    {
      "epoch": 0.4284887087434858,
      "grad_norm": 17.098100662231445,
      "learning_rate": 1.7858521520941904e-05,
      "loss": 4.3027,
      "step": 2220
    },
    {
      "epoch": 0.43041883806215014,
      "grad_norm": 8.250944137573242,
      "learning_rate": 1.7848870874348583e-05,
      "loss": 4.6012,
      "step": 2230
    },
    {
      "epoch": 0.4323489673808145,
      "grad_norm": 9.661117553710938,
      "learning_rate": 1.7839220227755262e-05,
      "loss": 4.2998,
      "step": 2240
    },
    {
      "epoch": 0.43427909669947884,
      "grad_norm": 5.911857604980469,
      "learning_rate": 1.7829569581161937e-05,
      "loss": 3.5109,
      "step": 2250
    },
    {
      "epoch": 0.4362092260181432,
      "grad_norm": 6.645362854003906,
      "learning_rate": 1.781991893456862e-05,
      "loss": 4.1452,
      "step": 2260
    },
    {
      "epoch": 0.4381393553368076,
      "grad_norm": 6.411318778991699,
      "learning_rate": 1.7810268287975295e-05,
      "loss": 4.5264,
      "step": 2270
    },
    {
      "epoch": 0.4400694846554719,
      "grad_norm": 9.841276168823242,
      "learning_rate": 1.7800617641381974e-05,
      "loss": 4.0439,
      "step": 2280
    },
    {
      "epoch": 0.4419996139741363,
      "grad_norm": 12.910196304321289,
      "learning_rate": 1.7790966994788653e-05,
      "loss": 3.609,
      "step": 2290
    },
    {
      "epoch": 0.4439297432928006,
      "grad_norm": 7.865506172180176,
      "learning_rate": 1.7781316348195328e-05,
      "loss": 4.3683,
      "step": 2300
    },
    {
      "epoch": 0.445859872611465,
      "grad_norm": 22.770177841186523,
      "learning_rate": 1.7771665701602007e-05,
      "loss": 4.0467,
      "step": 2310
    },
    {
      "epoch": 0.4477900019301293,
      "grad_norm": 7.775924205780029,
      "learning_rate": 1.776201505500869e-05,
      "loss": 3.9441,
      "step": 2320
    },
    {
      "epoch": 0.4497201312487937,
      "grad_norm": 6.765857696533203,
      "learning_rate": 1.7752364408415365e-05,
      "loss": 3.4522,
      "step": 2330
    },
    {
      "epoch": 0.451650260567458,
      "grad_norm": 7.670359134674072,
      "learning_rate": 1.7742713761822044e-05,
      "loss": 4.2729,
      "step": 2340
    },
    {
      "epoch": 0.45358038988612237,
      "grad_norm": 10.862842559814453,
      "learning_rate": 1.7733063115228722e-05,
      "loss": 4.0545,
      "step": 2350
    },
    {
      "epoch": 0.45551051920478675,
      "grad_norm": 6.098352432250977,
      "learning_rate": 1.7723412468635398e-05,
      "loss": 3.7113,
      "step": 2360
    },
    {
      "epoch": 0.45744064852345107,
      "grad_norm": 6.078566551208496,
      "learning_rate": 1.771376182204208e-05,
      "loss": 3.5331,
      "step": 2370
    },
    {
      "epoch": 0.45937077784211544,
      "grad_norm": 6.2240495681762695,
      "learning_rate": 1.7704111175448756e-05,
      "loss": 4.1856,
      "step": 2380
    },
    {
      "epoch": 0.46130090716077976,
      "grad_norm": 15.53762149810791,
      "learning_rate": 1.7694460528855434e-05,
      "loss": 3.5276,
      "step": 2390
    },
    {
      "epoch": 0.46323103647944414,
      "grad_norm": 12.622014999389648,
      "learning_rate": 1.7684809882262113e-05,
      "loss": 3.6626,
      "step": 2400
    },
    {
      "epoch": 0.46516116579810846,
      "grad_norm": 25.38123321533203,
      "learning_rate": 1.7675159235668792e-05,
      "loss": 3.5912,
      "step": 2410
    },
    {
      "epoch": 0.46709129511677283,
      "grad_norm": 6.33734130859375,
      "learning_rate": 1.7665508589075467e-05,
      "loss": 4.1572,
      "step": 2420
    },
    {
      "epoch": 0.46902142443543715,
      "grad_norm": 38.49562454223633,
      "learning_rate": 1.765585794248215e-05,
      "loss": 4.1508,
      "step": 2430
    },
    {
      "epoch": 0.4709515537541015,
      "grad_norm": 10.528665542602539,
      "learning_rate": 1.7646207295888825e-05,
      "loss": 3.7937,
      "step": 2440
    },
    {
      "epoch": 0.4728816830727659,
      "grad_norm": 6.140833854675293,
      "learning_rate": 1.7636556649295504e-05,
      "loss": 3.7679,
      "step": 2450
    },
    {
      "epoch": 0.4748118123914302,
      "grad_norm": 9.86474609375,
      "learning_rate": 1.7626906002702183e-05,
      "loss": 3.6415,
      "step": 2460
    },
    {
      "epoch": 0.4767419417100946,
      "grad_norm": 14.617626190185547,
      "learning_rate": 1.7617255356108858e-05,
      "loss": 3.9313,
      "step": 2470
    },
    {
      "epoch": 0.4786720710287589,
      "grad_norm": 9.230982780456543,
      "learning_rate": 1.760760470951554e-05,
      "loss": 3.9983,
      "step": 2480
    },
    {
      "epoch": 0.4806022003474233,
      "grad_norm": 24.04367446899414,
      "learning_rate": 1.759795406292222e-05,
      "loss": 3.8768,
      "step": 2490
    },
    {
      "epoch": 0.4825323296660876,
      "grad_norm": 13.333711624145508,
      "learning_rate": 1.7588303416328895e-05,
      "loss": 3.5233,
      "step": 2500
    },
    {
      "epoch": 0.484462458984752,
      "grad_norm": 5.174585819244385,
      "learning_rate": 1.7578652769735574e-05,
      "loss": 3.8531,
      "step": 2510
    },
    {
      "epoch": 0.4863925883034163,
      "grad_norm": 7.800748348236084,
      "learning_rate": 1.7569002123142252e-05,
      "loss": 3.8051,
      "step": 2520
    },
    {
      "epoch": 0.4883227176220807,
      "grad_norm": 6.981714725494385,
      "learning_rate": 1.7559351476548928e-05,
      "loss": 3.9069,
      "step": 2530
    },
    {
      "epoch": 0.49025284694074506,
      "grad_norm": 8.9419584274292,
      "learning_rate": 1.754970082995561e-05,
      "loss": 3.2824,
      "step": 2540
    },
    {
      "epoch": 0.4921829762594094,
      "grad_norm": 14.196240425109863,
      "learning_rate": 1.7540050183362286e-05,
      "loss": 3.2785,
      "step": 2550
    },
    {
      "epoch": 0.49411310557807375,
      "grad_norm": 10.692461967468262,
      "learning_rate": 1.7530399536768964e-05,
      "loss": 3.6898,
      "step": 2560
    },
    {
      "epoch": 0.49604323489673807,
      "grad_norm": 7.147149085998535,
      "learning_rate": 1.7520748890175643e-05,
      "loss": 3.5698,
      "step": 2570
    },
    {
      "epoch": 0.49797336421540245,
      "grad_norm": 9.731304168701172,
      "learning_rate": 1.7511098243582322e-05,
      "loss": 3.5632,
      "step": 2580
    },
    {
      "epoch": 0.49990349353406677,
      "grad_norm": 7.830231666564941,
      "learning_rate": 1.7501447596989e-05,
      "loss": 3.3803,
      "step": 2590
    },
    {
      "epoch": 0.5018336228527311,
      "grad_norm": 11.193885803222656,
      "learning_rate": 1.749179695039568e-05,
      "loss": 3.8181,
      "step": 2600
    },
    {
      "epoch": 0.5037637521713955,
      "grad_norm": 7.315860748291016,
      "learning_rate": 1.7482146303802355e-05,
      "loss": 3.8097,
      "step": 2610
    },
    {
      "epoch": 0.5056938814900598,
      "grad_norm": 8.980403900146484,
      "learning_rate": 1.7472495657209034e-05,
      "loss": 3.0628,
      "step": 2620
    },
    {
      "epoch": 0.5076240108087242,
      "grad_norm": 8.450217247009277,
      "learning_rate": 1.7462845010615713e-05,
      "loss": 3.7893,
      "step": 2630
    },
    {
      "epoch": 0.5095541401273885,
      "grad_norm": 11.515682220458984,
      "learning_rate": 1.745319436402239e-05,
      "loss": 3.953,
      "step": 2640
    },
    {
      "epoch": 0.5114842694460529,
      "grad_norm": 14.505084991455078,
      "learning_rate": 1.744354371742907e-05,
      "loss": 3.8598,
      "step": 2650
    },
    {
      "epoch": 0.5134143987647173,
      "grad_norm": 21.0445499420166,
      "learning_rate": 1.743389307083575e-05,
      "loss": 3.898,
      "step": 2660
    },
    {
      "epoch": 0.5153445280833816,
      "grad_norm": 6.005434513092041,
      "learning_rate": 1.7424242424242425e-05,
      "loss": 2.8886,
      "step": 2670
    },
    {
      "epoch": 0.5172746574020459,
      "grad_norm": 7.178379058837891,
      "learning_rate": 1.7414591777649104e-05,
      "loss": 3.5047,
      "step": 2680
    },
    {
      "epoch": 0.5192047867207102,
      "grad_norm": 15.215638160705566,
      "learning_rate": 1.7404941131055782e-05,
      "loss": 3.7887,
      "step": 2690
    },
    {
      "epoch": 0.5211349160393747,
      "grad_norm": 9.0122709274292,
      "learning_rate": 1.739529048446246e-05,
      "loss": 3.6963,
      "step": 2700
    },
    {
      "epoch": 0.523065045358039,
      "grad_norm": 9.435420989990234,
      "learning_rate": 1.738563983786914e-05,
      "loss": 3.8544,
      "step": 2710
    },
    {
      "epoch": 0.5249951746767033,
      "grad_norm": 8.19388198852539,
      "learning_rate": 1.7375989191275816e-05,
      "loss": 3.472,
      "step": 2720
    },
    {
      "epoch": 0.5269253039953677,
      "grad_norm": 9.184945106506348,
      "learning_rate": 1.7366338544682494e-05,
      "loss": 3.7813,
      "step": 2730
    },
    {
      "epoch": 0.5288554333140321,
      "grad_norm": 11.583895683288574,
      "learning_rate": 1.7356687898089173e-05,
      "loss": 3.5731,
      "step": 2740
    },
    {
      "epoch": 0.5307855626326964,
      "grad_norm": 13.413361549377441,
      "learning_rate": 1.7347037251495852e-05,
      "loss": 3.4733,
      "step": 2750
    },
    {
      "epoch": 0.5327156919513607,
      "grad_norm": 35.7728271484375,
      "learning_rate": 1.733738660490253e-05,
      "loss": 3.3703,
      "step": 2760
    },
    {
      "epoch": 0.5346458212700251,
      "grad_norm": 7.771111488342285,
      "learning_rate": 1.732773595830921e-05,
      "loss": 3.4673,
      "step": 2770
    },
    {
      "epoch": 0.5365759505886895,
      "grad_norm": 10.666478157043457,
      "learning_rate": 1.7318085311715885e-05,
      "loss": 3.5871,
      "step": 2780
    },
    {
      "epoch": 0.5385060799073538,
      "grad_norm": 11.084209442138672,
      "learning_rate": 1.7308434665122564e-05,
      "loss": 3.685,
      "step": 2790
    },
    {
      "epoch": 0.5404362092260181,
      "grad_norm": 21.586355209350586,
      "learning_rate": 1.7298784018529243e-05,
      "loss": 3.9039,
      "step": 2800
    },
    {
      "epoch": 0.5423663385446825,
      "grad_norm": 22.158781051635742,
      "learning_rate": 1.728913337193592e-05,
      "loss": 3.7775,
      "step": 2810
    },
    {
      "epoch": 0.5442964678633468,
      "grad_norm": 8.502593040466309,
      "learning_rate": 1.72794827253426e-05,
      "loss": 3.9409,
      "step": 2820
    },
    {
      "epoch": 0.5462265971820112,
      "grad_norm": 91.3979721069336,
      "learning_rate": 1.726983207874928e-05,
      "loss": 3.9963,
      "step": 2830
    },
    {
      "epoch": 0.5481567265006756,
      "grad_norm": 4.990839958190918,
      "learning_rate": 1.7260181432155955e-05,
      "loss": 3.1512,
      "step": 2840
    },
    {
      "epoch": 0.5500868558193399,
      "grad_norm": 8.922628402709961,
      "learning_rate": 1.7250530785562634e-05,
      "loss": 4.015,
      "step": 2850
    },
    {
      "epoch": 0.5520169851380042,
      "grad_norm": 19.98396110534668,
      "learning_rate": 1.7240880138969312e-05,
      "loss": 4.1068,
      "step": 2860
    },
    {
      "epoch": 0.5539471144566686,
      "grad_norm": 17.021228790283203,
      "learning_rate": 1.723122949237599e-05,
      "loss": 3.0682,
      "step": 2870
    },
    {
      "epoch": 0.555877243775333,
      "grad_norm": 24.480079650878906,
      "learning_rate": 1.722157884578267e-05,
      "loss": 3.9348,
      "step": 2880
    },
    {
      "epoch": 0.5578073730939973,
      "grad_norm": 11.079781532287598,
      "learning_rate": 1.7211928199189346e-05,
      "loss": 2.7971,
      "step": 2890
    },
    {
      "epoch": 0.5597375024126616,
      "grad_norm": 7.907978057861328,
      "learning_rate": 1.7202277552596024e-05,
      "loss": 3.3366,
      "step": 2900
    },
    {
      "epoch": 0.561667631731326,
      "grad_norm": 5.970550537109375,
      "learning_rate": 1.7192626906002703e-05,
      "loss": 2.7998,
      "step": 2910
    },
    {
      "epoch": 0.5635977610499904,
      "grad_norm": 14.42388916015625,
      "learning_rate": 1.7182976259409382e-05,
      "loss": 3.0079,
      "step": 2920
    },
    {
      "epoch": 0.5655278903686547,
      "grad_norm": 18.17230796813965,
      "learning_rate": 1.717332561281606e-05,
      "loss": 3.8249,
      "step": 2930
    },
    {
      "epoch": 0.567458019687319,
      "grad_norm": 6.268819808959961,
      "learning_rate": 1.716367496622274e-05,
      "loss": 3.2793,
      "step": 2940
    },
    {
      "epoch": 0.5693881490059834,
      "grad_norm": 6.04655122756958,
      "learning_rate": 1.7154024319629415e-05,
      "loss": 3.0177,
      "step": 2950
    },
    {
      "epoch": 0.5713182783246478,
      "grad_norm": 11.537164688110352,
      "learning_rate": 1.7144373673036094e-05,
      "loss": 3.9443,
      "step": 2960
    },
    {
      "epoch": 0.5732484076433121,
      "grad_norm": 12.414835929870605,
      "learning_rate": 1.7134723026442773e-05,
      "loss": 3.0555,
      "step": 2970
    },
    {
      "epoch": 0.5751785369619764,
      "grad_norm": 13.978362083435059,
      "learning_rate": 1.712507237984945e-05,
      "loss": 3.6011,
      "step": 2980
    },
    {
      "epoch": 0.5771086662806408,
      "grad_norm": 21.587947845458984,
      "learning_rate": 1.711542173325613e-05,
      "loss": 3.1449,
      "step": 2990
    },
    {
      "epoch": 0.5790387955993052,
      "grad_norm": 7.6136884689331055,
      "learning_rate": 1.710577108666281e-05,
      "loss": 3.4289,
      "step": 3000
    },
    {
      "epoch": 0.5809689249179695,
      "grad_norm": 17.014087677001953,
      "learning_rate": 1.7096120440069485e-05,
      "loss": 3.5344,
      "step": 3010
    },
    {
      "epoch": 0.5828990542366339,
      "grad_norm": 52.374908447265625,
      "learning_rate": 1.7086469793476164e-05,
      "loss": 4.0569,
      "step": 3020
    },
    {
      "epoch": 0.5848291835552982,
      "grad_norm": 13.4239501953125,
      "learning_rate": 1.7076819146882842e-05,
      "loss": 3.3683,
      "step": 3030
    },
    {
      "epoch": 0.5867593128739625,
      "grad_norm": 8.74343490600586,
      "learning_rate": 1.706716850028952e-05,
      "loss": 3.5365,
      "step": 3040
    },
    {
      "epoch": 0.5886894421926269,
      "grad_norm": 11.671133041381836,
      "learning_rate": 1.70575178536962e-05,
      "loss": 3.8069,
      "step": 3050
    },
    {
      "epoch": 0.5906195715112913,
      "grad_norm": 9.643363952636719,
      "learning_rate": 1.7047867207102876e-05,
      "loss": 3.3345,
      "step": 3060
    },
    {
      "epoch": 0.5925497008299556,
      "grad_norm": 14.43710708618164,
      "learning_rate": 1.7038216560509554e-05,
      "loss": 3.5804,
      "step": 3070
    },
    {
      "epoch": 0.5944798301486199,
      "grad_norm": 7.69384765625,
      "learning_rate": 1.7028565913916233e-05,
      "loss": 3.6423,
      "step": 3080
    },
    {
      "epoch": 0.5964099594672843,
      "grad_norm": 6.064235210418701,
      "learning_rate": 1.7018915267322912e-05,
      "loss": 2.7077,
      "step": 3090
    },
    {
      "epoch": 0.5983400887859487,
      "grad_norm": 7.91792631149292,
      "learning_rate": 1.700926462072959e-05,
      "loss": 3.1507,
      "step": 3100
    },
    {
      "epoch": 0.600270218104613,
      "grad_norm": 12.469233512878418,
      "learning_rate": 1.699961397413627e-05,
      "loss": 3.2433,
      "step": 3110
    },
    {
      "epoch": 0.6022003474232773,
      "grad_norm": 5.340635776519775,
      "learning_rate": 1.6989963327542945e-05,
      "loss": 3.4188,
      "step": 3120
    },
    {
      "epoch": 0.6041304767419418,
      "grad_norm": 10.047033309936523,
      "learning_rate": 1.6980312680949624e-05,
      "loss": 2.9717,
      "step": 3130
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 19.71468162536621,
      "learning_rate": 1.6970662034356303e-05,
      "loss": 3.0221,
      "step": 3140
    },
    {
      "epoch": 0.6079907353792704,
      "grad_norm": 6.612780570983887,
      "learning_rate": 1.696101138776298e-05,
      "loss": 3.6559,
      "step": 3150
    },
    {
      "epoch": 0.6099208646979347,
      "grad_norm": 12.935467720031738,
      "learning_rate": 1.695136074116966e-05,
      "loss": 3.1544,
      "step": 3160
    },
    {
      "epoch": 0.6118509940165991,
      "grad_norm": 13.622587203979492,
      "learning_rate": 1.694171009457634e-05,
      "loss": 3.1351,
      "step": 3170
    },
    {
      "epoch": 0.6137811233352635,
      "grad_norm": 6.236276149749756,
      "learning_rate": 1.6932059447983015e-05,
      "loss": 3.4818,
      "step": 3180
    },
    {
      "epoch": 0.6157112526539278,
      "grad_norm": 10.102008819580078,
      "learning_rate": 1.6922408801389694e-05,
      "loss": 2.8289,
      "step": 3190
    },
    {
      "epoch": 0.6176413819725922,
      "grad_norm": 9.28075122833252,
      "learning_rate": 1.6912758154796372e-05,
      "loss": 3.7551,
      "step": 3200
    },
    {
      "epoch": 0.6195715112912565,
      "grad_norm": 13.57631778717041,
      "learning_rate": 1.690310750820305e-05,
      "loss": 3.9836,
      "step": 3210
    },
    {
      "epoch": 0.6215016406099209,
      "grad_norm": 24.614410400390625,
      "learning_rate": 1.689345686160973e-05,
      "loss": 3.2603,
      "step": 3220
    },
    {
      "epoch": 0.6234317699285852,
      "grad_norm": 8.870604515075684,
      "learning_rate": 1.688380621501641e-05,
      "loss": 3.1542,
      "step": 3230
    },
    {
      "epoch": 0.6253618992472496,
      "grad_norm": 15.897994995117188,
      "learning_rate": 1.6874155568423084e-05,
      "loss": 3.7188,
      "step": 3240
    },
    {
      "epoch": 0.6272920285659139,
      "grad_norm": 12.094499588012695,
      "learning_rate": 1.6864504921829763e-05,
      "loss": 3.3858,
      "step": 3250
    },
    {
      "epoch": 0.6292221578845782,
      "grad_norm": 64.72811889648438,
      "learning_rate": 1.6854854275236442e-05,
      "loss": 3.4885,
      "step": 3260
    },
    {
      "epoch": 0.6311522872032426,
      "grad_norm": 6.89347505569458,
      "learning_rate": 1.684520362864312e-05,
      "loss": 3.0094,
      "step": 3270
    },
    {
      "epoch": 0.633082416521907,
      "grad_norm": 11.28564739227295,
      "learning_rate": 1.68355529820498e-05,
      "loss": 3.3873,
      "step": 3280
    },
    {
      "epoch": 0.6350125458405713,
      "grad_norm": 11.633718490600586,
      "learning_rate": 1.6825902335456475e-05,
      "loss": 3.2574,
      "step": 3290
    },
    {
      "epoch": 0.6369426751592356,
      "grad_norm": 6.6336493492126465,
      "learning_rate": 1.6816251688863154e-05,
      "loss": 3.3067,
      "step": 3300
    },
    {
      "epoch": 0.6388728044779001,
      "grad_norm": 9.454107284545898,
      "learning_rate": 1.6806601042269833e-05,
      "loss": 3.3331,
      "step": 3310
    },
    {
      "epoch": 0.6408029337965644,
      "grad_norm": 9.55730152130127,
      "learning_rate": 1.679695039567651e-05,
      "loss": 3.2324,
      "step": 3320
    },
    {
      "epoch": 0.6427330631152287,
      "grad_norm": 6.369880676269531,
      "learning_rate": 1.678729974908319e-05,
      "loss": 3.0149,
      "step": 3330
    },
    {
      "epoch": 0.644663192433893,
      "grad_norm": 10.612445831298828,
      "learning_rate": 1.677764910248987e-05,
      "loss": 3.5685,
      "step": 3340
    },
    {
      "epoch": 0.6465933217525575,
      "grad_norm": 9.556178092956543,
      "learning_rate": 1.6767998455896545e-05,
      "loss": 4.1296,
      "step": 3350
    },
    {
      "epoch": 0.6485234510712218,
      "grad_norm": 6.829568862915039,
      "learning_rate": 1.6758347809303224e-05,
      "loss": 4.103,
      "step": 3360
    },
    {
      "epoch": 0.6504535803898861,
      "grad_norm": 13.051278114318848,
      "learning_rate": 1.6748697162709902e-05,
      "loss": 3.5235,
      "step": 3370
    },
    {
      "epoch": 0.6523837097085504,
      "grad_norm": 8.569080352783203,
      "learning_rate": 1.673904651611658e-05,
      "loss": 4.2221,
      "step": 3380
    },
    {
      "epoch": 0.6543138390272148,
      "grad_norm": 9.899374008178711,
      "learning_rate": 1.672939586952326e-05,
      "loss": 3.203,
      "step": 3390
    },
    {
      "epoch": 0.6562439683458792,
      "grad_norm": 10.93232536315918,
      "learning_rate": 1.671974522292994e-05,
      "loss": 3.8278,
      "step": 3400
    },
    {
      "epoch": 0.6581740976645435,
      "grad_norm": 15.98065185546875,
      "learning_rate": 1.6710094576336614e-05,
      "loss": 2.76,
      "step": 3410
    },
    {
      "epoch": 0.6601042269832079,
      "grad_norm": 9.333832740783691,
      "learning_rate": 1.6700443929743297e-05,
      "loss": 3.506,
      "step": 3420
    },
    {
      "epoch": 0.6620343563018722,
      "grad_norm": 15.307792663574219,
      "learning_rate": 1.6690793283149972e-05,
      "loss": 3.2614,
      "step": 3430
    },
    {
      "epoch": 0.6639644856205366,
      "grad_norm": 23.258180618286133,
      "learning_rate": 1.668114263655665e-05,
      "loss": 2.9087,
      "step": 3440
    },
    {
      "epoch": 0.6658946149392009,
      "grad_norm": 11.375710487365723,
      "learning_rate": 1.667149198996333e-05,
      "loss": 3.376,
      "step": 3450
    },
    {
      "epoch": 0.6678247442578653,
      "grad_norm": 27.877683639526367,
      "learning_rate": 1.6661841343370005e-05,
      "loss": 2.9783,
      "step": 3460
    },
    {
      "epoch": 0.6697548735765296,
      "grad_norm": 24.99981117248535,
      "learning_rate": 1.6652190696776684e-05,
      "loss": 3.2014,
      "step": 3470
    },
    {
      "epoch": 0.671685002895194,
      "grad_norm": 4.930009365081787,
      "learning_rate": 1.6642540050183363e-05,
      "loss": 3.2658,
      "step": 3480
    },
    {
      "epoch": 0.6736151322138584,
      "grad_norm": 11.515230178833008,
      "learning_rate": 1.663288940359004e-05,
      "loss": 2.8784,
      "step": 3490
    },
    {
      "epoch": 0.6755452615325227,
      "grad_norm": 8.753475189208984,
      "learning_rate": 1.662323875699672e-05,
      "loss": 3.5489,
      "step": 3500
    },
    {
      "epoch": 0.677475390851187,
      "grad_norm": 9.420660018920898,
      "learning_rate": 1.66135881104034e-05,
      "loss": 3.7755,
      "step": 3510
    },
    {
      "epoch": 0.6794055201698513,
      "grad_norm": 6.9482550621032715,
      "learning_rate": 1.6603937463810075e-05,
      "loss": 3.0185,
      "step": 3520
    },
    {
      "epoch": 0.6813356494885158,
      "grad_norm": 18.775110244750977,
      "learning_rate": 1.6594286817216757e-05,
      "loss": 2.2446,
      "step": 3530
    },
    {
      "epoch": 0.6832657788071801,
      "grad_norm": 29.42607307434082,
      "learning_rate": 1.6584636170623432e-05,
      "loss": 3.7386,
      "step": 3540
    },
    {
      "epoch": 0.6851959081258444,
      "grad_norm": 36.83091354370117,
      "learning_rate": 1.657498552403011e-05,
      "loss": 3.2236,
      "step": 3550
    },
    {
      "epoch": 0.6871260374445087,
      "grad_norm": 7.353139877319336,
      "learning_rate": 1.656533487743679e-05,
      "loss": 2.8517,
      "step": 3560
    },
    {
      "epoch": 0.6890561667631732,
      "grad_norm": 36.19585418701172,
      "learning_rate": 1.655568423084347e-05,
      "loss": 3.1059,
      "step": 3570
    },
    {
      "epoch": 0.6909862960818375,
      "grad_norm": 11.92916202545166,
      "learning_rate": 1.6546033584250144e-05,
      "loss": 3.075,
      "step": 3580
    },
    {
      "epoch": 0.6929164254005018,
      "grad_norm": 17.04616355895996,
      "learning_rate": 1.6536382937656827e-05,
      "loss": 2.8869,
      "step": 3590
    },
    {
      "epoch": 0.6948465547191662,
      "grad_norm": 12.057775497436523,
      "learning_rate": 1.6526732291063502e-05,
      "loss": 3.4995,
      "step": 3600
    },
    {
      "epoch": 0.6967766840378306,
      "grad_norm": 6.13566780090332,
      "learning_rate": 1.651708164447018e-05,
      "loss": 2.9765,
      "step": 3610
    },
    {
      "epoch": 0.6987068133564949,
      "grad_norm": 39.76248550415039,
      "learning_rate": 1.650743099787686e-05,
      "loss": 3.4608,
      "step": 3620
    },
    {
      "epoch": 0.7006369426751592,
      "grad_norm": 28.71685791015625,
      "learning_rate": 1.6497780351283535e-05,
      "loss": 3.1276,
      "step": 3630
    },
    {
      "epoch": 0.7025670719938236,
      "grad_norm": 9.052225112915039,
      "learning_rate": 1.6488129704690217e-05,
      "loss": 2.8355,
      "step": 3640
    },
    {
      "epoch": 0.7044972013124879,
      "grad_norm": 3.571897506713867,
      "learning_rate": 1.6478479058096893e-05,
      "loss": 2.5702,
      "step": 3650
    },
    {
      "epoch": 0.7064273306311523,
      "grad_norm": 48.2286491394043,
      "learning_rate": 1.6468828411503572e-05,
      "loss": 2.9623,
      "step": 3660
    },
    {
      "epoch": 0.7083574599498167,
      "grad_norm": 19.87491798400879,
      "learning_rate": 1.645917776491025e-05,
      "loss": 3.4991,
      "step": 3670
    },
    {
      "epoch": 0.710287589268481,
      "grad_norm": 7.790203094482422,
      "learning_rate": 1.644952711831693e-05,
      "loss": 2.5422,
      "step": 3680
    },
    {
      "epoch": 0.7122177185871453,
      "grad_norm": 5.434534549713135,
      "learning_rate": 1.6439876471723605e-05,
      "loss": 2.9362,
      "step": 3690
    },
    {
      "epoch": 0.7141478479058097,
      "grad_norm": 16.292030334472656,
      "learning_rate": 1.6430225825130287e-05,
      "loss": 3.3782,
      "step": 3700
    },
    {
      "epoch": 0.7160779772244741,
      "grad_norm": 23.440959930419922,
      "learning_rate": 1.6420575178536962e-05,
      "loss": 2.8519,
      "step": 3710
    },
    {
      "epoch": 0.7180081065431384,
      "grad_norm": 16.322349548339844,
      "learning_rate": 1.641092453194364e-05,
      "loss": 2.5556,
      "step": 3720
    },
    {
      "epoch": 0.7199382358618027,
      "grad_norm": 56.1932373046875,
      "learning_rate": 1.640127388535032e-05,
      "loss": 3.3178,
      "step": 3730
    },
    {
      "epoch": 0.721868365180467,
      "grad_norm": 31.851116180419922,
      "learning_rate": 1.6391623238757e-05,
      "loss": 2.6495,
      "step": 3740
    },
    {
      "epoch": 0.7237984944991315,
      "grad_norm": 6.234755039215088,
      "learning_rate": 1.6381972592163678e-05,
      "loss": 3.1531,
      "step": 3750
    },
    {
      "epoch": 0.7257286238177958,
      "grad_norm": 9.04871940612793,
      "learning_rate": 1.6372321945570357e-05,
      "loss": 3.0768,
      "step": 3760
    },
    {
      "epoch": 0.7276587531364601,
      "grad_norm": 11.79047966003418,
      "learning_rate": 1.6362671298977032e-05,
      "loss": 2.5062,
      "step": 3770
    },
    {
      "epoch": 0.7295888824551245,
      "grad_norm": 60.704689025878906,
      "learning_rate": 1.635302065238371e-05,
      "loss": 3.2266,
      "step": 3780
    },
    {
      "epoch": 0.7315190117737889,
      "grad_norm": 7.449141502380371,
      "learning_rate": 1.634337000579039e-05,
      "loss": 2.8728,
      "step": 3790
    },
    {
      "epoch": 0.7334491410924532,
      "grad_norm": 9.932861328125,
      "learning_rate": 1.6333719359197065e-05,
      "loss": 2.4015,
      "step": 3800
    },
    {
      "epoch": 0.7353792704111175,
      "grad_norm": 11.80871868133545,
      "learning_rate": 1.6324068712603747e-05,
      "loss": 3.268,
      "step": 3810
    },
    {
      "epoch": 0.7373093997297819,
      "grad_norm": 14.598430633544922,
      "learning_rate": 1.6314418066010423e-05,
      "loss": 2.8383,
      "step": 3820
    },
    {
      "epoch": 0.7392395290484463,
      "grad_norm": 8.98936939239502,
      "learning_rate": 1.6304767419417102e-05,
      "loss": 3.2928,
      "step": 3830
    },
    {
      "epoch": 0.7411696583671106,
      "grad_norm": 5.610025882720947,
      "learning_rate": 1.629511677282378e-05,
      "loss": 2.4269,
      "step": 3840
    },
    {
      "epoch": 0.7430997876857749,
      "grad_norm": 6.6231303215026855,
      "learning_rate": 1.628546612623046e-05,
      "loss": 2.7384,
      "step": 3850
    },
    {
      "epoch": 0.7450299170044393,
      "grad_norm": 9.609184265136719,
      "learning_rate": 1.6275815479637135e-05,
      "loss": 3.0373,
      "step": 3860
    },
    {
      "epoch": 0.7469600463231036,
      "grad_norm": 18.07203483581543,
      "learning_rate": 1.6266164833043817e-05,
      "loss": 3.5384,
      "step": 3870
    },
    {
      "epoch": 0.748890175641768,
      "grad_norm": 17.57476234436035,
      "learning_rate": 1.6256514186450492e-05,
      "loss": 3.2555,
      "step": 3880
    },
    {
      "epoch": 0.7508203049604324,
      "grad_norm": 18.90932273864746,
      "learning_rate": 1.624686353985717e-05,
      "loss": 3.4571,
      "step": 3890
    },
    {
      "epoch": 0.7527504342790967,
      "grad_norm": 6.273112773895264,
      "learning_rate": 1.623721289326385e-05,
      "loss": 2.8663,
      "step": 3900
    },
    {
      "epoch": 0.754680563597761,
      "grad_norm": 6.111484050750732,
      "learning_rate": 1.622756224667053e-05,
      "loss": 2.9198,
      "step": 3910
    },
    {
      "epoch": 0.7566106929164254,
      "grad_norm": 7.073675632476807,
      "learning_rate": 1.6217911600077208e-05,
      "loss": 2.7048,
      "step": 3920
    },
    {
      "epoch": 0.7585408222350898,
      "grad_norm": 16.904712677001953,
      "learning_rate": 1.6208260953483887e-05,
      "loss": 2.5611,
      "step": 3930
    },
    {
      "epoch": 0.7604709515537541,
      "grad_norm": 8.962367057800293,
      "learning_rate": 1.6198610306890562e-05,
      "loss": 2.7735,
      "step": 3940
    },
    {
      "epoch": 0.7624010808724184,
      "grad_norm": 6.822296619415283,
      "learning_rate": 1.618895966029724e-05,
      "loss": 2.5074,
      "step": 3950
    },
    {
      "epoch": 0.7643312101910829,
      "grad_norm": 14.453258514404297,
      "learning_rate": 1.617930901370392e-05,
      "loss": 2.5127,
      "step": 3960
    },
    {
      "epoch": 0.7662613395097472,
      "grad_norm": 10.286723136901855,
      "learning_rate": 1.6169658367110595e-05,
      "loss": 3.1512,
      "step": 3970
    },
    {
      "epoch": 0.7681914688284115,
      "grad_norm": 10.355208396911621,
      "learning_rate": 1.6160007720517277e-05,
      "loss": 2.233,
      "step": 3980
    },
    {
      "epoch": 0.7701215981470758,
      "grad_norm": 33.669761657714844,
      "learning_rate": 1.6150357073923956e-05,
      "loss": 2.8695,
      "step": 3990
    },
    {
      "epoch": 0.7720517274657402,
      "grad_norm": 14.773234367370605,
      "learning_rate": 1.6140706427330632e-05,
      "loss": 2.8215,
      "step": 4000
    },
    {
      "epoch": 0.7739818567844046,
      "grad_norm": 7.062147617340088,
      "learning_rate": 1.613105578073731e-05,
      "loss": 2.4659,
      "step": 4010
    },
    {
      "epoch": 0.7759119861030689,
      "grad_norm": 12.025416374206543,
      "learning_rate": 1.612140513414399e-05,
      "loss": 2.9964,
      "step": 4020
    },
    {
      "epoch": 0.7778421154217332,
      "grad_norm": 5.7577104568481445,
      "learning_rate": 1.6111754487550668e-05,
      "loss": 3.0994,
      "step": 4030
    },
    {
      "epoch": 0.7797722447403976,
      "grad_norm": 6.214175701141357,
      "learning_rate": 1.6102103840957347e-05,
      "loss": 2.5877,
      "step": 4040
    },
    {
      "epoch": 0.781702374059062,
      "grad_norm": 10.123525619506836,
      "learning_rate": 1.6092453194364022e-05,
      "loss": 2.9551,
      "step": 4050
    },
    {
      "epoch": 0.7836325033777263,
      "grad_norm": 15.049646377563477,
      "learning_rate": 1.60828025477707e-05,
      "loss": 2.4678,
      "step": 4060
    },
    {
      "epoch": 0.7855626326963907,
      "grad_norm": 4.4600419998168945,
      "learning_rate": 1.607315190117738e-05,
      "loss": 2.4951,
      "step": 4070
    },
    {
      "epoch": 0.787492762015055,
      "grad_norm": 14.181716918945312,
      "learning_rate": 1.606350125458406e-05,
      "loss": 3.019,
      "step": 4080
    },
    {
      "epoch": 0.7894228913337193,
      "grad_norm": 10.2423095703125,
      "learning_rate": 1.6053850607990738e-05,
      "loss": 2.8194,
      "step": 4090
    },
    {
      "epoch": 0.7913530206523837,
      "grad_norm": 4.4921674728393555,
      "learning_rate": 1.6044199961397417e-05,
      "loss": 3.2749,
      "step": 4100
    },
    {
      "epoch": 0.7932831499710481,
      "grad_norm": 9.099245071411133,
      "learning_rate": 1.6034549314804092e-05,
      "loss": 2.7981,
      "step": 4110
    },
    {
      "epoch": 0.7952132792897124,
      "grad_norm": 9.286816596984863,
      "learning_rate": 1.602489866821077e-05,
      "loss": 2.8048,
      "step": 4120
    },
    {
      "epoch": 0.7971434086083767,
      "grad_norm": 9.841703414916992,
      "learning_rate": 1.601524802161745e-05,
      "loss": 2.6876,
      "step": 4130
    },
    {
      "epoch": 0.7990735379270412,
      "grad_norm": 8.871543884277344,
      "learning_rate": 1.600559737502413e-05,
      "loss": 2.4337,
      "step": 4140
    },
    {
      "epoch": 0.8010036672457055,
      "grad_norm": 30.344938278198242,
      "learning_rate": 1.5995946728430807e-05,
      "loss": 3.2361,
      "step": 4150
    },
    {
      "epoch": 0.8029337965643698,
      "grad_norm": 6.406156539916992,
      "learning_rate": 1.5986296081837486e-05,
      "loss": 2.4589,
      "step": 4160
    },
    {
      "epoch": 0.8048639258830341,
      "grad_norm": 10.664615631103516,
      "learning_rate": 1.5976645435244162e-05,
      "loss": 2.7194,
      "step": 4170
    },
    {
      "epoch": 0.8067940552016986,
      "grad_norm": 3.811953067779541,
      "learning_rate": 1.596699478865084e-05,
      "loss": 2.6486,
      "step": 4180
    },
    {
      "epoch": 0.8087241845203629,
      "grad_norm": 15.862503051757812,
      "learning_rate": 1.595734414205752e-05,
      "loss": 2.8283,
      "step": 4190
    },
    {
      "epoch": 0.8106543138390272,
      "grad_norm": 4.255658149719238,
      "learning_rate": 1.5947693495464198e-05,
      "loss": 3.0282,
      "step": 4200
    },
    {
      "epoch": 0.8125844431576915,
      "grad_norm": 5.366822242736816,
      "learning_rate": 1.5938042848870877e-05,
      "loss": 2.9807,
      "step": 4210
    },
    {
      "epoch": 0.814514572476356,
      "grad_norm": 33.69711685180664,
      "learning_rate": 1.5928392202277552e-05,
      "loss": 2.2526,
      "step": 4220
    },
    {
      "epoch": 0.8164447017950203,
      "grad_norm": 14.70229721069336,
      "learning_rate": 1.591874155568423e-05,
      "loss": 3.6708,
      "step": 4230
    },
    {
      "epoch": 0.8183748311136846,
      "grad_norm": 13.929484367370605,
      "learning_rate": 1.590909090909091e-05,
      "loss": 2.9418,
      "step": 4240
    },
    {
      "epoch": 0.820304960432349,
      "grad_norm": 15.553504943847656,
      "learning_rate": 1.589944026249759e-05,
      "loss": 2.3594,
      "step": 4250
    },
    {
      "epoch": 0.8222350897510133,
      "grad_norm": 30.276248931884766,
      "learning_rate": 1.5889789615904268e-05,
      "loss": 2.4775,
      "step": 4260
    },
    {
      "epoch": 0.8241652190696777,
      "grad_norm": 6.89615535736084,
      "learning_rate": 1.5880138969310947e-05,
      "loss": 2.2871,
      "step": 4270
    },
    {
      "epoch": 0.826095348388342,
      "grad_norm": 8.340616226196289,
      "learning_rate": 1.5870488322717622e-05,
      "loss": 2.9781,
      "step": 4280
    },
    {
      "epoch": 0.8280254777070064,
      "grad_norm": 38.06652069091797,
      "learning_rate": 1.58608376761243e-05,
      "loss": 3.1489,
      "step": 4290
    },
    {
      "epoch": 0.8299556070256707,
      "grad_norm": 27.9891414642334,
      "learning_rate": 1.585118702953098e-05,
      "loss": 2.8673,
      "step": 4300
    },
    {
      "epoch": 0.831885736344335,
      "grad_norm": 8.713174819946289,
      "learning_rate": 1.584153638293766e-05,
      "loss": 2.6419,
      "step": 4310
    },
    {
      "epoch": 0.8338158656629994,
      "grad_norm": 44.78571701049805,
      "learning_rate": 1.5831885736344337e-05,
      "loss": 2.1454,
      "step": 4320
    },
    {
      "epoch": 0.8357459949816638,
      "grad_norm": 5.82949686050415,
      "learning_rate": 1.5822235089751016e-05,
      "loss": 2.8967,
      "step": 4330
    },
    {
      "epoch": 0.8376761243003281,
      "grad_norm": 6.723079681396484,
      "learning_rate": 1.5812584443157692e-05,
      "loss": 3.0934,
      "step": 4340
    },
    {
      "epoch": 0.8396062536189924,
      "grad_norm": 5.693741321563721,
      "learning_rate": 1.580293379656437e-05,
      "loss": 2.835,
      "step": 4350
    },
    {
      "epoch": 0.8415363829376569,
      "grad_norm": 7.481176853179932,
      "learning_rate": 1.579328314997105e-05,
      "loss": 2.6048,
      "step": 4360
    },
    {
      "epoch": 0.8434665122563212,
      "grad_norm": 6.205816268920898,
      "learning_rate": 1.5783632503377728e-05,
      "loss": 3.2227,
      "step": 4370
    },
    {
      "epoch": 0.8453966415749855,
      "grad_norm": 64.9307632446289,
      "learning_rate": 1.5773981856784407e-05,
      "loss": 2.6881,
      "step": 4380
    },
    {
      "epoch": 0.8473267708936498,
      "grad_norm": 34.071102142333984,
      "learning_rate": 1.5764331210191083e-05,
      "loss": 2.985,
      "step": 4390
    },
    {
      "epoch": 0.8492569002123143,
      "grad_norm": 57.71902084350586,
      "learning_rate": 1.575468056359776e-05,
      "loss": 2.1552,
      "step": 4400
    },
    {
      "epoch": 0.8511870295309786,
      "grad_norm": 25.113494873046875,
      "learning_rate": 1.574502991700444e-05,
      "loss": 2.9929,
      "step": 4410
    },
    {
      "epoch": 0.8531171588496429,
      "grad_norm": 11.44904613494873,
      "learning_rate": 1.573537927041112e-05,
      "loss": 1.4603,
      "step": 4420
    },
    {
      "epoch": 0.8550472881683073,
      "grad_norm": 16.231115341186523,
      "learning_rate": 1.5725728623817798e-05,
      "loss": 3.1473,
      "step": 4430
    },
    {
      "epoch": 0.8569774174869716,
      "grad_norm": 14.562348365783691,
      "learning_rate": 1.5716077977224477e-05,
      "loss": 2.6993,
      "step": 4440
    },
    {
      "epoch": 0.858907546805636,
      "grad_norm": 4.435051441192627,
      "learning_rate": 1.5706427330631152e-05,
      "loss": 2.4842,
      "step": 4450
    },
    {
      "epoch": 0.8608376761243003,
      "grad_norm": 16.04781723022461,
      "learning_rate": 1.569677668403783e-05,
      "loss": 2.6751,
      "step": 4460
    },
    {
      "epoch": 0.8627678054429647,
      "grad_norm": 29.1357364654541,
      "learning_rate": 1.568712603744451e-05,
      "loss": 2.5052,
      "step": 4470
    },
    {
      "epoch": 0.864697934761629,
      "grad_norm": 21.93231773376465,
      "learning_rate": 1.567747539085119e-05,
      "loss": 2.178,
      "step": 4480
    },
    {
      "epoch": 0.8666280640802934,
      "grad_norm": 5.966254234313965,
      "learning_rate": 1.5667824744257867e-05,
      "loss": 2.6496,
      "step": 4490
    },
    {
      "epoch": 0.8685581933989577,
      "grad_norm": 12.111167907714844,
      "learning_rate": 1.5658174097664546e-05,
      "loss": 2.2818,
      "step": 4500
    },
    {
      "epoch": 0.8704883227176221,
      "grad_norm": 5.266273498535156,
      "learning_rate": 1.5648523451071222e-05,
      "loss": 2.9602,
      "step": 4510
    },
    {
      "epoch": 0.8724184520362864,
      "grad_norm": 17.854291915893555,
      "learning_rate": 1.56388728044779e-05,
      "loss": 2.5116,
      "step": 4520
    },
    {
      "epoch": 0.8743485813549507,
      "grad_norm": 7.432783126831055,
      "learning_rate": 1.562922215788458e-05,
      "loss": 2.812,
      "step": 4530
    },
    {
      "epoch": 0.8762787106736152,
      "grad_norm": 18.3514347076416,
      "learning_rate": 1.5619571511291258e-05,
      "loss": 2.5119,
      "step": 4540
    },
    {
      "epoch": 0.8782088399922795,
      "grad_norm": 7.753026962280273,
      "learning_rate": 1.5609920864697937e-05,
      "loss": 2.1341,
      "step": 4550
    },
    {
      "epoch": 0.8801389693109438,
      "grad_norm": 10.822052001953125,
      "learning_rate": 1.5600270218104613e-05,
      "loss": 3.1707,
      "step": 4560
    },
    {
      "epoch": 0.8820690986296081,
      "grad_norm": 5.203958511352539,
      "learning_rate": 1.559061957151129e-05,
      "loss": 2.8462,
      "step": 4570
    },
    {
      "epoch": 0.8839992279482726,
      "grad_norm": 66.40520477294922,
      "learning_rate": 1.558096892491797e-05,
      "loss": 2.4365,
      "step": 4580
    },
    {
      "epoch": 0.8859293572669369,
      "grad_norm": 8.648571968078613,
      "learning_rate": 1.557131827832465e-05,
      "loss": 2.9169,
      "step": 4590
    },
    {
      "epoch": 0.8878594865856012,
      "grad_norm": 8.056113243103027,
      "learning_rate": 1.5561667631731328e-05,
      "loss": 2.3648,
      "step": 4600
    },
    {
      "epoch": 0.8897896159042656,
      "grad_norm": 17.29505157470703,
      "learning_rate": 1.5552016985138007e-05,
      "loss": 2.0122,
      "step": 4610
    },
    {
      "epoch": 0.89171974522293,
      "grad_norm": 15.502542495727539,
      "learning_rate": 1.5542366338544682e-05,
      "loss": 2.4878,
      "step": 4620
    },
    {
      "epoch": 0.8936498745415943,
      "grad_norm": 12.300379753112793,
      "learning_rate": 1.553271569195136e-05,
      "loss": 2.3212,
      "step": 4630
    },
    {
      "epoch": 0.8955800038602586,
      "grad_norm": 31.183977127075195,
      "learning_rate": 1.552306504535804e-05,
      "loss": 2.7029,
      "step": 4640
    },
    {
      "epoch": 0.897510133178923,
      "grad_norm": 12.948437690734863,
      "learning_rate": 1.551341439876472e-05,
      "loss": 3.2897,
      "step": 4650
    },
    {
      "epoch": 0.8994402624975874,
      "grad_norm": 7.433976650238037,
      "learning_rate": 1.5503763752171397e-05,
      "loss": 2.1371,
      "step": 4660
    },
    {
      "epoch": 0.9013703918162517,
      "grad_norm": 3.3433847427368164,
      "learning_rate": 1.5494113105578076e-05,
      "loss": 2.295,
      "step": 4670
    },
    {
      "epoch": 0.903300521134916,
      "grad_norm": 42.16172409057617,
      "learning_rate": 1.5484462458984752e-05,
      "loss": 2.2732,
      "step": 4680
    },
    {
      "epoch": 0.9052306504535804,
      "grad_norm": 14.819722175598145,
      "learning_rate": 1.5474811812391434e-05,
      "loss": 2.2727,
      "step": 4690
    },
    {
      "epoch": 0.9071607797722447,
      "grad_norm": 20.647216796875,
      "learning_rate": 1.546516116579811e-05,
      "loss": 2.5579,
      "step": 4700
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 7.544191837310791,
      "learning_rate": 1.5455510519204788e-05,
      "loss": 2.0707,
      "step": 4710
    },
    {
      "epoch": 0.9110210384095735,
      "grad_norm": 8.134742736816406,
      "learning_rate": 1.5445859872611467e-05,
      "loss": 2.5737,
      "step": 4720
    },
    {
      "epoch": 0.9129511677282378,
      "grad_norm": 6.242347717285156,
      "learning_rate": 1.5436209226018143e-05,
      "loss": 2.3069,
      "step": 4730
    },
    {
      "epoch": 0.9148812970469021,
      "grad_norm": 8.6797513961792,
      "learning_rate": 1.542655857942482e-05,
      "loss": 2.2833,
      "step": 4740
    },
    {
      "epoch": 0.9168114263655665,
      "grad_norm": 5.413093566894531,
      "learning_rate": 1.5416907932831504e-05,
      "loss": 2.8315,
      "step": 4750
    },
    {
      "epoch": 0.9187415556842309,
      "grad_norm": 11.878490447998047,
      "learning_rate": 1.540725728623818e-05,
      "loss": 3.3762,
      "step": 4760
    },
    {
      "epoch": 0.9206716850028952,
      "grad_norm": 4.7884907722473145,
      "learning_rate": 1.5397606639644858e-05,
      "loss": 2.3594,
      "step": 4770
    },
    {
      "epoch": 0.9226018143215595,
      "grad_norm": 35.840110778808594,
      "learning_rate": 1.5387955993051537e-05,
      "loss": 2.1176,
      "step": 4780
    },
    {
      "epoch": 0.9245319436402238,
      "grad_norm": 5.872596263885498,
      "learning_rate": 1.5378305346458212e-05,
      "loss": 2.4819,
      "step": 4790
    },
    {
      "epoch": 0.9264620729588883,
      "grad_norm": 52.29739761352539,
      "learning_rate": 1.536865469986489e-05,
      "loss": 1.629,
      "step": 4800
    },
    {
      "epoch": 0.9283922022775526,
      "grad_norm": 21.178890228271484,
      "learning_rate": 1.535900405327157e-05,
      "loss": 2.708,
      "step": 4810
    },
    {
      "epoch": 0.9303223315962169,
      "grad_norm": 33.205204010009766,
      "learning_rate": 1.534935340667825e-05,
      "loss": 2.4839,
      "step": 4820
    },
    {
      "epoch": 0.9322524609148813,
      "grad_norm": 11.207283973693848,
      "learning_rate": 1.5339702760084927e-05,
      "loss": 1.9878,
      "step": 4830
    },
    {
      "epoch": 0.9341825902335457,
      "grad_norm": 18.431156158447266,
      "learning_rate": 1.5330052113491606e-05,
      "loss": 2.4734,
      "step": 4840
    },
    {
      "epoch": 0.93611271955221,
      "grad_norm": 6.967078685760498,
      "learning_rate": 1.5320401466898282e-05,
      "loss": 3.2588,
      "step": 4850
    },
    {
      "epoch": 0.9380428488708743,
      "grad_norm": 7.856616973876953,
      "learning_rate": 1.5310750820304964e-05,
      "loss": 1.7946,
      "step": 4860
    },
    {
      "epoch": 0.9399729781895387,
      "grad_norm": 7.094188213348389,
      "learning_rate": 1.530110017371164e-05,
      "loss": 1.5447,
      "step": 4870
    },
    {
      "epoch": 0.941903107508203,
      "grad_norm": 45.888816833496094,
      "learning_rate": 1.5291449527118318e-05,
      "loss": 2.428,
      "step": 4880
    },
    {
      "epoch": 0.9438332368268674,
      "grad_norm": 5.166431903839111,
      "learning_rate": 1.5281798880524997e-05,
      "loss": 1.879,
      "step": 4890
    },
    {
      "epoch": 0.9457633661455318,
      "grad_norm": 6.451969623565674,
      "learning_rate": 1.5272148233931673e-05,
      "loss": 2.1147,
      "step": 4900
    },
    {
      "epoch": 0.9476934954641961,
      "grad_norm": 8.150400161743164,
      "learning_rate": 1.526249758733835e-05,
      "loss": 2.6242,
      "step": 4910
    },
    {
      "epoch": 0.9496236247828604,
      "grad_norm": 10.228230476379395,
      "learning_rate": 1.5252846940745032e-05,
      "loss": 1.9209,
      "step": 4920
    },
    {
      "epoch": 0.9515537541015248,
      "grad_norm": 22.90669822692871,
      "learning_rate": 1.5243196294151709e-05,
      "loss": 2.9715,
      "step": 4930
    },
    {
      "epoch": 0.9534838834201892,
      "grad_norm": 14.811582565307617,
      "learning_rate": 1.5233545647558388e-05,
      "loss": 2.353,
      "step": 4940
    },
    {
      "epoch": 0.9554140127388535,
      "grad_norm": 8.638494491577148,
      "learning_rate": 1.5223895000965065e-05,
      "loss": 3.1177,
      "step": 4950
    },
    {
      "epoch": 0.9573441420575178,
      "grad_norm": 37.17524719238281,
      "learning_rate": 1.5214244354371744e-05,
      "loss": 2.835,
      "step": 4960
    },
    {
      "epoch": 0.9592742713761822,
      "grad_norm": 16.99079704284668,
      "learning_rate": 1.5204593707778423e-05,
      "loss": 2.263,
      "step": 4970
    },
    {
      "epoch": 0.9612044006948466,
      "grad_norm": 61.47795486450195,
      "learning_rate": 1.5194943061185101e-05,
      "loss": 2.5372,
      "step": 4980
    },
    {
      "epoch": 0.9631345300135109,
      "grad_norm": 9.553987503051758,
      "learning_rate": 1.5185292414591779e-05,
      "loss": 2.402,
      "step": 4990
    },
    {
      "epoch": 0.9650646593321752,
      "grad_norm": 7.673786640167236,
      "learning_rate": 1.5175641767998457e-05,
      "loss": 2.2453,
      "step": 5000
    },
    {
      "epoch": 0.9669947886508397,
      "grad_norm": 4.662721157073975,
      "learning_rate": 1.5165991121405135e-05,
      "loss": 1.8894,
      "step": 5010
    },
    {
      "epoch": 0.968924917969504,
      "grad_norm": 6.425994396209717,
      "learning_rate": 1.5156340474811812e-05,
      "loss": 2.2984,
      "step": 5020
    },
    {
      "epoch": 0.9708550472881683,
      "grad_norm": 47.678627014160156,
      "learning_rate": 1.5146689828218492e-05,
      "loss": 2.4425,
      "step": 5030
    },
    {
      "epoch": 0.9727851766068326,
      "grad_norm": 31.538537979125977,
      "learning_rate": 1.5137039181625171e-05,
      "loss": 2.2842,
      "step": 5040
    },
    {
      "epoch": 0.974715305925497,
      "grad_norm": 20.270408630371094,
      "learning_rate": 1.5127388535031848e-05,
      "loss": 3.0377,
      "step": 5050
    },
    {
      "epoch": 0.9766454352441614,
      "grad_norm": 14.128602027893066,
      "learning_rate": 1.5117737888438525e-05,
      "loss": 2.8543,
      "step": 5060
    },
    {
      "epoch": 0.9785755645628257,
      "grad_norm": 9.722307205200195,
      "learning_rate": 1.5108087241845204e-05,
      "loss": 2.2329,
      "step": 5070
    },
    {
      "epoch": 0.9805056938814901,
      "grad_norm": 41.6328239440918,
      "learning_rate": 1.5098436595251885e-05,
      "loss": 2.4739,
      "step": 5080
    },
    {
      "epoch": 0.9824358232001544,
      "grad_norm": 3.035896062850952,
      "learning_rate": 1.5088785948658562e-05,
      "loss": 2.267,
      "step": 5090
    },
    {
      "epoch": 0.9843659525188188,
      "grad_norm": 31.043338775634766,
      "learning_rate": 1.5079135302065239e-05,
      "loss": 2.4718,
      "step": 5100
    },
    {
      "epoch": 0.9862960818374831,
      "grad_norm": 12.695929527282715,
      "learning_rate": 1.5069484655471918e-05,
      "loss": 1.7744,
      "step": 5110
    },
    {
      "epoch": 0.9882262111561475,
      "grad_norm": 4.221500873565674,
      "learning_rate": 1.5059834008878595e-05,
      "loss": 1.9522,
      "step": 5120
    },
    {
      "epoch": 0.9901563404748118,
      "grad_norm": 5.659612655639648,
      "learning_rate": 1.5050183362285274e-05,
      "loss": 2.736,
      "step": 5130
    },
    {
      "epoch": 0.9920864697934761,
      "grad_norm": 11.953378677368164,
      "learning_rate": 1.5040532715691953e-05,
      "loss": 2.3559,
      "step": 5140
    },
    {
      "epoch": 0.9940165991121405,
      "grad_norm": 10.714877128601074,
      "learning_rate": 1.5030882069098632e-05,
      "loss": 1.745,
      "step": 5150
    },
    {
      "epoch": 0.9959467284308049,
      "grad_norm": 83.76753234863281,
      "learning_rate": 1.5021231422505309e-05,
      "loss": 3.5231,
      "step": 5160
    },
    {
      "epoch": 0.9978768577494692,
      "grad_norm": 43.38895034790039,
      "learning_rate": 1.5011580775911987e-05,
      "loss": 2.6962,
      "step": 5170
    },
    {
      "epoch": 0.9998069870681335,
      "grad_norm": 31.09933090209961,
      "learning_rate": 1.5001930129318665e-05,
      "loss": 2.0881,
      "step": 5180
    },
    {
      "epoch": 1.0,
      "eval_intent_acc": 0.6935763888888888,
      "eval_loss": 2.365294933319092,
      "eval_runtime": 22.7561,
      "eval_samples_per_second": 50.624,
      "eval_slot_f1": 0.4765166340508807,
      "eval_slot_precision": 0.5369349503858876,
      "eval_slot_recall": 0.4283201407211961,
      "eval_steps_per_second": 25.312,
      "step": 5181
    },
    {
      "epoch": 1.0017371163867979,
      "grad_norm": 6.236470699310303,
      "learning_rate": 1.4992279482725345e-05,
      "loss": 2.3186,
      "step": 5190
    },
    {
      "epoch": 1.0036672457054623,
      "grad_norm": 5.8155999183654785,
      "learning_rate": 1.4982628836132022e-05,
      "loss": 2.0252,
      "step": 5200
    },
    {
      "epoch": 1.0055973750241267,
      "grad_norm": 9.022046089172363,
      "learning_rate": 1.4972978189538701e-05,
      "loss": 1.8783,
      "step": 5210
    },
    {
      "epoch": 1.007527504342791,
      "grad_norm": 12.69614315032959,
      "learning_rate": 1.4963327542945378e-05,
      "loss": 2.5868,
      "step": 5220
    },
    {
      "epoch": 1.0094576336614554,
      "grad_norm": 1.7923614978790283,
      "learning_rate": 1.4953676896352055e-05,
      "loss": 2.2247,
      "step": 5230
    },
    {
      "epoch": 1.0113877629801196,
      "grad_norm": 8.562881469726562,
      "learning_rate": 1.4944026249758734e-05,
      "loss": 2.2584,
      "step": 5240
    },
    {
      "epoch": 1.013317892298784,
      "grad_norm": 5.225815296173096,
      "learning_rate": 1.4934375603165415e-05,
      "loss": 2.6741,
      "step": 5250
    },
    {
      "epoch": 1.0152480216174484,
      "grad_norm": 20.292545318603516,
      "learning_rate": 1.4924724956572092e-05,
      "loss": 1.8323,
      "step": 5260
    },
    {
      "epoch": 1.0171781509361126,
      "grad_norm": 6.0288004875183105,
      "learning_rate": 1.4915074309978769e-05,
      "loss": 1.6774,
      "step": 5270
    },
    {
      "epoch": 1.019108280254777,
      "grad_norm": 3.861860513687134,
      "learning_rate": 1.4905423663385448e-05,
      "loss": 1.9934,
      "step": 5280
    },
    {
      "epoch": 1.0210384095734415,
      "grad_norm": 9.108674049377441,
      "learning_rate": 1.4895773016792125e-05,
      "loss": 1.8111,
      "step": 5290
    },
    {
      "epoch": 1.0229685388921057,
      "grad_norm": 6.372337818145752,
      "learning_rate": 1.4886122370198806e-05,
      "loss": 2.3063,
      "step": 5300
    },
    {
      "epoch": 1.0248986682107701,
      "grad_norm": 9.138615608215332,
      "learning_rate": 1.4876471723605483e-05,
      "loss": 1.9247,
      "step": 5310
    },
    {
      "epoch": 1.0268287975294346,
      "grad_norm": 16.900671005249023,
      "learning_rate": 1.4866821077012162e-05,
      "loss": 2.5184,
      "step": 5320
    },
    {
      "epoch": 1.0287589268480988,
      "grad_norm": 5.9734086990356445,
      "learning_rate": 1.4857170430418839e-05,
      "loss": 2.6392,
      "step": 5330
    },
    {
      "epoch": 1.0306890561667632,
      "grad_norm": 7.141120910644531,
      "learning_rate": 1.4847519783825517e-05,
      "loss": 2.1215,
      "step": 5340
    },
    {
      "epoch": 1.0326191854854274,
      "grad_norm": 24.076833724975586,
      "learning_rate": 1.4837869137232195e-05,
      "loss": 2.3792,
      "step": 5350
    },
    {
      "epoch": 1.0345493148040918,
      "grad_norm": 16.64017677307129,
      "learning_rate": 1.4828218490638875e-05,
      "loss": 3.1343,
      "step": 5360
    },
    {
      "epoch": 1.0364794441227563,
      "grad_norm": 5.445941925048828,
      "learning_rate": 1.4818567844045552e-05,
      "loss": 1.9378,
      "step": 5370
    },
    {
      "epoch": 1.0384095734414205,
      "grad_norm": 24.48090934753418,
      "learning_rate": 1.4808917197452231e-05,
      "loss": 1.4665,
      "step": 5380
    },
    {
      "epoch": 1.040339702760085,
      "grad_norm": 15.948715209960938,
      "learning_rate": 1.4799266550858908e-05,
      "loss": 2.1433,
      "step": 5390
    },
    {
      "epoch": 1.0422698320787493,
      "grad_norm": 22.902328491210938,
      "learning_rate": 1.4789615904265585e-05,
      "loss": 2.8298,
      "step": 5400
    },
    {
      "epoch": 1.0441999613974136,
      "grad_norm": 5.638338565826416,
      "learning_rate": 1.4779965257672266e-05,
      "loss": 2.2033,
      "step": 5410
    },
    {
      "epoch": 1.046130090716078,
      "grad_norm": 40.1965217590332,
      "learning_rate": 1.4770314611078945e-05,
      "loss": 2.1068,
      "step": 5420
    },
    {
      "epoch": 1.0480602200347424,
      "grad_norm": 6.504009246826172,
      "learning_rate": 1.4760663964485622e-05,
      "loss": 1.8406,
      "step": 5430
    },
    {
      "epoch": 1.0499903493534066,
      "grad_norm": 19.00457763671875,
      "learning_rate": 1.4751013317892299e-05,
      "loss": 2.065,
      "step": 5440
    },
    {
      "epoch": 1.051920478672071,
      "grad_norm": 21.741750717163086,
      "learning_rate": 1.4741362671298978e-05,
      "loss": 1.7829,
      "step": 5450
    },
    {
      "epoch": 1.0538506079907353,
      "grad_norm": 2.669718027114868,
      "learning_rate": 1.4731712024705655e-05,
      "loss": 2.0302,
      "step": 5460
    },
    {
      "epoch": 1.0557807373093997,
      "grad_norm": 7.064241886138916,
      "learning_rate": 1.4722061378112336e-05,
      "loss": 2.3714,
      "step": 5470
    },
    {
      "epoch": 1.0577108666280641,
      "grad_norm": 8.222122192382812,
      "learning_rate": 1.4712410731519013e-05,
      "loss": 2.1933,
      "step": 5480
    },
    {
      "epoch": 1.0596409959467283,
      "grad_norm": 10.906907081604004,
      "learning_rate": 1.4702760084925692e-05,
      "loss": 2.3655,
      "step": 5490
    },
    {
      "epoch": 1.0615711252653928,
      "grad_norm": 6.9397501945495605,
      "learning_rate": 1.4693109438332369e-05,
      "loss": 2.1571,
      "step": 5500
    },
    {
      "epoch": 1.0635012545840572,
      "grad_norm": 2.485079765319824,
      "learning_rate": 1.4683458791739047e-05,
      "loss": 1.351,
      "step": 5510
    },
    {
      "epoch": 1.0654313839027214,
      "grad_norm": 6.4660868644714355,
      "learning_rate": 1.4673808145145726e-05,
      "loss": 1.8499,
      "step": 5520
    },
    {
      "epoch": 1.0673615132213858,
      "grad_norm": 10.87532901763916,
      "learning_rate": 1.4664157498552405e-05,
      "loss": 2.5111,
      "step": 5530
    },
    {
      "epoch": 1.0692916425400503,
      "grad_norm": 6.294435977935791,
      "learning_rate": 1.4654506851959082e-05,
      "loss": 2.0142,
      "step": 5540
    },
    {
      "epoch": 1.0712217718587145,
      "grad_norm": 32.65982437133789,
      "learning_rate": 1.4644856205365761e-05,
      "loss": 1.758,
      "step": 5550
    },
    {
      "epoch": 1.073151901177379,
      "grad_norm": 9.29031753540039,
      "learning_rate": 1.4635205558772438e-05,
      "loss": 2.0867,
      "step": 5560
    },
    {
      "epoch": 1.0750820304960431,
      "grad_norm": 36.29301071166992,
      "learning_rate": 1.4625554912179117e-05,
      "loss": 1.5204,
      "step": 5570
    },
    {
      "epoch": 1.0770121598147075,
      "grad_norm": 4.362534999847412,
      "learning_rate": 1.4615904265585796e-05,
      "loss": 1.9261,
      "step": 5580
    },
    {
      "epoch": 1.078942289133372,
      "grad_norm": 6.152461528778076,
      "learning_rate": 1.4606253618992475e-05,
      "loss": 2.0342,
      "step": 5590
    },
    {
      "epoch": 1.0808724184520362,
      "grad_norm": 6.619088649749756,
      "learning_rate": 1.4596602972399152e-05,
      "loss": 2.2747,
      "step": 5600
    },
    {
      "epoch": 1.0828025477707006,
      "grad_norm": 3.85827374458313,
      "learning_rate": 1.4586952325805829e-05,
      "loss": 2.3346,
      "step": 5610
    },
    {
      "epoch": 1.084732677089365,
      "grad_norm": 3.0161590576171875,
      "learning_rate": 1.4577301679212508e-05,
      "loss": 2.3708,
      "step": 5620
    },
    {
      "epoch": 1.0866628064080293,
      "grad_norm": 4.229267597198486,
      "learning_rate": 1.4567651032619188e-05,
      "loss": 1.6199,
      "step": 5630
    },
    {
      "epoch": 1.0885929357266937,
      "grad_norm": 40.51176834106445,
      "learning_rate": 1.4558000386025866e-05,
      "loss": 1.8886,
      "step": 5640
    },
    {
      "epoch": 1.0905230650453581,
      "grad_norm": 2.7953546047210693,
      "learning_rate": 1.4548349739432543e-05,
      "loss": 1.9042,
      "step": 5650
    },
    {
      "epoch": 1.0924531943640223,
      "grad_norm": 10.74968147277832,
      "learning_rate": 1.4538699092839222e-05,
      "loss": 2.7835,
      "step": 5660
    },
    {
      "epoch": 1.0943833236826868,
      "grad_norm": 14.43886947631836,
      "learning_rate": 1.4529048446245899e-05,
      "loss": 1.6602,
      "step": 5670
    },
    {
      "epoch": 1.0963134530013512,
      "grad_norm": 10.07282543182373,
      "learning_rate": 1.4519397799652577e-05,
      "loss": 2.1119,
      "step": 5680
    },
    {
      "epoch": 1.0982435823200154,
      "grad_norm": 5.278014659881592,
      "learning_rate": 1.4509747153059256e-05,
      "loss": 2.3563,
      "step": 5690
    },
    {
      "epoch": 1.1001737116386798,
      "grad_norm": 8.340392112731934,
      "learning_rate": 1.4500096506465935e-05,
      "loss": 2.2711,
      "step": 5700
    },
    {
      "epoch": 1.102103840957344,
      "grad_norm": 5.538665294647217,
      "learning_rate": 1.4490445859872612e-05,
      "loss": 1.8006,
      "step": 5710
    },
    {
      "epoch": 1.1040339702760085,
      "grad_norm": 1.950091004371643,
      "learning_rate": 1.4480795213279291e-05,
      "loss": 1.7776,
      "step": 5720
    },
    {
      "epoch": 1.105964099594673,
      "grad_norm": 11.657246589660645,
      "learning_rate": 1.4471144566685968e-05,
      "loss": 1.6438,
      "step": 5730
    },
    {
      "epoch": 1.107894228913337,
      "grad_norm": 6.953534126281738,
      "learning_rate": 1.4461493920092647e-05,
      "loss": 2.1516,
      "step": 5740
    },
    {
      "epoch": 1.1098243582320015,
      "grad_norm": 2.3814878463745117,
      "learning_rate": 1.4451843273499326e-05,
      "loss": 2.1565,
      "step": 5750
    },
    {
      "epoch": 1.111754487550666,
      "grad_norm": 3.396618127822876,
      "learning_rate": 1.4442192626906005e-05,
      "loss": 1.7664,
      "step": 5760
    },
    {
      "epoch": 1.1136846168693302,
      "grad_norm": 8.790497779846191,
      "learning_rate": 1.4432541980312682e-05,
      "loss": 2.3147,
      "step": 5770
    },
    {
      "epoch": 1.1156147461879946,
      "grad_norm": 8.344658851623535,
      "learning_rate": 1.4422891333719359e-05,
      "loss": 2.3251,
      "step": 5780
    },
    {
      "epoch": 1.117544875506659,
      "grad_norm": 7.006673812866211,
      "learning_rate": 1.4413240687126038e-05,
      "loss": 2.04,
      "step": 5790
    },
    {
      "epoch": 1.1194750048253232,
      "grad_norm": 4.602826118469238,
      "learning_rate": 1.4403590040532718e-05,
      "loss": 1.678,
      "step": 5800
    },
    {
      "epoch": 1.1214051341439877,
      "grad_norm": 19.72563362121582,
      "learning_rate": 1.4393939393939396e-05,
      "loss": 2.1649,
      "step": 5810
    },
    {
      "epoch": 1.1233352634626521,
      "grad_norm": 8.12460708618164,
      "learning_rate": 1.4384288747346073e-05,
      "loss": 2.2084,
      "step": 5820
    },
    {
      "epoch": 1.1252653927813163,
      "grad_norm": 15.393325805664062,
      "learning_rate": 1.4374638100752752e-05,
      "loss": 2.2412,
      "step": 5830
    },
    {
      "epoch": 1.1271955220999808,
      "grad_norm": 29.754138946533203,
      "learning_rate": 1.4364987454159429e-05,
      "loss": 1.8277,
      "step": 5840
    },
    {
      "epoch": 1.129125651418645,
      "grad_norm": 23.971975326538086,
      "learning_rate": 1.4355336807566107e-05,
      "loss": 1.8643,
      "step": 5850
    },
    {
      "epoch": 1.1310557807373094,
      "grad_norm": 14.915324211120605,
      "learning_rate": 1.4345686160972786e-05,
      "loss": 1.6884,
      "step": 5860
    },
    {
      "epoch": 1.1329859100559738,
      "grad_norm": 29.068201065063477,
      "learning_rate": 1.4336035514379465e-05,
      "loss": 2.223,
      "step": 5870
    },
    {
      "epoch": 1.134916039374638,
      "grad_norm": 5.737203598022461,
      "learning_rate": 1.4326384867786142e-05,
      "loss": 1.9915,
      "step": 5880
    },
    {
      "epoch": 1.1368461686933025,
      "grad_norm": 38.84318542480469,
      "learning_rate": 1.4316734221192821e-05,
      "loss": 2.2833,
      "step": 5890
    },
    {
      "epoch": 1.138776298011967,
      "grad_norm": 4.704560279846191,
      "learning_rate": 1.4307083574599498e-05,
      "loss": 2.4073,
      "step": 5900
    },
    {
      "epoch": 1.140706427330631,
      "grad_norm": 18.799951553344727,
      "learning_rate": 1.4297432928006179e-05,
      "loss": 1.6301,
      "step": 5910
    },
    {
      "epoch": 1.1426365566492955,
      "grad_norm": 6.438791275024414,
      "learning_rate": 1.4287782281412856e-05,
      "loss": 2.1695,
      "step": 5920
    },
    {
      "epoch": 1.14456668596796,
      "grad_norm": 74.62030792236328,
      "learning_rate": 1.4278131634819535e-05,
      "loss": 2.5832,
      "step": 5930
    },
    {
      "epoch": 1.1464968152866242,
      "grad_norm": 4.363396644592285,
      "learning_rate": 1.4268480988226212e-05,
      "loss": 1.8456,
      "step": 5940
    },
    {
      "epoch": 1.1484269446052886,
      "grad_norm": 5.5345072746276855,
      "learning_rate": 1.425883034163289e-05,
      "loss": 1.5617,
      "step": 5950
    },
    {
      "epoch": 1.1503570739239528,
      "grad_norm": 29.022174835205078,
      "learning_rate": 1.4249179695039568e-05,
      "loss": 1.8822,
      "step": 5960
    },
    {
      "epoch": 1.1522872032426172,
      "grad_norm": 8.504110336303711,
      "learning_rate": 1.4239529048446248e-05,
      "loss": 2.3636,
      "step": 5970
    },
    {
      "epoch": 1.1542173325612817,
      "grad_norm": 76.76195526123047,
      "learning_rate": 1.4229878401852926e-05,
      "loss": 2.8506,
      "step": 5980
    },
    {
      "epoch": 1.1561474618799459,
      "grad_norm": 6.15925407409668,
      "learning_rate": 1.4220227755259603e-05,
      "loss": 1.4743,
      "step": 5990
    },
    {
      "epoch": 1.1580775911986103,
      "grad_norm": 3.7128183841705322,
      "learning_rate": 1.4210577108666282e-05,
      "loss": 1.11,
      "step": 6000
    },
    {
      "epoch": 1.1600077205172747,
      "grad_norm": 6.227746486663818,
      "learning_rate": 1.4200926462072959e-05,
      "loss": 2.0972,
      "step": 6010
    },
    {
      "epoch": 1.161937849835939,
      "grad_norm": 4.038928508758545,
      "learning_rate": 1.419127581547964e-05,
      "loss": 1.3947,
      "step": 6020
    },
    {
      "epoch": 1.1638679791546034,
      "grad_norm": 12.269804954528809,
      "learning_rate": 1.4181625168886316e-05,
      "loss": 1.515,
      "step": 6030
    },
    {
      "epoch": 1.1657981084732678,
      "grad_norm": 4.576448440551758,
      "learning_rate": 1.4171974522292995e-05,
      "loss": 1.876,
      "step": 6040
    },
    {
      "epoch": 1.167728237791932,
      "grad_norm": 17.17877769470215,
      "learning_rate": 1.4162323875699672e-05,
      "loss": 1.9511,
      "step": 6050
    },
    {
      "epoch": 1.1696583671105965,
      "grad_norm": 10.270593643188477,
      "learning_rate": 1.4152673229106351e-05,
      "loss": 1.6161,
      "step": 6060
    },
    {
      "epoch": 1.1715884964292607,
      "grad_norm": 14.29057502746582,
      "learning_rate": 1.4143022582513028e-05,
      "loss": 2.1587,
      "step": 6070
    },
    {
      "epoch": 1.173518625747925,
      "grad_norm": 7.951818466186523,
      "learning_rate": 1.4133371935919709e-05,
      "loss": 1.4219,
      "step": 6080
    },
    {
      "epoch": 1.1754487550665895,
      "grad_norm": 9.545679092407227,
      "learning_rate": 1.4123721289326386e-05,
      "loss": 1.2921,
      "step": 6090
    },
    {
      "epoch": 1.1773788843852537,
      "grad_norm": 4.2438788414001465,
      "learning_rate": 1.4114070642733065e-05,
      "loss": 1.4487,
      "step": 6100
    },
    {
      "epoch": 1.1793090137039182,
      "grad_norm": 11.710800170898438,
      "learning_rate": 1.4104419996139742e-05,
      "loss": 1.3102,
      "step": 6110
    },
    {
      "epoch": 1.1812391430225826,
      "grad_norm": 7.082712173461914,
      "learning_rate": 1.409476934954642e-05,
      "loss": 1.0641,
      "step": 6120
    },
    {
      "epoch": 1.1831692723412468,
      "grad_norm": 11.445172309875488,
      "learning_rate": 1.40851187029531e-05,
      "loss": 1.8186,
      "step": 6130
    },
    {
      "epoch": 1.1850994016599112,
      "grad_norm": 51.732242584228516,
      "learning_rate": 1.4075468056359778e-05,
      "loss": 1.8352,
      "step": 6140
    },
    {
      "epoch": 1.1870295309785757,
      "grad_norm": 6.542693614959717,
      "learning_rate": 1.4065817409766456e-05,
      "loss": 2.1102,
      "step": 6150
    },
    {
      "epoch": 1.1889596602972399,
      "grad_norm": 2.0737953186035156,
      "learning_rate": 1.4056166763173134e-05,
      "loss": 1.5577,
      "step": 6160
    },
    {
      "epoch": 1.1908897896159043,
      "grad_norm": 7.365407466888428,
      "learning_rate": 1.4046516116579812e-05,
      "loss": 2.3181,
      "step": 6170
    },
    {
      "epoch": 1.1928199189345685,
      "grad_norm": 37.77587127685547,
      "learning_rate": 1.4036865469986489e-05,
      "loss": 1.8618,
      "step": 6180
    },
    {
      "epoch": 1.194750048253233,
      "grad_norm": 3.331972122192383,
      "learning_rate": 1.402721482339317e-05,
      "loss": 2.2616,
      "step": 6190
    },
    {
      "epoch": 1.1966801775718974,
      "grad_norm": 8.464397430419922,
      "learning_rate": 1.4017564176799846e-05,
      "loss": 1.5631,
      "step": 6200
    },
    {
      "epoch": 1.1986103068905616,
      "grad_norm": 40.2886962890625,
      "learning_rate": 1.4007913530206525e-05,
      "loss": 2.0764,
      "step": 6210
    },
    {
      "epoch": 1.200540436209226,
      "grad_norm": 26.061187744140625,
      "learning_rate": 1.3998262883613202e-05,
      "loss": 2.1893,
      "step": 6220
    },
    {
      "epoch": 1.2024705655278904,
      "grad_norm": 14.66734790802002,
      "learning_rate": 1.3988612237019881e-05,
      "loss": 1.7916,
      "step": 6230
    },
    {
      "epoch": 1.2044006948465547,
      "grad_norm": 12.230817794799805,
      "learning_rate": 1.397896159042656e-05,
      "loss": 2.7182,
      "step": 6240
    },
    {
      "epoch": 1.206330824165219,
      "grad_norm": 9.960282325744629,
      "learning_rate": 1.3969310943833239e-05,
      "loss": 1.8165,
      "step": 6250
    },
    {
      "epoch": 1.2082609534838835,
      "grad_norm": 5.315381050109863,
      "learning_rate": 1.3959660297239916e-05,
      "loss": 1.9308,
      "step": 6260
    },
    {
      "epoch": 1.2101910828025477,
      "grad_norm": 5.245309829711914,
      "learning_rate": 1.3950009650646595e-05,
      "loss": 1.467,
      "step": 6270
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 14.332338333129883,
      "learning_rate": 1.3940359004053272e-05,
      "loss": 1.7677,
      "step": 6280
    },
    {
      "epoch": 1.2140513414398764,
      "grad_norm": 10.412750244140625,
      "learning_rate": 1.393070835745995e-05,
      "loss": 2.2534,
      "step": 6290
    },
    {
      "epoch": 1.2159814707585408,
      "grad_norm": 2.368375778198242,
      "learning_rate": 1.392105771086663e-05,
      "loss": 1.7581,
      "step": 6300
    },
    {
      "epoch": 1.2179116000772052,
      "grad_norm": 68.72466278076172,
      "learning_rate": 1.3911407064273308e-05,
      "loss": 2.0373,
      "step": 6310
    },
    {
      "epoch": 1.2198417293958694,
      "grad_norm": 47.54949951171875,
      "learning_rate": 1.3901756417679986e-05,
      "loss": 1.9785,
      "step": 6320
    },
    {
      "epoch": 1.2217718587145339,
      "grad_norm": 11.930033683776855,
      "learning_rate": 1.3892105771086664e-05,
      "loss": 2.0279,
      "step": 6330
    },
    {
      "epoch": 1.2237019880331983,
      "grad_norm": 5.206739902496338,
      "learning_rate": 1.3882455124493342e-05,
      "loss": 1.7677,
      "step": 6340
    },
    {
      "epoch": 1.2256321173518625,
      "grad_norm": 68.34455871582031,
      "learning_rate": 1.3872804477900022e-05,
      "loss": 2.1057,
      "step": 6350
    },
    {
      "epoch": 1.227562246670527,
      "grad_norm": 13.473808288574219,
      "learning_rate": 1.38631538313067e-05,
      "loss": 2.0319,
      "step": 6360
    },
    {
      "epoch": 1.2294923759891914,
      "grad_norm": 3.177116632461548,
      "learning_rate": 1.3853503184713376e-05,
      "loss": 1.66,
      "step": 6370
    },
    {
      "epoch": 1.2314225053078556,
      "grad_norm": 7.25661039352417,
      "learning_rate": 1.3843852538120055e-05,
      "loss": 1.7295,
      "step": 6380
    },
    {
      "epoch": 1.23335263462652,
      "grad_norm": 8.790247917175293,
      "learning_rate": 1.3834201891526732e-05,
      "loss": 2.0924,
      "step": 6390
    },
    {
      "epoch": 1.2352827639451842,
      "grad_norm": 19.590667724609375,
      "learning_rate": 1.3824551244933411e-05,
      "loss": 1.8937,
      "step": 6400
    },
    {
      "epoch": 1.2372128932638486,
      "grad_norm": 8.02580738067627,
      "learning_rate": 1.381490059834009e-05,
      "loss": 1.8691,
      "step": 6410
    },
    {
      "epoch": 1.239143022582513,
      "grad_norm": 16.921361923217773,
      "learning_rate": 1.3805249951746769e-05,
      "loss": 1.5191,
      "step": 6420
    },
    {
      "epoch": 1.2410731519011773,
      "grad_norm": 27.445749282836914,
      "learning_rate": 1.3795599305153446e-05,
      "loss": 2.0217,
      "step": 6430
    },
    {
      "epoch": 1.2430032812198417,
      "grad_norm": 20.720415115356445,
      "learning_rate": 1.3785948658560125e-05,
      "loss": 1.5306,
      "step": 6440
    },
    {
      "epoch": 1.2449334105385061,
      "grad_norm": 25.7647762298584,
      "learning_rate": 1.3776298011966802e-05,
      "loss": 1.9758,
      "step": 6450
    },
    {
      "epoch": 1.2468635398571704,
      "grad_norm": 13.11365795135498,
      "learning_rate": 1.3766647365373482e-05,
      "loss": 1.5672,
      "step": 6460
    },
    {
      "epoch": 1.2487936691758348,
      "grad_norm": 34.943626403808594,
      "learning_rate": 1.375699671878016e-05,
      "loss": 1.8811,
      "step": 6470
    },
    {
      "epoch": 1.2507237984944992,
      "grad_norm": 3.1369125843048096,
      "learning_rate": 1.3747346072186838e-05,
      "loss": 1.3951,
      "step": 6480
    },
    {
      "epoch": 1.2526539278131634,
      "grad_norm": 3.6321005821228027,
      "learning_rate": 1.3737695425593516e-05,
      "loss": 1.9878,
      "step": 6490
    },
    {
      "epoch": 1.2545840571318279,
      "grad_norm": 11.082709312438965,
      "learning_rate": 1.3728044779000194e-05,
      "loss": 1.6253,
      "step": 6500
    },
    {
      "epoch": 1.256514186450492,
      "grad_norm": 124.44507598876953,
      "learning_rate": 1.3718394132406872e-05,
      "loss": 2.3517,
      "step": 6510
    },
    {
      "epoch": 1.2584443157691565,
      "grad_norm": 5.200803279876709,
      "learning_rate": 1.3708743485813552e-05,
      "loss": 1.5955,
      "step": 6520
    },
    {
      "epoch": 1.260374445087821,
      "grad_norm": 13.169983863830566,
      "learning_rate": 1.369909283922023e-05,
      "loss": 1.7354,
      "step": 6530
    },
    {
      "epoch": 1.2623045744064854,
      "grad_norm": 12.749817848205566,
      "learning_rate": 1.3689442192626908e-05,
      "loss": 1.5209,
      "step": 6540
    },
    {
      "epoch": 1.2642347037251496,
      "grad_norm": 12.847485542297363,
      "learning_rate": 1.3679791546033585e-05,
      "loss": 2.1267,
      "step": 6550
    },
    {
      "epoch": 1.266164833043814,
      "grad_norm": 39.263946533203125,
      "learning_rate": 1.3670140899440262e-05,
      "loss": 1.8409,
      "step": 6560
    },
    {
      "epoch": 1.2680949623624782,
      "grad_norm": 3.999488353729248,
      "learning_rate": 1.3660490252846943e-05,
      "loss": 2.3265,
      "step": 6570
    },
    {
      "epoch": 1.2700250916811426,
      "grad_norm": 4.287204742431641,
      "learning_rate": 1.365083960625362e-05,
      "loss": 2.0491,
      "step": 6580
    },
    {
      "epoch": 1.271955220999807,
      "grad_norm": 76.3817138671875,
      "learning_rate": 1.3641188959660299e-05,
      "loss": 2.6317,
      "step": 6590
    },
    {
      "epoch": 1.2738853503184713,
      "grad_norm": 25.162397384643555,
      "learning_rate": 1.3631538313066976e-05,
      "loss": 1.6905,
      "step": 6600
    },
    {
      "epoch": 1.2758154796371357,
      "grad_norm": 15.762516975402832,
      "learning_rate": 1.3621887666473655e-05,
      "loss": 2.3321,
      "step": 6610
    },
    {
      "epoch": 1.2777456089558,
      "grad_norm": 5.642911911010742,
      "learning_rate": 1.3612237019880332e-05,
      "loss": 2.1226,
      "step": 6620
    },
    {
      "epoch": 1.2796757382744643,
      "grad_norm": 18.65985107421875,
      "learning_rate": 1.3602586373287012e-05,
      "loss": 1.1874,
      "step": 6630
    },
    {
      "epoch": 1.2816058675931288,
      "grad_norm": 15.409518241882324,
      "learning_rate": 1.359293572669369e-05,
      "loss": 1.6301,
      "step": 6640
    },
    {
      "epoch": 1.2835359969117932,
      "grad_norm": 5.926801681518555,
      "learning_rate": 1.3583285080100368e-05,
      "loss": 1.0591,
      "step": 6650
    },
    {
      "epoch": 1.2854661262304574,
      "grad_norm": 15.866233825683594,
      "learning_rate": 1.3573634433507046e-05,
      "loss": 1.6084,
      "step": 6660
    },
    {
      "epoch": 1.2873962555491218,
      "grad_norm": 3.9934046268463135,
      "learning_rate": 1.3563983786913724e-05,
      "loss": 1.4839,
      "step": 6670
    },
    {
      "epoch": 1.289326384867786,
      "grad_norm": 12.93154525756836,
      "learning_rate": 1.3554333140320402e-05,
      "loss": 1.6001,
      "step": 6680
    },
    {
      "epoch": 1.2912565141864505,
      "grad_norm": 12.567705154418945,
      "learning_rate": 1.3544682493727082e-05,
      "loss": 1.9276,
      "step": 6690
    },
    {
      "epoch": 1.293186643505115,
      "grad_norm": 0.9710010886192322,
      "learning_rate": 1.353503184713376e-05,
      "loss": 1.5234,
      "step": 6700
    },
    {
      "epoch": 1.2951167728237791,
      "grad_norm": 18.921724319458008,
      "learning_rate": 1.3525381200540438e-05,
      "loss": 1.9445,
      "step": 6710
    },
    {
      "epoch": 1.2970469021424436,
      "grad_norm": 3.6443300247192383,
      "learning_rate": 1.3515730553947115e-05,
      "loss": 1.195,
      "step": 6720
    },
    {
      "epoch": 1.2989770314611078,
      "grad_norm": 3.9093222618103027,
      "learning_rate": 1.3506079907353792e-05,
      "loss": 1.4909,
      "step": 6730
    },
    {
      "epoch": 1.3009071607797722,
      "grad_norm": 15.550298690795898,
      "learning_rate": 1.3496429260760473e-05,
      "loss": 2.043,
      "step": 6740
    },
    {
      "epoch": 1.3028372900984366,
      "grad_norm": 7.121188163757324,
      "learning_rate": 1.348677861416715e-05,
      "loss": 2.2477,
      "step": 6750
    },
    {
      "epoch": 1.304767419417101,
      "grad_norm": 2.999525547027588,
      "learning_rate": 1.3477127967573829e-05,
      "loss": 1.4732,
      "step": 6760
    },
    {
      "epoch": 1.3066975487357653,
      "grad_norm": 13.386137008666992,
      "learning_rate": 1.3467477320980506e-05,
      "loss": 1.8151,
      "step": 6770
    },
    {
      "epoch": 1.3086276780544297,
      "grad_norm": 26.59748077392578,
      "learning_rate": 1.3457826674387185e-05,
      "loss": 2.66,
      "step": 6780
    },
    {
      "epoch": 1.310557807373094,
      "grad_norm": 6.5169901847839355,
      "learning_rate": 1.3448176027793862e-05,
      "loss": 1.4788,
      "step": 6790
    },
    {
      "epoch": 1.3124879366917583,
      "grad_norm": 2.5268638134002686,
      "learning_rate": 1.3438525381200542e-05,
      "loss": 1.4013,
      "step": 6800
    },
    {
      "epoch": 1.3144180660104228,
      "grad_norm": 38.03809356689453,
      "learning_rate": 1.342887473460722e-05,
      "loss": 1.504,
      "step": 6810
    },
    {
      "epoch": 1.316348195329087,
      "grad_norm": 8.10256290435791,
      "learning_rate": 1.3419224088013898e-05,
      "loss": 2.3907,
      "step": 6820
    },
    {
      "epoch": 1.3182783246477514,
      "grad_norm": 47.71583938598633,
      "learning_rate": 1.3409573441420576e-05,
      "loss": 1.5032,
      "step": 6830
    },
    {
      "epoch": 1.3202084539664156,
      "grad_norm": 12.93851089477539,
      "learning_rate": 1.3399922794827254e-05,
      "loss": 1.6254,
      "step": 6840
    },
    {
      "epoch": 1.32213858328508,
      "grad_norm": 19.62936782836914,
      "learning_rate": 1.3390272148233933e-05,
      "loss": 1.293,
      "step": 6850
    },
    {
      "epoch": 1.3240687126037445,
      "grad_norm": 11.900360107421875,
      "learning_rate": 1.3380621501640612e-05,
      "loss": 1.3177,
      "step": 6860
    },
    {
      "epoch": 1.325998841922409,
      "grad_norm": 5.123586654663086,
      "learning_rate": 1.337097085504729e-05,
      "loss": 1.4154,
      "step": 6870
    },
    {
      "epoch": 1.3279289712410731,
      "grad_norm": 9.350334167480469,
      "learning_rate": 1.3361320208453968e-05,
      "loss": 1.9608,
      "step": 6880
    },
    {
      "epoch": 1.3298591005597376,
      "grad_norm": 8.654411315917969,
      "learning_rate": 1.3351669561860645e-05,
      "loss": 1.9349,
      "step": 6890
    },
    {
      "epoch": 1.3317892298784018,
      "grad_norm": 12.246397972106934,
      "learning_rate": 1.3342018915267322e-05,
      "loss": 1.7004,
      "step": 6900
    },
    {
      "epoch": 1.3337193591970662,
      "grad_norm": 2.3232297897338867,
      "learning_rate": 1.3332368268674003e-05,
      "loss": 2.2615,
      "step": 6910
    },
    {
      "epoch": 1.3356494885157306,
      "grad_norm": 8.111929893493652,
      "learning_rate": 1.3322717622080682e-05,
      "loss": 1.7342,
      "step": 6920
    },
    {
      "epoch": 1.3375796178343948,
      "grad_norm": 3.6726348400115967,
      "learning_rate": 1.3313066975487359e-05,
      "loss": 1.5151,
      "step": 6930
    },
    {
      "epoch": 1.3395097471530593,
      "grad_norm": 84.17814636230469,
      "learning_rate": 1.3303416328894036e-05,
      "loss": 2.4036,
      "step": 6940
    },
    {
      "epoch": 1.3414398764717235,
      "grad_norm": 6.318485736846924,
      "learning_rate": 1.3293765682300715e-05,
      "loss": 1.4888,
      "step": 6950
    },
    {
      "epoch": 1.343370005790388,
      "grad_norm": 38.943260192871094,
      "learning_rate": 1.3284115035707394e-05,
      "loss": 2.4214,
      "step": 6960
    },
    {
      "epoch": 1.3453001351090523,
      "grad_norm": 53.305419921875,
      "learning_rate": 1.3274464389114072e-05,
      "loss": 1.7019,
      "step": 6970
    },
    {
      "epoch": 1.3472302644277168,
      "grad_norm": 56.294273376464844,
      "learning_rate": 1.326481374252075e-05,
      "loss": 1.7498,
      "step": 6980
    },
    {
      "epoch": 1.349160393746381,
      "grad_norm": 36.73777770996094,
      "learning_rate": 1.3255163095927428e-05,
      "loss": 2.4655,
      "step": 6990
    },
    {
      "epoch": 1.3510905230650454,
      "grad_norm": 20.68590545654297,
      "learning_rate": 1.3245512449334106e-05,
      "loss": 1.9526,
      "step": 7000
    },
    {
      "epoch": 1.3530206523837096,
      "grad_norm": 29.046796798706055,
      "learning_rate": 1.3235861802740784e-05,
      "loss": 1.8216,
      "step": 7010
    },
    {
      "epoch": 1.354950781702374,
      "grad_norm": 4.258534908294678,
      "learning_rate": 1.3226211156147463e-05,
      "loss": 0.9574,
      "step": 7020
    },
    {
      "epoch": 1.3568809110210385,
      "grad_norm": 1.0987151861190796,
      "learning_rate": 1.3216560509554142e-05,
      "loss": 1.6831,
      "step": 7030
    },
    {
      "epoch": 1.3588110403397027,
      "grad_norm": 8.932379722595215,
      "learning_rate": 1.320690986296082e-05,
      "loss": 2.3524,
      "step": 7040
    },
    {
      "epoch": 1.3607411696583671,
      "grad_norm": 1.391734004020691,
      "learning_rate": 1.3197259216367498e-05,
      "loss": 1.072,
      "step": 7050
    },
    {
      "epoch": 1.3626712989770315,
      "grad_norm": 11.762913703918457,
      "learning_rate": 1.3187608569774175e-05,
      "loss": 1.8451,
      "step": 7060
    },
    {
      "epoch": 1.3646014282956958,
      "grad_norm": 3.9328911304473877,
      "learning_rate": 1.3177957923180856e-05,
      "loss": 1.5142,
      "step": 7070
    },
    {
      "epoch": 1.3665315576143602,
      "grad_norm": 44.4769172668457,
      "learning_rate": 1.3168307276587533e-05,
      "loss": 2.2541,
      "step": 7080
    },
    {
      "epoch": 1.3684616869330246,
      "grad_norm": 7.807225227355957,
      "learning_rate": 1.3158656629994212e-05,
      "loss": 1.6234,
      "step": 7090
    },
    {
      "epoch": 1.3703918162516888,
      "grad_norm": 24.9366455078125,
      "learning_rate": 1.3149005983400889e-05,
      "loss": 1.5974,
      "step": 7100
    },
    {
      "epoch": 1.3723219455703533,
      "grad_norm": 7.031485080718994,
      "learning_rate": 1.3139355336807566e-05,
      "loss": 1.4357,
      "step": 7110
    },
    {
      "epoch": 1.3742520748890175,
      "grad_norm": 1.4015541076660156,
      "learning_rate": 1.3129704690214245e-05,
      "loss": 1.4957,
      "step": 7120
    },
    {
      "epoch": 1.376182204207682,
      "grad_norm": 8.09211540222168,
      "learning_rate": 1.3120054043620924e-05,
      "loss": 1.8168,
      "step": 7130
    },
    {
      "epoch": 1.3781123335263463,
      "grad_norm": 19.502138137817383,
      "learning_rate": 1.3110403397027602e-05,
      "loss": 1.3288,
      "step": 7140
    },
    {
      "epoch": 1.3800424628450108,
      "grad_norm": 7.171455383300781,
      "learning_rate": 1.310075275043428e-05,
      "loss": 2.0613,
      "step": 7150
    },
    {
      "epoch": 1.381972592163675,
      "grad_norm": 5.720644474029541,
      "learning_rate": 1.3091102103840958e-05,
      "loss": 1.487,
      "step": 7160
    },
    {
      "epoch": 1.3839027214823394,
      "grad_norm": 13.544563293457031,
      "learning_rate": 1.3081451457247636e-05,
      "loss": 1.7315,
      "step": 7170
    },
    {
      "epoch": 1.3858328508010036,
      "grad_norm": 35.08237838745117,
      "learning_rate": 1.3071800810654316e-05,
      "loss": 1.7925,
      "step": 7180
    },
    {
      "epoch": 1.387762980119668,
      "grad_norm": 9.68612003326416,
      "learning_rate": 1.3062150164060993e-05,
      "loss": 1.5473,
      "step": 7190
    },
    {
      "epoch": 1.3896931094383325,
      "grad_norm": 15.730846405029297,
      "learning_rate": 1.3052499517467672e-05,
      "loss": 1.6833,
      "step": 7200
    },
    {
      "epoch": 1.3916232387569967,
      "grad_norm": 4.148620128631592,
      "learning_rate": 1.304284887087435e-05,
      "loss": 1.6822,
      "step": 7210
    },
    {
      "epoch": 1.393553368075661,
      "grad_norm": 12.252552032470703,
      "learning_rate": 1.3033198224281028e-05,
      "loss": 1.8836,
      "step": 7220
    },
    {
      "epoch": 1.3954834973943253,
      "grad_norm": 38.3156623840332,
      "learning_rate": 1.3023547577687705e-05,
      "loss": 1.29,
      "step": 7230
    },
    {
      "epoch": 1.3974136267129897,
      "grad_norm": 6.339162826538086,
      "learning_rate": 1.3013896931094386e-05,
      "loss": 2.0657,
      "step": 7240
    },
    {
      "epoch": 1.3993437560316542,
      "grad_norm": 31.12128257751465,
      "learning_rate": 1.3004246284501063e-05,
      "loss": 1.9503,
      "step": 7250
    },
    {
      "epoch": 1.4012738853503186,
      "grad_norm": 5.120231628417969,
      "learning_rate": 1.2994595637907742e-05,
      "loss": 1.5508,
      "step": 7260
    },
    {
      "epoch": 1.4032040146689828,
      "grad_norm": 62.9559326171875,
      "learning_rate": 1.2984944991314419e-05,
      "loss": 1.607,
      "step": 7270
    },
    {
      "epoch": 1.4051341439876472,
      "grad_norm": 3.329059600830078,
      "learning_rate": 1.2975294344721096e-05,
      "loss": 1.2463,
      "step": 7280
    },
    {
      "epoch": 1.4070642733063115,
      "grad_norm": 4.64016580581665,
      "learning_rate": 1.2965643698127777e-05,
      "loss": 1.9784,
      "step": 7290
    },
    {
      "epoch": 1.4089944026249759,
      "grad_norm": 4.880349636077881,
      "learning_rate": 1.2955993051534455e-05,
      "loss": 1.588,
      "step": 7300
    },
    {
      "epoch": 1.4109245319436403,
      "grad_norm": 32.16554260253906,
      "learning_rate": 1.2946342404941132e-05,
      "loss": 1.6144,
      "step": 7310
    },
    {
      "epoch": 1.4128546612623045,
      "grad_norm": 5.953625202178955,
      "learning_rate": 1.293669175834781e-05,
      "loss": 1.4152,
      "step": 7320
    },
    {
      "epoch": 1.414784790580969,
      "grad_norm": 2.177067518234253,
      "learning_rate": 1.2927041111754488e-05,
      "loss": 1.4378,
      "step": 7330
    },
    {
      "epoch": 1.4167149198996332,
      "grad_norm": 4.201221466064453,
      "learning_rate": 1.2917390465161166e-05,
      "loss": 1.7373,
      "step": 7340
    },
    {
      "epoch": 1.4186450492182976,
      "grad_norm": 11.095300674438477,
      "learning_rate": 1.2907739818567846e-05,
      "loss": 1.4328,
      "step": 7350
    },
    {
      "epoch": 1.420575178536962,
      "grad_norm": 6.582441329956055,
      "learning_rate": 1.2898089171974523e-05,
      "loss": 2.0328,
      "step": 7360
    },
    {
      "epoch": 1.4225053078556265,
      "grad_norm": 13.816405296325684,
      "learning_rate": 1.2888438525381202e-05,
      "loss": 1.7348,
      "step": 7370
    },
    {
      "epoch": 1.4244354371742907,
      "grad_norm": 18.462997436523438,
      "learning_rate": 1.287878787878788e-05,
      "loss": 1.7996,
      "step": 7380
    },
    {
      "epoch": 1.426365566492955,
      "grad_norm": 3.8825559616088867,
      "learning_rate": 1.2869137232194558e-05,
      "loss": 1.4873,
      "step": 7390
    },
    {
      "epoch": 1.4282956958116193,
      "grad_norm": 43.37272644042969,
      "learning_rate": 1.2859486585601237e-05,
      "loss": 1.3895,
      "step": 7400
    },
    {
      "epoch": 1.4302258251302837,
      "grad_norm": 6.211751937866211,
      "learning_rate": 1.2849835939007916e-05,
      "loss": 1.8593,
      "step": 7410
    },
    {
      "epoch": 1.4321559544489482,
      "grad_norm": 26.227174758911133,
      "learning_rate": 1.2840185292414593e-05,
      "loss": 1.9885,
      "step": 7420
    },
    {
      "epoch": 1.4340860837676124,
      "grad_norm": 6.109251499176025,
      "learning_rate": 1.2830534645821272e-05,
      "loss": 1.6612,
      "step": 7430
    },
    {
      "epoch": 1.4360162130862768,
      "grad_norm": 31.203716278076172,
      "learning_rate": 1.2820883999227949e-05,
      "loss": 1.2743,
      "step": 7440
    },
    {
      "epoch": 1.437946342404941,
      "grad_norm": 53.45915985107422,
      "learning_rate": 1.2811233352634626e-05,
      "loss": 2.1483,
      "step": 7450
    },
    {
      "epoch": 1.4398764717236054,
      "grad_norm": 23.872940063476562,
      "learning_rate": 1.2801582706041307e-05,
      "loss": 1.9449,
      "step": 7460
    },
    {
      "epoch": 1.4418066010422699,
      "grad_norm": 87.1302490234375,
      "learning_rate": 1.2791932059447985e-05,
      "loss": 2.6666,
      "step": 7470
    },
    {
      "epoch": 1.4437367303609343,
      "grad_norm": 71.1731185913086,
      "learning_rate": 1.2782281412854663e-05,
      "loss": 1.5578,
      "step": 7480
    },
    {
      "epoch": 1.4456668596795985,
      "grad_norm": 8.551355361938477,
      "learning_rate": 1.277263076626134e-05,
      "loss": 1.721,
      "step": 7490
    },
    {
      "epoch": 1.447596988998263,
      "grad_norm": 0.3929424583911896,
      "learning_rate": 1.2762980119668018e-05,
      "loss": 1.1424,
      "step": 7500
    },
    {
      "epoch": 1.4495271183169272,
      "grad_norm": 25.377071380615234,
      "learning_rate": 1.2753329473074697e-05,
      "loss": 1.6281,
      "step": 7510
    },
    {
      "epoch": 1.4514572476355916,
      "grad_norm": 30.99683952331543,
      "learning_rate": 1.2743678826481376e-05,
      "loss": 1.5607,
      "step": 7520
    },
    {
      "epoch": 1.453387376954256,
      "grad_norm": 30.71113395690918,
      "learning_rate": 1.2734028179888053e-05,
      "loss": 1.5751,
      "step": 7530
    },
    {
      "epoch": 1.4553175062729202,
      "grad_norm": 3.084195613861084,
      "learning_rate": 1.2724377533294732e-05,
      "loss": 1.5141,
      "step": 7540
    },
    {
      "epoch": 1.4572476355915847,
      "grad_norm": 25.89521026611328,
      "learning_rate": 1.271472688670141e-05,
      "loss": 0.7163,
      "step": 7550
    },
    {
      "epoch": 1.4591777649102489,
      "grad_norm": 6.888705730438232,
      "learning_rate": 1.2705076240108088e-05,
      "loss": 2.1052,
      "step": 7560
    },
    {
      "epoch": 1.4611078942289133,
      "grad_norm": 17.253355026245117,
      "learning_rate": 1.2695425593514767e-05,
      "loss": 1.4039,
      "step": 7570
    },
    {
      "epoch": 1.4630380235475777,
      "grad_norm": 36.14435958862305,
      "learning_rate": 1.2685774946921446e-05,
      "loss": 1.7877,
      "step": 7580
    },
    {
      "epoch": 1.4649681528662422,
      "grad_norm": 7.308866024017334,
      "learning_rate": 1.2676124300328123e-05,
      "loss": 1.7001,
      "step": 7590
    },
    {
      "epoch": 1.4668982821849064,
      "grad_norm": 4.4331889152526855,
      "learning_rate": 1.2666473653734802e-05,
      "loss": 1.6881,
      "step": 7600
    },
    {
      "epoch": 1.4688284115035708,
      "grad_norm": 38.453369140625,
      "learning_rate": 1.2656823007141479e-05,
      "loss": 1.9617,
      "step": 7610
    },
    {
      "epoch": 1.470758540822235,
      "grad_norm": 36.150630950927734,
      "learning_rate": 1.2647172360548156e-05,
      "loss": 2.0469,
      "step": 7620
    },
    {
      "epoch": 1.4726886701408994,
      "grad_norm": 2.551936388015747,
      "learning_rate": 1.2637521713954837e-05,
      "loss": 1.5982,
      "step": 7630
    },
    {
      "epoch": 1.4746187994595639,
      "grad_norm": 7.968770503997803,
      "learning_rate": 1.2627871067361515e-05,
      "loss": 1.2567,
      "step": 7640
    },
    {
      "epoch": 1.476548928778228,
      "grad_norm": 5.889614582061768,
      "learning_rate": 1.2618220420768193e-05,
      "loss": 1.6067,
      "step": 7650
    },
    {
      "epoch": 1.4784790580968925,
      "grad_norm": 5.767523765563965,
      "learning_rate": 1.260856977417487e-05,
      "loss": 1.4403,
      "step": 7660
    },
    {
      "epoch": 1.4804091874155567,
      "grad_norm": 5.148632049560547,
      "learning_rate": 1.2598919127581548e-05,
      "loss": 2.0328,
      "step": 7670
    },
    {
      "epoch": 1.4823393167342211,
      "grad_norm": 3.7038395404815674,
      "learning_rate": 1.2589268480988229e-05,
      "loss": 2.0604,
      "step": 7680
    },
    {
      "epoch": 1.4842694460528856,
      "grad_norm": 8.916131973266602,
      "learning_rate": 1.2579617834394906e-05,
      "loss": 1.5054,
      "step": 7690
    },
    {
      "epoch": 1.48619957537155,
      "grad_norm": 0.8247081637382507,
      "learning_rate": 1.2569967187801583e-05,
      "loss": 2.0606,
      "step": 7700
    },
    {
      "epoch": 1.4881297046902142,
      "grad_norm": 7.687270164489746,
      "learning_rate": 1.2560316541208262e-05,
      "loss": 1.6339,
      "step": 7710
    },
    {
      "epoch": 1.4900598340088786,
      "grad_norm": 97.12609100341797,
      "learning_rate": 1.255066589461494e-05,
      "loss": 1.1533,
      "step": 7720
    },
    {
      "epoch": 1.4919899633275429,
      "grad_norm": 6.887597560882568,
      "learning_rate": 1.2541015248021618e-05,
      "loss": 1.6609,
      "step": 7730
    },
    {
      "epoch": 1.4939200926462073,
      "grad_norm": 5.644479751586914,
      "learning_rate": 1.2531364601428297e-05,
      "loss": 1.9048,
      "step": 7740
    },
    {
      "epoch": 1.4958502219648717,
      "grad_norm": 16.819255828857422,
      "learning_rate": 1.2521713954834976e-05,
      "loss": 1.4125,
      "step": 7750
    },
    {
      "epoch": 1.497780351283536,
      "grad_norm": 42.89485168457031,
      "learning_rate": 1.2512063308241653e-05,
      "loss": 2.5398,
      "step": 7760
    },
    {
      "epoch": 1.4997104806022004,
      "grad_norm": 94.43029022216797,
      "learning_rate": 1.2502412661648332e-05,
      "loss": 1.5113,
      "step": 7770
    },
    {
      "epoch": 1.5016406099208646,
      "grad_norm": 2.585601329803467,
      "learning_rate": 1.2492762015055009e-05,
      "loss": 1.8107,
      "step": 7780
    },
    {
      "epoch": 1.503570739239529,
      "grad_norm": 9.648555755615234,
      "learning_rate": 1.248311136846169e-05,
      "loss": 1.7546,
      "step": 7790
    },
    {
      "epoch": 1.5055008685581934,
      "grad_norm": 23.99834632873535,
      "learning_rate": 1.2473460721868367e-05,
      "loss": 1.5761,
      "step": 7800
    },
    {
      "epoch": 1.5074309978768579,
      "grad_norm": 29.885438919067383,
      "learning_rate": 1.2463810075275045e-05,
      "loss": 0.8899,
      "step": 7810
    },
    {
      "epoch": 1.509361127195522,
      "grad_norm": 0.7888540029525757,
      "learning_rate": 1.2454159428681723e-05,
      "loss": 1.4127,
      "step": 7820
    },
    {
      "epoch": 1.5112912565141865,
      "grad_norm": 4.432056427001953,
      "learning_rate": 1.24445087820884e-05,
      "loss": 0.9095,
      "step": 7830
    },
    {
      "epoch": 1.5132213858328507,
      "grad_norm": 19.86098289489746,
      "learning_rate": 1.2434858135495078e-05,
      "loss": 2.1161,
      "step": 7840
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 3.6647629737854004,
      "learning_rate": 1.2425207488901759e-05,
      "loss": 1.3525,
      "step": 7850
    },
    {
      "epoch": 1.5170816444701796,
      "grad_norm": 70.0616226196289,
      "learning_rate": 1.2415556842308436e-05,
      "loss": 1.7257,
      "step": 7860
    },
    {
      "epoch": 1.519011773788844,
      "grad_norm": 7.09074592590332,
      "learning_rate": 1.2405906195715113e-05,
      "loss": 1.6913,
      "step": 7870
    },
    {
      "epoch": 1.5209419031075082,
      "grad_norm": 27.39335823059082,
      "learning_rate": 1.2396255549121792e-05,
      "loss": 1.6107,
      "step": 7880
    },
    {
      "epoch": 1.5228720324261724,
      "grad_norm": 31.99618911743164,
      "learning_rate": 1.238660490252847e-05,
      "loss": 1.1513,
      "step": 7890
    },
    {
      "epoch": 1.5248021617448368,
      "grad_norm": 23.122282028198242,
      "learning_rate": 1.237695425593515e-05,
      "loss": 1.5322,
      "step": 7900
    },
    {
      "epoch": 1.5267322910635013,
      "grad_norm": 11.165101051330566,
      "learning_rate": 1.2367303609341827e-05,
      "loss": 1.375,
      "step": 7910
    },
    {
      "epoch": 1.5286624203821657,
      "grad_norm": 8.626309394836426,
      "learning_rate": 1.2357652962748506e-05,
      "loss": 1.4236,
      "step": 7920
    },
    {
      "epoch": 1.53059254970083,
      "grad_norm": 8.071060180664062,
      "learning_rate": 1.2348002316155183e-05,
      "loss": 1.3853,
      "step": 7930
    },
    {
      "epoch": 1.5325226790194943,
      "grad_norm": 7.114538192749023,
      "learning_rate": 1.2338351669561862e-05,
      "loss": 1.5195,
      "step": 7940
    },
    {
      "epoch": 1.5344528083381586,
      "grad_norm": 3.3559327125549316,
      "learning_rate": 1.2328701022968539e-05,
      "loss": 1.1764,
      "step": 7950
    },
    {
      "epoch": 1.536382937656823,
      "grad_norm": 3.8061065673828125,
      "learning_rate": 1.231905037637522e-05,
      "loss": 1.9041,
      "step": 7960
    },
    {
      "epoch": 1.5383130669754874,
      "grad_norm": 17.13751792907715,
      "learning_rate": 1.2309399729781897e-05,
      "loss": 2.2986,
      "step": 7970
    },
    {
      "epoch": 1.5402431962941519,
      "grad_norm": 22.89958953857422,
      "learning_rate": 1.2299749083188575e-05,
      "loss": 1.7,
      "step": 7980
    },
    {
      "epoch": 1.542173325612816,
      "grad_norm": 9.24837589263916,
      "learning_rate": 1.2290098436595253e-05,
      "loss": 1.4432,
      "step": 7990
    },
    {
      "epoch": 1.5441034549314803,
      "grad_norm": 24.07939910888672,
      "learning_rate": 1.228044779000193e-05,
      "loss": 1.3945,
      "step": 8000
    },
    {
      "epoch": 1.5460335842501447,
      "grad_norm": 37.00797653198242,
      "learning_rate": 1.227079714340861e-05,
      "loss": 1.4081,
      "step": 8010
    },
    {
      "epoch": 1.5479637135688091,
      "grad_norm": 5.662740230560303,
      "learning_rate": 1.2261146496815289e-05,
      "loss": 1.1152,
      "step": 8020
    },
    {
      "epoch": 1.5498938428874736,
      "grad_norm": 23.74241065979004,
      "learning_rate": 1.2251495850221966e-05,
      "loss": 1.8076,
      "step": 8030
    },
    {
      "epoch": 1.5518239722061378,
      "grad_norm": 12.322418212890625,
      "learning_rate": 1.2241845203628643e-05,
      "loss": 1.5647,
      "step": 8040
    },
    {
      "epoch": 1.5537541015248022,
      "grad_norm": 6.06859016418457,
      "learning_rate": 1.2232194557035322e-05,
      "loss": 1.4601,
      "step": 8050
    },
    {
      "epoch": 1.5556842308434664,
      "grad_norm": 5.796666622161865,
      "learning_rate": 1.2222543910442e-05,
      "loss": 1.1352,
      "step": 8060
    },
    {
      "epoch": 1.5576143601621308,
      "grad_norm": 5.676494121551514,
      "learning_rate": 1.221289326384868e-05,
      "loss": 1.8616,
      "step": 8070
    },
    {
      "epoch": 1.5595444894807953,
      "grad_norm": 10.48854923248291,
      "learning_rate": 1.2203242617255357e-05,
      "loss": 1.6574,
      "step": 8080
    },
    {
      "epoch": 1.5614746187994597,
      "grad_norm": 47.85109329223633,
      "learning_rate": 1.2193591970662036e-05,
      "loss": 1.131,
      "step": 8090
    },
    {
      "epoch": 1.563404748118124,
      "grad_norm": 14.075332641601562,
      "learning_rate": 1.2183941324068713e-05,
      "loss": 1.1788,
      "step": 8100
    },
    {
      "epoch": 1.5653348774367881,
      "grad_norm": 14.37929916381836,
      "learning_rate": 1.2174290677475392e-05,
      "loss": 2.4329,
      "step": 8110
    },
    {
      "epoch": 1.5672650067554525,
      "grad_norm": 82.98597717285156,
      "learning_rate": 1.216464003088207e-05,
      "loss": 1.6358,
      "step": 8120
    },
    {
      "epoch": 1.569195136074117,
      "grad_norm": 28.410879135131836,
      "learning_rate": 1.215498938428875e-05,
      "loss": 2.0226,
      "step": 8130
    },
    {
      "epoch": 1.5711252653927814,
      "grad_norm": 4.054202556610107,
      "learning_rate": 1.2145338737695427e-05,
      "loss": 2.2828,
      "step": 8140
    },
    {
      "epoch": 1.5730553947114456,
      "grad_norm": 68.93785858154297,
      "learning_rate": 1.2135688091102105e-05,
      "loss": 1.4286,
      "step": 8150
    },
    {
      "epoch": 1.57498552403011,
      "grad_norm": 2.984970808029175,
      "learning_rate": 1.2126037444508783e-05,
      "loss": 1.6231,
      "step": 8160
    },
    {
      "epoch": 1.5769156533487743,
      "grad_norm": 1.4561724662780762,
      "learning_rate": 1.211638679791546e-05,
      "loss": 0.8932,
      "step": 8170
    },
    {
      "epoch": 1.5788457826674387,
      "grad_norm": 92.39208221435547,
      "learning_rate": 1.210673615132214e-05,
      "loss": 1.2245,
      "step": 8180
    },
    {
      "epoch": 1.5807759119861031,
      "grad_norm": 3.7156660556793213,
      "learning_rate": 1.2097085504728819e-05,
      "loss": 0.8789,
      "step": 8190
    },
    {
      "epoch": 1.5827060413047676,
      "grad_norm": 42.30109405517578,
      "learning_rate": 1.2087434858135496e-05,
      "loss": 0.9943,
      "step": 8200
    },
    {
      "epoch": 1.5846361706234318,
      "grad_norm": 1.3476557731628418,
      "learning_rate": 1.2077784211542173e-05,
      "loss": 1.7815,
      "step": 8210
    },
    {
      "epoch": 1.586566299942096,
      "grad_norm": 3.4394609928131104,
      "learning_rate": 1.2068133564948852e-05,
      "loss": 1.7226,
      "step": 8220
    },
    {
      "epoch": 1.5884964292607604,
      "grad_norm": 30.303916931152344,
      "learning_rate": 1.2058482918355533e-05,
      "loss": 1.8884,
      "step": 8230
    },
    {
      "epoch": 1.5904265585794248,
      "grad_norm": 7.774349212646484,
      "learning_rate": 1.204883227176221e-05,
      "loss": 1.9293,
      "step": 8240
    },
    {
      "epoch": 1.5923566878980893,
      "grad_norm": 61.620235443115234,
      "learning_rate": 1.2039181625168887e-05,
      "loss": 1.6429,
      "step": 8250
    },
    {
      "epoch": 1.5942868172167535,
      "grad_norm": 1.9475500583648682,
      "learning_rate": 1.2029530978575566e-05,
      "loss": 0.912,
      "step": 8260
    },
    {
      "epoch": 1.596216946535418,
      "grad_norm": 14.494144439697266,
      "learning_rate": 1.2019880331982243e-05,
      "loss": 1.3712,
      "step": 8270
    },
    {
      "epoch": 1.598147075854082,
      "grad_norm": 38.86720275878906,
      "learning_rate": 1.2010229685388922e-05,
      "loss": 1.5191,
      "step": 8280
    },
    {
      "epoch": 1.6000772051727465,
      "grad_norm": 3.0866384506225586,
      "learning_rate": 1.20005790387956e-05,
      "loss": 1.2669,
      "step": 8290
    },
    {
      "epoch": 1.602007334491411,
      "grad_norm": 8.5379638671875,
      "learning_rate": 1.199092839220228e-05,
      "loss": 1.8756,
      "step": 8300
    },
    {
      "epoch": 1.6039374638100754,
      "grad_norm": 23.005489349365234,
      "learning_rate": 1.1981277745608957e-05,
      "loss": 1.9383,
      "step": 8310
    },
    {
      "epoch": 1.6058675931287396,
      "grad_norm": 3.5827436447143555,
      "learning_rate": 1.1971627099015635e-05,
      "loss": 1.787,
      "step": 8320
    },
    {
      "epoch": 1.6077977224474038,
      "grad_norm": 4.761033535003662,
      "learning_rate": 1.1961976452422313e-05,
      "loss": 1.7262,
      "step": 8330
    },
    {
      "epoch": 1.6097278517660683,
      "grad_norm": 177.5806884765625,
      "learning_rate": 1.1952325805828993e-05,
      "loss": 1.6639,
      "step": 8340
    },
    {
      "epoch": 1.6116579810847327,
      "grad_norm": 36.402408599853516,
      "learning_rate": 1.194267515923567e-05,
      "loss": 1.719,
      "step": 8350
    },
    {
      "epoch": 1.6135881104033971,
      "grad_norm": 30.800506591796875,
      "learning_rate": 1.1933024512642349e-05,
      "loss": 2.3491,
      "step": 8360
    },
    {
      "epoch": 1.6155182397220615,
      "grad_norm": 0.4854852855205536,
      "learning_rate": 1.1923373866049026e-05,
      "loss": 1.6776,
      "step": 8370
    },
    {
      "epoch": 1.6174483690407258,
      "grad_norm": 19.425983428955078,
      "learning_rate": 1.1913723219455703e-05,
      "loss": 1.4632,
      "step": 8380
    },
    {
      "epoch": 1.61937849835939,
      "grad_norm": 4.267023086547852,
      "learning_rate": 1.1904072572862382e-05,
      "loss": 1.2963,
      "step": 8390
    },
    {
      "epoch": 1.6213086276780544,
      "grad_norm": 8.81845760345459,
      "learning_rate": 1.1894421926269063e-05,
      "loss": 1.1986,
      "step": 8400
    },
    {
      "epoch": 1.6232387569967188,
      "grad_norm": 98.26934814453125,
      "learning_rate": 1.188477127967574e-05,
      "loss": 1.249,
      "step": 8410
    },
    {
      "epoch": 1.6251688863153833,
      "grad_norm": 5.235218524932861,
      "learning_rate": 1.1875120633082417e-05,
      "loss": 1.6898,
      "step": 8420
    },
    {
      "epoch": 1.6270990156340475,
      "grad_norm": 2.9642109870910645,
      "learning_rate": 1.1865469986489096e-05,
      "loss": 1.5086,
      "step": 8430
    },
    {
      "epoch": 1.6290291449527117,
      "grad_norm": 7.881686210632324,
      "learning_rate": 1.1855819339895773e-05,
      "loss": 1.4206,
      "step": 8440
    },
    {
      "epoch": 1.630959274271376,
      "grad_norm": 11.502507209777832,
      "learning_rate": 1.1846168693302453e-05,
      "loss": 1.2961,
      "step": 8450
    },
    {
      "epoch": 1.6328894035900405,
      "grad_norm": 32.5803337097168,
      "learning_rate": 1.183651804670913e-05,
      "loss": 1.5177,
      "step": 8460
    },
    {
      "epoch": 1.634819532908705,
      "grad_norm": 2.9166319370269775,
      "learning_rate": 1.182686740011581e-05,
      "loss": 1.8414,
      "step": 8470
    },
    {
      "epoch": 1.6367496622273694,
      "grad_norm": 8.531089782714844,
      "learning_rate": 1.1817216753522487e-05,
      "loss": 1.5888,
      "step": 8480
    },
    {
      "epoch": 1.6386797915460336,
      "grad_norm": 0.9136856198310852,
      "learning_rate": 1.1807566106929165e-05,
      "loss": 1.4975,
      "step": 8490
    },
    {
      "epoch": 1.6406099208646978,
      "grad_norm": 6.7524542808532715,
      "learning_rate": 1.1797915460335843e-05,
      "loss": 1.1729,
      "step": 8500
    },
    {
      "epoch": 1.6425400501833622,
      "grad_norm": 2.137284517288208,
      "learning_rate": 1.1788264813742523e-05,
      "loss": 1.005,
      "step": 8510
    },
    {
      "epoch": 1.6444701795020267,
      "grad_norm": 3.9094300270080566,
      "learning_rate": 1.17786141671492e-05,
      "loss": 2.3622,
      "step": 8520
    },
    {
      "epoch": 1.646400308820691,
      "grad_norm": 64.11863708496094,
      "learning_rate": 1.1768963520555879e-05,
      "loss": 1.0062,
      "step": 8530
    },
    {
      "epoch": 1.6483304381393553,
      "grad_norm": 12.034603118896484,
      "learning_rate": 1.1759312873962556e-05,
      "loss": 1.8821,
      "step": 8540
    },
    {
      "epoch": 1.6502605674580195,
      "grad_norm": 11.75560188293457,
      "learning_rate": 1.1749662227369233e-05,
      "loss": 0.8469,
      "step": 8550
    },
    {
      "epoch": 1.652190696776684,
      "grad_norm": 15.498313903808594,
      "learning_rate": 1.1740011580775912e-05,
      "loss": 1.455,
      "step": 8560
    },
    {
      "epoch": 1.6541208260953484,
      "grad_norm": 52.82221984863281,
      "learning_rate": 1.1730360934182593e-05,
      "loss": 1.7049,
      "step": 8570
    },
    {
      "epoch": 1.6560509554140128,
      "grad_norm": 4.969077110290527,
      "learning_rate": 1.172071028758927e-05,
      "loss": 1.5715,
      "step": 8580
    },
    {
      "epoch": 1.6579810847326772,
      "grad_norm": 9.888670921325684,
      "learning_rate": 1.1711059640995947e-05,
      "loss": 1.0862,
      "step": 8590
    },
    {
      "epoch": 1.6599112140513415,
      "grad_norm": 3.0465481281280518,
      "learning_rate": 1.1701408994402626e-05,
      "loss": 1.7735,
      "step": 8600
    },
    {
      "epoch": 1.6618413433700057,
      "grad_norm": 53.100807189941406,
      "learning_rate": 1.1691758347809303e-05,
      "loss": 0.9532,
      "step": 8610
    },
    {
      "epoch": 1.66377147268867,
      "grad_norm": 5.085355281829834,
      "learning_rate": 1.1682107701215983e-05,
      "loss": 1.298,
      "step": 8620
    },
    {
      "epoch": 1.6657016020073345,
      "grad_norm": 26.529361724853516,
      "learning_rate": 1.167245705462266e-05,
      "loss": 2.1883,
      "step": 8630
    },
    {
      "epoch": 1.667631731325999,
      "grad_norm": 4.743184566497803,
      "learning_rate": 1.166280640802934e-05,
      "loss": 1.6561,
      "step": 8640
    },
    {
      "epoch": 1.6695618606446632,
      "grad_norm": 2.932060480117798,
      "learning_rate": 1.1653155761436017e-05,
      "loss": 1.6288,
      "step": 8650
    },
    {
      "epoch": 1.6714919899633276,
      "grad_norm": 126.35940551757812,
      "learning_rate": 1.1643505114842695e-05,
      "loss": 2.0416,
      "step": 8660
    },
    {
      "epoch": 1.6734221192819918,
      "grad_norm": 81.31012725830078,
      "learning_rate": 1.1633854468249373e-05,
      "loss": 1.6133,
      "step": 8670
    },
    {
      "epoch": 1.6753522486006562,
      "grad_norm": 51.91716384887695,
      "learning_rate": 1.1624203821656053e-05,
      "loss": 1.5171,
      "step": 8680
    },
    {
      "epoch": 1.6772823779193207,
      "grad_norm": 2.1567304134368896,
      "learning_rate": 1.161455317506273e-05,
      "loss": 1.8156,
      "step": 8690
    },
    {
      "epoch": 1.679212507237985,
      "grad_norm": 6.119302272796631,
      "learning_rate": 1.1604902528469409e-05,
      "loss": 1.3049,
      "step": 8700
    },
    {
      "epoch": 1.6811426365566493,
      "grad_norm": 4.012165546417236,
      "learning_rate": 1.1595251881876086e-05,
      "loss": 1.4206,
      "step": 8710
    },
    {
      "epoch": 1.6830727658753135,
      "grad_norm": 4.896777153015137,
      "learning_rate": 1.1585601235282763e-05,
      "loss": 1.3638,
      "step": 8720
    },
    {
      "epoch": 1.685002895193978,
      "grad_norm": 83.64251708984375,
      "learning_rate": 1.1575950588689444e-05,
      "loss": 1.0791,
      "step": 8730
    },
    {
      "epoch": 1.6869330245126424,
      "grad_norm": 3.955557107925415,
      "learning_rate": 1.1566299942096123e-05,
      "loss": 1.5641,
      "step": 8740
    },
    {
      "epoch": 1.6888631538313068,
      "grad_norm": 34.85369873046875,
      "learning_rate": 1.15566492955028e-05,
      "loss": 1.001,
      "step": 8750
    },
    {
      "epoch": 1.690793283149971,
      "grad_norm": 43.65999221801758,
      "learning_rate": 1.1546998648909477e-05,
      "loss": 1.4314,
      "step": 8760
    },
    {
      "epoch": 1.6927234124686354,
      "grad_norm": 9.738452911376953,
      "learning_rate": 1.1537348002316156e-05,
      "loss": 1.8813,
      "step": 8770
    },
    {
      "epoch": 1.6946535417872997,
      "grad_norm": 3.1291210651397705,
      "learning_rate": 1.1527697355722833e-05,
      "loss": 1.061,
      "step": 8780
    },
    {
      "epoch": 1.696583671105964,
      "grad_norm": 101.46493530273438,
      "learning_rate": 1.1518046709129513e-05,
      "loss": 1.6043,
      "step": 8790
    },
    {
      "epoch": 1.6985138004246285,
      "grad_norm": 5.419314861297607,
      "learning_rate": 1.150839606253619e-05,
      "loss": 1.4382,
      "step": 8800
    },
    {
      "epoch": 1.700443929743293,
      "grad_norm": 26.694814682006836,
      "learning_rate": 1.149874541594287e-05,
      "loss": 1.6667,
      "step": 8810
    },
    {
      "epoch": 1.7023740590619572,
      "grad_norm": 1.630582571029663,
      "learning_rate": 1.1489094769349547e-05,
      "loss": 1.8439,
      "step": 8820
    },
    {
      "epoch": 1.7043041883806214,
      "grad_norm": 1.3223185539245605,
      "learning_rate": 1.1479444122756225e-05,
      "loss": 1.3485,
      "step": 8830
    },
    {
      "epoch": 1.7062343176992858,
      "grad_norm": 45.96067810058594,
      "learning_rate": 1.1469793476162904e-05,
      "loss": 1.2224,
      "step": 8840
    },
    {
      "epoch": 1.7081644470179502,
      "grad_norm": 36.94049072265625,
      "learning_rate": 1.1460142829569583e-05,
      "loss": 1.9043,
      "step": 8850
    },
    {
      "epoch": 1.7100945763366147,
      "grad_norm": 85.78321075439453,
      "learning_rate": 1.145049218297626e-05,
      "loss": 1.2345,
      "step": 8860
    },
    {
      "epoch": 1.7120247056552789,
      "grad_norm": 30.78774070739746,
      "learning_rate": 1.1440841536382939e-05,
      "loss": 1.1047,
      "step": 8870
    },
    {
      "epoch": 1.7139548349739433,
      "grad_norm": 6.022653579711914,
      "learning_rate": 1.1431190889789616e-05,
      "loss": 1.5068,
      "step": 8880
    },
    {
      "epoch": 1.7158849642926075,
      "grad_norm": 35.03868865966797,
      "learning_rate": 1.1421540243196293e-05,
      "loss": 1.5834,
      "step": 8890
    },
    {
      "epoch": 1.717815093611272,
      "grad_norm": 1.6632238626480103,
      "learning_rate": 1.1411889596602974e-05,
      "loss": 1.1651,
      "step": 8900
    },
    {
      "epoch": 1.7197452229299364,
      "grad_norm": 3.9756243228912354,
      "learning_rate": 1.1402238950009653e-05,
      "loss": 1.4289,
      "step": 8910
    },
    {
      "epoch": 1.7216753522486008,
      "grad_norm": 18.854745864868164,
      "learning_rate": 1.139258830341633e-05,
      "loss": 1.7345,
      "step": 8920
    },
    {
      "epoch": 1.723605481567265,
      "grad_norm": 3.3920986652374268,
      "learning_rate": 1.1382937656823007e-05,
      "loss": 1.2338,
      "step": 8930
    },
    {
      "epoch": 1.7255356108859292,
      "grad_norm": 2.851781129837036,
      "learning_rate": 1.1373287010229686e-05,
      "loss": 1.076,
      "step": 8940
    },
    {
      "epoch": 1.7274657402045936,
      "grad_norm": 47.659236907958984,
      "learning_rate": 1.1363636363636366e-05,
      "loss": 1.2843,
      "step": 8950
    },
    {
      "epoch": 1.729395869523258,
      "grad_norm": 24.169479370117188,
      "learning_rate": 1.1353985717043043e-05,
      "loss": 2.029,
      "step": 8960
    },
    {
      "epoch": 1.7313259988419225,
      "grad_norm": 0.48381122946739197,
      "learning_rate": 1.134433507044972e-05,
      "loss": 2.0561,
      "step": 8970
    },
    {
      "epoch": 1.7332561281605867,
      "grad_norm": 2.1313016414642334,
      "learning_rate": 1.13346844238564e-05,
      "loss": 1.6798,
      "step": 8980
    },
    {
      "epoch": 1.7351862574792511,
      "grad_norm": 7.502373218536377,
      "learning_rate": 1.1325033777263077e-05,
      "loss": 1.4958,
      "step": 8990
    },
    {
      "epoch": 1.7371163867979154,
      "grad_norm": 5.475282192230225,
      "learning_rate": 1.1315383130669755e-05,
      "loss": 1.532,
      "step": 9000
    },
    {
      "epoch": 1.7390465161165798,
      "grad_norm": 1.1280232667922974,
      "learning_rate": 1.1305732484076434e-05,
      "loss": 1.8074,
      "step": 9010
    },
    {
      "epoch": 1.7409766454352442,
      "grad_norm": 12.395020484924316,
      "learning_rate": 1.1296081837483113e-05,
      "loss": 1.7479,
      "step": 9020
    },
    {
      "epoch": 1.7429067747539086,
      "grad_norm": 21.794240951538086,
      "learning_rate": 1.128643119088979e-05,
      "loss": 1.3286,
      "step": 9030
    },
    {
      "epoch": 1.7448369040725729,
      "grad_norm": 21.84976577758789,
      "learning_rate": 1.1276780544296469e-05,
      "loss": 1.4688,
      "step": 9040
    },
    {
      "epoch": 1.746767033391237,
      "grad_norm": 11.253694534301758,
      "learning_rate": 1.1267129897703146e-05,
      "loss": 1.7369,
      "step": 9050
    },
    {
      "epoch": 1.7486971627099015,
      "grad_norm": 18.377483367919922,
      "learning_rate": 1.1257479251109827e-05,
      "loss": 1.9491,
      "step": 9060
    },
    {
      "epoch": 1.750627292028566,
      "grad_norm": 1.7614388465881348,
      "learning_rate": 1.1247828604516504e-05,
      "loss": 1.1605,
      "step": 9070
    },
    {
      "epoch": 1.7525574213472304,
      "grad_norm": 8.179688453674316,
      "learning_rate": 1.1238177957923183e-05,
      "loss": 1.2497,
      "step": 9080
    },
    {
      "epoch": 1.7544875506658946,
      "grad_norm": 143.75552368164062,
      "learning_rate": 1.122852731132986e-05,
      "loss": 2.0681,
      "step": 9090
    },
    {
      "epoch": 1.756417679984559,
      "grad_norm": 15.644429206848145,
      "learning_rate": 1.1218876664736537e-05,
      "loss": 1.5816,
      "step": 9100
    },
    {
      "epoch": 1.7583478093032232,
      "grad_norm": 41.04838180541992,
      "learning_rate": 1.1209226018143216e-05,
      "loss": 1.1036,
      "step": 9110
    },
    {
      "epoch": 1.7602779386218876,
      "grad_norm": 16.281572341918945,
      "learning_rate": 1.1199575371549896e-05,
      "loss": 1.5898,
      "step": 9120
    },
    {
      "epoch": 1.762208067940552,
      "grad_norm": 166.57398986816406,
      "learning_rate": 1.1189924724956573e-05,
      "loss": 0.6934,
      "step": 9130
    },
    {
      "epoch": 1.7641381972592165,
      "grad_norm": 30.168865203857422,
      "learning_rate": 1.118027407836325e-05,
      "loss": 1.964,
      "step": 9140
    },
    {
      "epoch": 1.7660683265778807,
      "grad_norm": 63.092281341552734,
      "learning_rate": 1.117062343176993e-05,
      "loss": 1.9163,
      "step": 9150
    },
    {
      "epoch": 1.767998455896545,
      "grad_norm": 70.11479187011719,
      "learning_rate": 1.1160972785176607e-05,
      "loss": 1.4439,
      "step": 9160
    },
    {
      "epoch": 1.7699285852152093,
      "grad_norm": 52.02939224243164,
      "learning_rate": 1.1151322138583287e-05,
      "loss": 1.0444,
      "step": 9170
    },
    {
      "epoch": 1.7718587145338738,
      "grad_norm": 110.27327728271484,
      "learning_rate": 1.1141671491989964e-05,
      "loss": 2.3161,
      "step": 9180
    },
    {
      "epoch": 1.7737888438525382,
      "grad_norm": 16.10862159729004,
      "learning_rate": 1.1132020845396643e-05,
      "loss": 0.9407,
      "step": 9190
    },
    {
      "epoch": 1.7757189731712024,
      "grad_norm": 118.77385711669922,
      "learning_rate": 1.112237019880332e-05,
      "loss": 2.0013,
      "step": 9200
    },
    {
      "epoch": 1.7776491024898669,
      "grad_norm": 45.843788146972656,
      "learning_rate": 1.1112719552209999e-05,
      "loss": 0.8533,
      "step": 9210
    },
    {
      "epoch": 1.779579231808531,
      "grad_norm": 10.79599666595459,
      "learning_rate": 1.1103068905616676e-05,
      "loss": 1.0258,
      "step": 9220
    },
    {
      "epoch": 1.7815093611271955,
      "grad_norm": 23.7026309967041,
      "learning_rate": 1.1093418259023357e-05,
      "loss": 1.7156,
      "step": 9230
    },
    {
      "epoch": 1.78343949044586,
      "grad_norm": 0.7139685750007629,
      "learning_rate": 1.1083767612430034e-05,
      "loss": 0.9523,
      "step": 9240
    },
    {
      "epoch": 1.7853696197645244,
      "grad_norm": 5.145205020904541,
      "learning_rate": 1.1074116965836713e-05,
      "loss": 0.7769,
      "step": 9250
    },
    {
      "epoch": 1.7872997490831886,
      "grad_norm": 11.08179759979248,
      "learning_rate": 1.106446631924339e-05,
      "loss": 1.5724,
      "step": 9260
    },
    {
      "epoch": 1.7892298784018528,
      "grad_norm": 8.03313159942627,
      "learning_rate": 1.1054815672650067e-05,
      "loss": 1.6218,
      "step": 9270
    },
    {
      "epoch": 1.7911600077205172,
      "grad_norm": 4.629258155822754,
      "learning_rate": 1.1045165026056748e-05,
      "loss": 1.4532,
      "step": 9280
    },
    {
      "epoch": 1.7930901370391816,
      "grad_norm": 64.81465148925781,
      "learning_rate": 1.1035514379463426e-05,
      "loss": 1.5903,
      "step": 9290
    },
    {
      "epoch": 1.795020266357846,
      "grad_norm": 7.668053150177002,
      "learning_rate": 1.1025863732870103e-05,
      "loss": 1.3286,
      "step": 9300
    },
    {
      "epoch": 1.7969503956765105,
      "grad_norm": 3.0192556381225586,
      "learning_rate": 1.101621308627678e-05,
      "loss": 1.4274,
      "step": 9310
    },
    {
      "epoch": 1.7988805249951747,
      "grad_norm": 41.72069549560547,
      "learning_rate": 1.100656243968346e-05,
      "loss": 2.1696,
      "step": 9320
    },
    {
      "epoch": 1.800810654313839,
      "grad_norm": 1.3397436141967773,
      "learning_rate": 1.0996911793090137e-05,
      "loss": 0.9211,
      "step": 9330
    },
    {
      "epoch": 1.8027407836325033,
      "grad_norm": 4.512686729431152,
      "learning_rate": 1.0987261146496817e-05,
      "loss": 1.7872,
      "step": 9340
    },
    {
      "epoch": 1.8046709129511678,
      "grad_norm": 8.147347450256348,
      "learning_rate": 1.0977610499903494e-05,
      "loss": 1.7119,
      "step": 9350
    },
    {
      "epoch": 1.8066010422698322,
      "grad_norm": 3.275175094604492,
      "learning_rate": 1.0967959853310173e-05,
      "loss": 1.2492,
      "step": 9360
    },
    {
      "epoch": 1.8085311715884964,
      "grad_norm": 259.2332458496094,
      "learning_rate": 1.095830920671685e-05,
      "loss": 1.8047,
      "step": 9370
    },
    {
      "epoch": 1.8104613009071606,
      "grad_norm": 16.545679092407227,
      "learning_rate": 1.0948658560123529e-05,
      "loss": 1.2531,
      "step": 9380
    },
    {
      "epoch": 1.812391430225825,
      "grad_norm": 8.98931884765625,
      "learning_rate": 1.0939007913530208e-05,
      "loss": 1.657,
      "step": 9390
    },
    {
      "epoch": 1.8143215595444895,
      "grad_norm": 3.2274880409240723,
      "learning_rate": 1.0929357266936887e-05,
      "loss": 1.1873,
      "step": 9400
    },
    {
      "epoch": 1.816251688863154,
      "grad_norm": 17.799949645996094,
      "learning_rate": 1.0919706620343564e-05,
      "loss": 1.1063,
      "step": 9410
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 3.3396079540252686,
      "learning_rate": 1.0910055973750243e-05,
      "loss": 0.8974,
      "step": 9420
    },
    {
      "epoch": 1.8201119475004826,
      "grad_norm": 14.857319831848145,
      "learning_rate": 1.090040532715692e-05,
      "loss": 1.6563,
      "step": 9430
    },
    {
      "epoch": 1.8220420768191468,
      "grad_norm": 5.387913703918457,
      "learning_rate": 1.0890754680563597e-05,
      "loss": 1.2665,
      "step": 9440
    },
    {
      "epoch": 1.8239722061378112,
      "grad_norm": 12.161266326904297,
      "learning_rate": 1.0881104033970278e-05,
      "loss": 1.2569,
      "step": 9450
    },
    {
      "epoch": 1.8259023354564756,
      "grad_norm": 2.7311036586761475,
      "learning_rate": 1.0871453387376956e-05,
      "loss": 1.0273,
      "step": 9460
    },
    {
      "epoch": 1.82783246477514,
      "grad_norm": 7.847316741943359,
      "learning_rate": 1.0861802740783633e-05,
      "loss": 0.7623,
      "step": 9470
    },
    {
      "epoch": 1.8297625940938043,
      "grad_norm": 53.21190643310547,
      "learning_rate": 1.085215209419031e-05,
      "loss": 1.517,
      "step": 9480
    },
    {
      "epoch": 1.8316927234124685,
      "grad_norm": 7.527439117431641,
      "learning_rate": 1.084250144759699e-05,
      "loss": 0.9926,
      "step": 9490
    },
    {
      "epoch": 1.833622852731133,
      "grad_norm": 7.300110816955566,
      "learning_rate": 1.0832850801003667e-05,
      "loss": 1.021,
      "step": 9500
    },
    {
      "epoch": 1.8355529820497973,
      "grad_norm": 5.942038536071777,
      "learning_rate": 1.0823200154410347e-05,
      "loss": 1.3503,
      "step": 9510
    },
    {
      "epoch": 1.8374831113684618,
      "grad_norm": 46.48426055908203,
      "learning_rate": 1.0813549507817024e-05,
      "loss": 1.7277,
      "step": 9520
    },
    {
      "epoch": 1.8394132406871262,
      "grad_norm": 5.579929351806641,
      "learning_rate": 1.0803898861223703e-05,
      "loss": 1.4197,
      "step": 9530
    },
    {
      "epoch": 1.8413433700057904,
      "grad_norm": 16.295339584350586,
      "learning_rate": 1.079424821463038e-05,
      "loss": 1.6998,
      "step": 9540
    },
    {
      "epoch": 1.8432734993244546,
      "grad_norm": 85.87850189208984,
      "learning_rate": 1.0784597568037059e-05,
      "loss": 1.1549,
      "step": 9550
    },
    {
      "epoch": 1.845203628643119,
      "grad_norm": 26.289630889892578,
      "learning_rate": 1.0774946921443738e-05,
      "loss": 1.5862,
      "step": 9560
    },
    {
      "epoch": 1.8471337579617835,
      "grad_norm": 5.496637344360352,
      "learning_rate": 1.0765296274850417e-05,
      "loss": 1.2884,
      "step": 9570
    },
    {
      "epoch": 1.849063887280448,
      "grad_norm": 6.279959201812744,
      "learning_rate": 1.0755645628257094e-05,
      "loss": 2.0244,
      "step": 9580
    },
    {
      "epoch": 1.8509940165991121,
      "grad_norm": 29.9210205078125,
      "learning_rate": 1.0745994981663773e-05,
      "loss": 1.6791,
      "step": 9590
    },
    {
      "epoch": 1.8529241459177765,
      "grad_norm": 4.89268684387207,
      "learning_rate": 1.073634433507045e-05,
      "loss": 1.3281,
      "step": 9600
    },
    {
      "epoch": 1.8548542752364408,
      "grad_norm": 29.89781951904297,
      "learning_rate": 1.0726693688477127e-05,
      "loss": 1.5232,
      "step": 9610
    },
    {
      "epoch": 1.8567844045551052,
      "grad_norm": 13.054118156433105,
      "learning_rate": 1.0717043041883808e-05,
      "loss": 1.6721,
      "step": 9620
    },
    {
      "epoch": 1.8587145338737696,
      "grad_norm": 2.862372398376465,
      "learning_rate": 1.0707392395290486e-05,
      "loss": 1.7952,
      "step": 9630
    },
    {
      "epoch": 1.860644663192434,
      "grad_norm": 9.239242553710938,
      "learning_rate": 1.0697741748697164e-05,
      "loss": 0.8393,
      "step": 9640
    },
    {
      "epoch": 1.8625747925110983,
      "grad_norm": 18.746917724609375,
      "learning_rate": 1.068809110210384e-05,
      "loss": 0.8581,
      "step": 9650
    },
    {
      "epoch": 1.8645049218297625,
      "grad_norm": 1.1567176580429077,
      "learning_rate": 1.067844045551052e-05,
      "loss": 1.9897,
      "step": 9660
    },
    {
      "epoch": 1.866435051148427,
      "grad_norm": 3.5444912910461426,
      "learning_rate": 1.06687898089172e-05,
      "loss": 0.9369,
      "step": 9670
    },
    {
      "epoch": 1.8683651804670913,
      "grad_norm": 79.60255432128906,
      "learning_rate": 1.0659139162323877e-05,
      "loss": 1.9612,
      "step": 9680
    },
    {
      "epoch": 1.8702953097857558,
      "grad_norm": 7.107695579528809,
      "learning_rate": 1.0649488515730554e-05,
      "loss": 1.0339,
      "step": 9690
    },
    {
      "epoch": 1.87222543910442,
      "grad_norm": 4.6156182289123535,
      "learning_rate": 1.0639837869137233e-05,
      "loss": 1.6604,
      "step": 9700
    },
    {
      "epoch": 1.8741555684230844,
      "grad_norm": 2.575462579727173,
      "learning_rate": 1.063018722254391e-05,
      "loss": 0.9631,
      "step": 9710
    },
    {
      "epoch": 1.8760856977417486,
      "grad_norm": 34.88881301879883,
      "learning_rate": 1.0620536575950589e-05,
      "loss": 1.4217,
      "step": 9720
    },
    {
      "epoch": 1.878015827060413,
      "grad_norm": 8.36410903930664,
      "learning_rate": 1.0610885929357268e-05,
      "loss": 1.3516,
      "step": 9730
    },
    {
      "epoch": 1.8799459563790775,
      "grad_norm": 6.7024946212768555,
      "learning_rate": 1.0601235282763947e-05,
      "loss": 0.7424,
      "step": 9740
    },
    {
      "epoch": 1.881876085697742,
      "grad_norm": 8.262516021728516,
      "learning_rate": 1.0591584636170624e-05,
      "loss": 1.5324,
      "step": 9750
    },
    {
      "epoch": 1.883806215016406,
      "grad_norm": 10.858196258544922,
      "learning_rate": 1.0581933989577303e-05,
      "loss": 1.2139,
      "step": 9760
    },
    {
      "epoch": 1.8857363443350703,
      "grad_norm": 11.660867691040039,
      "learning_rate": 1.057228334298398e-05,
      "loss": 1.277,
      "step": 9770
    },
    {
      "epoch": 1.8876664736537347,
      "grad_norm": 4.454832553863525,
      "learning_rate": 1.056263269639066e-05,
      "loss": 1.091,
      "step": 9780
    },
    {
      "epoch": 1.8895966029723992,
      "grad_norm": 6.973848342895508,
      "learning_rate": 1.0552982049797338e-05,
      "loss": 1.736,
      "step": 9790
    },
    {
      "epoch": 1.8915267322910636,
      "grad_norm": 14.218497276306152,
      "learning_rate": 1.0543331403204016e-05,
      "loss": 1.0225,
      "step": 9800
    },
    {
      "epoch": 1.8934568616097278,
      "grad_norm": 1.780318260192871,
      "learning_rate": 1.0533680756610694e-05,
      "loss": 0.921,
      "step": 9810
    },
    {
      "epoch": 1.8953869909283922,
      "grad_norm": 13.2127685546875,
      "learning_rate": 1.052403011001737e-05,
      "loss": 2.0769,
      "step": 9820
    },
    {
      "epoch": 1.8973171202470565,
      "grad_norm": 9.275071144104004,
      "learning_rate": 1.051437946342405e-05,
      "loss": 1.5922,
      "step": 9830
    },
    {
      "epoch": 1.8992472495657209,
      "grad_norm": 51.81960678100586,
      "learning_rate": 1.050472881683073e-05,
      "loss": 0.8535,
      "step": 9840
    },
    {
      "epoch": 1.9011773788843853,
      "grad_norm": 2.1399319171905518,
      "learning_rate": 1.0495078170237407e-05,
      "loss": 0.9124,
      "step": 9850
    },
    {
      "epoch": 1.9031075082030497,
      "grad_norm": 125.3017807006836,
      "learning_rate": 1.0485427523644084e-05,
      "loss": 1.2786,
      "step": 9860
    },
    {
      "epoch": 1.905037637521714,
      "grad_norm": 10.639422416687012,
      "learning_rate": 1.0475776877050763e-05,
      "loss": 0.8978,
      "step": 9870
    },
    {
      "epoch": 1.9069677668403782,
      "grad_norm": 18.85337257385254,
      "learning_rate": 1.046612623045744e-05,
      "loss": 1.7634,
      "step": 9880
    },
    {
      "epoch": 1.9088978961590426,
      "grad_norm": 66.59757995605469,
      "learning_rate": 1.045647558386412e-05,
      "loss": 0.8539,
      "step": 9890
    },
    {
      "epoch": 1.910828025477707,
      "grad_norm": 6.2523088455200195,
      "learning_rate": 1.0446824937270798e-05,
      "loss": 1.0622,
      "step": 9900
    },
    {
      "epoch": 1.9127581547963715,
      "grad_norm": 2.196223020553589,
      "learning_rate": 1.0437174290677477e-05,
      "loss": 1.1908,
      "step": 9910
    },
    {
      "epoch": 1.9146882841150357,
      "grad_norm": 6.025065898895264,
      "learning_rate": 1.0427523644084154e-05,
      "loss": 1.4765,
      "step": 9920
    },
    {
      "epoch": 1.9166184134337,
      "grad_norm": 0.3265032470226288,
      "learning_rate": 1.0417872997490833e-05,
      "loss": 1.2673,
      "step": 9930
    },
    {
      "epoch": 1.9185485427523643,
      "grad_norm": 14.985580444335938,
      "learning_rate": 1.040822235089751e-05,
      "loss": 1.6213,
      "step": 9940
    },
    {
      "epoch": 1.9204786720710287,
      "grad_norm": 5.697748184204102,
      "learning_rate": 1.039857170430419e-05,
      "loss": 1.3421,
      "step": 9950
    },
    {
      "epoch": 1.9224088013896932,
      "grad_norm": 15.353781700134277,
      "learning_rate": 1.0388921057710868e-05,
      "loss": 1.7122,
      "step": 9960
    },
    {
      "epoch": 1.9243389307083576,
      "grad_norm": 8.309226036071777,
      "learning_rate": 1.0379270411117546e-05,
      "loss": 1.0401,
      "step": 9970
    },
    {
      "epoch": 1.9262690600270218,
      "grad_norm": 126.41273498535156,
      "learning_rate": 1.0369619764524224e-05,
      "loss": 1.2799,
      "step": 9980
    },
    {
      "epoch": 1.928199189345686,
      "grad_norm": 75.29589080810547,
      "learning_rate": 1.03599691179309e-05,
      "loss": 1.1931,
      "step": 9990
    },
    {
      "epoch": 1.9301293186643504,
      "grad_norm": 2.379621982574463,
      "learning_rate": 1.0350318471337581e-05,
      "loss": 1.408,
      "step": 10000
    },
    {
      "epoch": 1.9320594479830149,
      "grad_norm": 7.388685703277588,
      "learning_rate": 1.034066782474426e-05,
      "loss": 1.2004,
      "step": 10010
    },
    {
      "epoch": 1.9339895773016793,
      "grad_norm": 9.00692081451416,
      "learning_rate": 1.0331017178150937e-05,
      "loss": 0.9623,
      "step": 10020
    },
    {
      "epoch": 1.9359197066203435,
      "grad_norm": 1.923952579498291,
      "learning_rate": 1.0321366531557614e-05,
      "loss": 1.0315,
      "step": 10030
    },
    {
      "epoch": 1.937849835939008,
      "grad_norm": 32.558223724365234,
      "learning_rate": 1.0311715884964293e-05,
      "loss": 1.4457,
      "step": 10040
    },
    {
      "epoch": 1.9397799652576722,
      "grad_norm": 0.880584180355072,
      "learning_rate": 1.030206523837097e-05,
      "loss": 1.0864,
      "step": 10050
    },
    {
      "epoch": 1.9417100945763366,
      "grad_norm": 3.0380666255950928,
      "learning_rate": 1.029241459177765e-05,
      "loss": 0.8124,
      "step": 10060
    },
    {
      "epoch": 1.943640223895001,
      "grad_norm": 2.8925082683563232,
      "learning_rate": 1.0282763945184328e-05,
      "loss": 1.0498,
      "step": 10070
    },
    {
      "epoch": 1.9455703532136654,
      "grad_norm": 14.456965446472168,
      "learning_rate": 1.0273113298591007e-05,
      "loss": 1.1734,
      "step": 10080
    },
    {
      "epoch": 1.9475004825323297,
      "grad_norm": 22.38852882385254,
      "learning_rate": 1.0263462651997684e-05,
      "loss": 1.3204,
      "step": 10090
    },
    {
      "epoch": 1.9494306118509939,
      "grad_norm": 12.212963104248047,
      "learning_rate": 1.0253812005404363e-05,
      "loss": 0.8096,
      "step": 10100
    },
    {
      "epoch": 1.9513607411696583,
      "grad_norm": 30.776569366455078,
      "learning_rate": 1.0244161358811042e-05,
      "loss": 1.3554,
      "step": 10110
    },
    {
      "epoch": 1.9532908704883227,
      "grad_norm": 3.36576247215271,
      "learning_rate": 1.023451071221772e-05,
      "loss": 1.5015,
      "step": 10120
    },
    {
      "epoch": 1.9552209998069872,
      "grad_norm": 3.4271366596221924,
      "learning_rate": 1.0224860065624398e-05,
      "loss": 1.8334,
      "step": 10130
    },
    {
      "epoch": 1.9571511291256514,
      "grad_norm": 6.901360034942627,
      "learning_rate": 1.0215209419031076e-05,
      "loss": 1.2902,
      "step": 10140
    },
    {
      "epoch": 1.9590812584443158,
      "grad_norm": 7.208682060241699,
      "learning_rate": 1.0205558772437754e-05,
      "loss": 1.2254,
      "step": 10150
    },
    {
      "epoch": 1.96101138776298,
      "grad_norm": 16.935935974121094,
      "learning_rate": 1.019590812584443e-05,
      "loss": 0.6189,
      "step": 10160
    },
    {
      "epoch": 1.9629415170816444,
      "grad_norm": 23.76266860961914,
      "learning_rate": 1.0186257479251111e-05,
      "loss": 0.9497,
      "step": 10170
    },
    {
      "epoch": 1.9648716464003089,
      "grad_norm": 40.465389251708984,
      "learning_rate": 1.017660683265779e-05,
      "loss": 0.746,
      "step": 10180
    },
    {
      "epoch": 1.9668017757189733,
      "grad_norm": 1.1390408277511597,
      "learning_rate": 1.0166956186064467e-05,
      "loss": 1.0191,
      "step": 10190
    },
    {
      "epoch": 1.9687319050376375,
      "grad_norm": 8.502421379089355,
      "learning_rate": 1.0157305539471144e-05,
      "loss": 1.4554,
      "step": 10200
    },
    {
      "epoch": 1.9706620343563017,
      "grad_norm": 11.070635795593262,
      "learning_rate": 1.0147654892877823e-05,
      "loss": 1.0464,
      "step": 10210
    },
    {
      "epoch": 1.9725921636749661,
      "grad_norm": 4.143725395202637,
      "learning_rate": 1.0138004246284504e-05,
      "loss": 1.2845,
      "step": 10220
    },
    {
      "epoch": 1.9745222929936306,
      "grad_norm": 154.53158569335938,
      "learning_rate": 1.012835359969118e-05,
      "loss": 2.1753,
      "step": 10230
    },
    {
      "epoch": 1.976452422312295,
      "grad_norm": 9.057817459106445,
      "learning_rate": 1.0118702953097858e-05,
      "loss": 1.4475,
      "step": 10240
    },
    {
      "epoch": 1.9783825516309594,
      "grad_norm": 3.777155637741089,
      "learning_rate": 1.0109052306504537e-05,
      "loss": 1.102,
      "step": 10250
    },
    {
      "epoch": 1.9803126809496236,
      "grad_norm": 27.15715217590332,
      "learning_rate": 1.0099401659911214e-05,
      "loss": 1.819,
      "step": 10260
    },
    {
      "epoch": 1.9822428102682879,
      "grad_norm": 28.050800323486328,
      "learning_rate": 1.0089751013317893e-05,
      "loss": 2.0237,
      "step": 10270
    },
    {
      "epoch": 1.9841729395869523,
      "grad_norm": 64.9788818359375,
      "learning_rate": 1.0080100366724572e-05,
      "loss": 0.8144,
      "step": 10280
    },
    {
      "epoch": 1.9861030689056167,
      "grad_norm": 1.4330254793167114,
      "learning_rate": 1.007044972013125e-05,
      "loss": 1.4902,
      "step": 10290
    },
    {
      "epoch": 1.9880331982242812,
      "grad_norm": 0.9805874228477478,
      "learning_rate": 1.0060799073537928e-05,
      "loss": 1.3896,
      "step": 10300
    },
    {
      "epoch": 1.9899633275429454,
      "grad_norm": 13.307014465332031,
      "learning_rate": 1.0051148426944606e-05,
      "loss": 0.9588,
      "step": 10310
    },
    {
      "epoch": 1.9918934568616096,
      "grad_norm": 3.3397908210754395,
      "learning_rate": 1.0041497780351284e-05,
      "loss": 0.6037,
      "step": 10320
    },
    {
      "epoch": 1.993823586180274,
      "grad_norm": 98.87821197509766,
      "learning_rate": 1.0031847133757964e-05,
      "loss": 1.1385,
      "step": 10330
    },
    {
      "epoch": 1.9957537154989384,
      "grad_norm": 31.696807861328125,
      "learning_rate": 1.0022196487164641e-05,
      "loss": 1.3941,
      "step": 10340
    },
    {
      "epoch": 1.9976838448176029,
      "grad_norm": 7.539937496185303,
      "learning_rate": 1.001254584057132e-05,
      "loss": 1.4861,
      "step": 10350
    },
    {
      "epoch": 1.9996139741362673,
      "grad_norm": 138.3412322998047,
      "learning_rate": 1.0002895193977997e-05,
      "loss": 1.9651,
      "step": 10360
    },
    {
      "epoch": 2.0,
      "eval_intent_acc": 0.8263888888888888,
      "eval_loss": 1.4758542776107788,
      "eval_runtime": 22.8247,
      "eval_samples_per_second": 50.472,
      "eval_slot_f1": 0.5635540929658577,
      "eval_slot_precision": 0.5293663060278208,
      "eval_slot_recall": 0.6024626209322779,
      "eval_steps_per_second": 25.236,
      "step": 10362
    },
    {
      "epoch": 2.0015441034549313,
      "grad_norm": 6.841487884521484,
      "learning_rate": 9.993244547384674e-06,
      "loss": 0.9401,
      "step": 10370
    },
    {
      "epoch": 2.0034742327735957,
      "grad_norm": 5.968897342681885,
      "learning_rate": 9.983593900791355e-06,
      "loss": 1.0575,
      "step": 10380
    },
    {
      "epoch": 2.00540436209226,
      "grad_norm": 6.475261211395264,
      "learning_rate": 9.973943254198032e-06,
      "loss": 1.0271,
      "step": 10390
    },
    {
      "epoch": 2.0073344914109246,
      "grad_norm": 2.0926835536956787,
      "learning_rate": 9.96429260760471e-06,
      "loss": 0.9605,
      "step": 10400
    },
    {
      "epoch": 2.009264620729589,
      "grad_norm": 2.6866207122802734,
      "learning_rate": 9.954641961011388e-06,
      "loss": 0.6369,
      "step": 10410
    },
    {
      "epoch": 2.0111947500482534,
      "grad_norm": 11.659452438354492,
      "learning_rate": 9.944991314418067e-06,
      "loss": 1.0832,
      "step": 10420
    },
    {
      "epoch": 2.0131248793669174,
      "grad_norm": 1.5955754518508911,
      "learning_rate": 9.935340667824746e-06,
      "loss": 0.7969,
      "step": 10430
    },
    {
      "epoch": 2.015055008685582,
      "grad_norm": 1.0458060503005981,
      "learning_rate": 9.925690021231423e-06,
      "loss": 2.1068,
      "step": 10440
    },
    {
      "epoch": 2.0169851380042463,
      "grad_norm": 29.662540435791016,
      "learning_rate": 9.916039374638102e-06,
      "loss": 1.2443,
      "step": 10450
    },
    {
      "epoch": 2.0189152673229107,
      "grad_norm": 9.23985481262207,
      "learning_rate": 9.90638872804478e-06,
      "loss": 1.1678,
      "step": 10460
    },
    {
      "epoch": 2.020845396641575,
      "grad_norm": 6.391180992126465,
      "learning_rate": 9.896738081451458e-06,
      "loss": 0.8107,
      "step": 10470
    },
    {
      "epoch": 2.022775525960239,
      "grad_norm": 78.91726684570312,
      "learning_rate": 9.887087434858136e-06,
      "loss": 1.288,
      "step": 10480
    },
    {
      "epoch": 2.0247056552789036,
      "grad_norm": 1.8325605392456055,
      "learning_rate": 9.877436788264815e-06,
      "loss": 1.0553,
      "step": 10490
    },
    {
      "epoch": 2.026635784597568,
      "grad_norm": 61.646522521972656,
      "learning_rate": 9.867786141671492e-06,
      "loss": 0.9913,
      "step": 10500
    },
    {
      "epoch": 2.0285659139162324,
      "grad_norm": 28.54792022705078,
      "learning_rate": 9.858135495078171e-06,
      "loss": 1.2927,
      "step": 10510
    },
    {
      "epoch": 2.030496043234897,
      "grad_norm": 8.3457612991333,
      "learning_rate": 9.84848484848485e-06,
      "loss": 1.499,
      "step": 10520
    },
    {
      "epoch": 2.0324261725535613,
      "grad_norm": 9.623222351074219,
      "learning_rate": 9.838834201891527e-06,
      "loss": 1.9004,
      "step": 10530
    },
    {
      "epoch": 2.0343563018722253,
      "grad_norm": 5.633960247039795,
      "learning_rate": 9.829183555298206e-06,
      "loss": 0.7523,
      "step": 10540
    },
    {
      "epoch": 2.0362864311908897,
      "grad_norm": 1.0453941822052002,
      "learning_rate": 9.819532908704885e-06,
      "loss": 0.9228,
      "step": 10550
    },
    {
      "epoch": 2.038216560509554,
      "grad_norm": 6.294490814208984,
      "learning_rate": 9.809882262111562e-06,
      "loss": 1.425,
      "step": 10560
    },
    {
      "epoch": 2.0401466898282186,
      "grad_norm": 3.3903396129608154,
      "learning_rate": 9.80023161551824e-06,
      "loss": 0.8707,
      "step": 10570
    },
    {
      "epoch": 2.042076819146883,
      "grad_norm": 3.6562023162841797,
      "learning_rate": 9.790580968924918e-06,
      "loss": 0.8812,
      "step": 10580
    },
    {
      "epoch": 2.044006948465547,
      "grad_norm": 29.213003158569336,
      "learning_rate": 9.780930322331597e-06,
      "loss": 1.0539,
      "step": 10590
    },
    {
      "epoch": 2.0459370777842114,
      "grad_norm": 1.3178749084472656,
      "learning_rate": 9.771279675738276e-06,
      "loss": 1.0355,
      "step": 10600
    },
    {
      "epoch": 2.047867207102876,
      "grad_norm": 0.779985785484314,
      "learning_rate": 9.761629029144953e-06,
      "loss": 0.9408,
      "step": 10610
    },
    {
      "epoch": 2.0497973364215403,
      "grad_norm": 73.80583953857422,
      "learning_rate": 9.751978382551632e-06,
      "loss": 2.4403,
      "step": 10620
    },
    {
      "epoch": 2.0517274657402047,
      "grad_norm": 136.0797882080078,
      "learning_rate": 9.74232773595831e-06,
      "loss": 0.9623,
      "step": 10630
    },
    {
      "epoch": 2.053657595058869,
      "grad_norm": 16.512645721435547,
      "learning_rate": 9.732677089364988e-06,
      "loss": 0.9356,
      "step": 10640
    },
    {
      "epoch": 2.055587724377533,
      "grad_norm": 105.91151428222656,
      "learning_rate": 9.723026442771666e-06,
      "loss": 1.5613,
      "step": 10650
    },
    {
      "epoch": 2.0575178536961976,
      "grad_norm": 47.11990737915039,
      "learning_rate": 9.713375796178345e-06,
      "loss": 2.032,
      "step": 10660
    },
    {
      "epoch": 2.059447983014862,
      "grad_norm": 8.84164810180664,
      "learning_rate": 9.703725149585022e-06,
      "loss": 0.8434,
      "step": 10670
    },
    {
      "epoch": 2.0613781123335264,
      "grad_norm": 14.757761001586914,
      "learning_rate": 9.694074502991701e-06,
      "loss": 1.006,
      "step": 10680
    },
    {
      "epoch": 2.063308241652191,
      "grad_norm": 11.038930892944336,
      "learning_rate": 9.68442385639838e-06,
      "loss": 1.8189,
      "step": 10690
    },
    {
      "epoch": 2.065238370970855,
      "grad_norm": 6.850643634796143,
      "learning_rate": 9.674773209805057e-06,
      "loss": 1.2602,
      "step": 10700
    },
    {
      "epoch": 2.0671685002895193,
      "grad_norm": 55.86719512939453,
      "learning_rate": 9.665122563211736e-06,
      "loss": 1.4418,
      "step": 10710
    },
    {
      "epoch": 2.0690986296081837,
      "grad_norm": 9.988048553466797,
      "learning_rate": 9.655471916618415e-06,
      "loss": 1.4663,
      "step": 10720
    },
    {
      "epoch": 2.071028758926848,
      "grad_norm": 16.76936149597168,
      "learning_rate": 9.645821270025092e-06,
      "loss": 0.4842,
      "step": 10730
    },
    {
      "epoch": 2.0729588882455126,
      "grad_norm": 7.904957294464111,
      "learning_rate": 9.63617062343177e-06,
      "loss": 1.6897,
      "step": 10740
    },
    {
      "epoch": 2.074889017564177,
      "grad_norm": 8.358598709106445,
      "learning_rate": 9.626519976838448e-06,
      "loss": 1.6181,
      "step": 10750
    },
    {
      "epoch": 2.076819146882841,
      "grad_norm": 7.151403903961182,
      "learning_rate": 9.616869330245128e-06,
      "loss": 1.281,
      "step": 10760
    },
    {
      "epoch": 2.0787492762015054,
      "grad_norm": 62.1771125793457,
      "learning_rate": 9.607218683651806e-06,
      "loss": 1.5906,
      "step": 10770
    },
    {
      "epoch": 2.08067940552017,
      "grad_norm": 10.173338890075684,
      "learning_rate": 9.597568037058483e-06,
      "loss": 1.2101,
      "step": 10780
    },
    {
      "epoch": 2.0826095348388343,
      "grad_norm": 14.204825401306152,
      "learning_rate": 9.587917390465162e-06,
      "loss": 0.9725,
      "step": 10790
    },
    {
      "epoch": 2.0845396641574987,
      "grad_norm": 163.94703674316406,
      "learning_rate": 9.57826674387184e-06,
      "loss": 1.0065,
      "step": 10800
    },
    {
      "epoch": 2.086469793476163,
      "grad_norm": 0.712289035320282,
      "learning_rate": 9.568616097278518e-06,
      "loss": 1.247,
      "step": 10810
    },
    {
      "epoch": 2.088399922794827,
      "grad_norm": 3.488430976867676,
      "learning_rate": 9.558965450685196e-06,
      "loss": 1.3421,
      "step": 10820
    },
    {
      "epoch": 2.0903300521134915,
      "grad_norm": 24.056488037109375,
      "learning_rate": 9.549314804091875e-06,
      "loss": 1.197,
      "step": 10830
    },
    {
      "epoch": 2.092260181432156,
      "grad_norm": 1.6282920837402344,
      "learning_rate": 9.539664157498552e-06,
      "loss": 1.1586,
      "step": 10840
    },
    {
      "epoch": 2.0941903107508204,
      "grad_norm": 1.765645146369934,
      "learning_rate": 9.530013510905231e-06,
      "loss": 0.6978,
      "step": 10850
    },
    {
      "epoch": 2.096120440069485,
      "grad_norm": 4.401559352874756,
      "learning_rate": 9.52036286431191e-06,
      "loss": 0.9286,
      "step": 10860
    },
    {
      "epoch": 2.098050569388149,
      "grad_norm": 14.81424331665039,
      "learning_rate": 9.510712217718589e-06,
      "loss": 1.535,
      "step": 10870
    },
    {
      "epoch": 2.0999806987068133,
      "grad_norm": 35.021610260009766,
      "learning_rate": 9.501061571125266e-06,
      "loss": 1.0979,
      "step": 10880
    },
    {
      "epoch": 2.1019108280254777,
      "grad_norm": 13.785571098327637,
      "learning_rate": 9.491410924531945e-06,
      "loss": 1.3955,
      "step": 10890
    },
    {
      "epoch": 2.103840957344142,
      "grad_norm": 3.0849976539611816,
      "learning_rate": 9.481760277938624e-06,
      "loss": 0.5389,
      "step": 10900
    },
    {
      "epoch": 2.1057710866628065,
      "grad_norm": 3.4669277667999268,
      "learning_rate": 9.4721096313453e-06,
      "loss": 0.7119,
      "step": 10910
    },
    {
      "epoch": 2.1077012159814705,
      "grad_norm": 26.903703689575195,
      "learning_rate": 9.462458984751978e-06,
      "loss": 1.4713,
      "step": 10920
    },
    {
      "epoch": 2.109631345300135,
      "grad_norm": 4.966637134552002,
      "learning_rate": 9.452808338158658e-06,
      "loss": 2.2969,
      "step": 10930
    },
    {
      "epoch": 2.1115614746187994,
      "grad_norm": 5.281750202178955,
      "learning_rate": 9.443157691565336e-06,
      "loss": 0.9639,
      "step": 10940
    },
    {
      "epoch": 2.113491603937464,
      "grad_norm": 2.202025890350342,
      "learning_rate": 9.433507044972013e-06,
      "loss": 0.6523,
      "step": 10950
    },
    {
      "epoch": 2.1154217332561283,
      "grad_norm": 16.888355255126953,
      "learning_rate": 9.423856398378692e-06,
      "loss": 0.8969,
      "step": 10960
    },
    {
      "epoch": 2.1173518625747927,
      "grad_norm": 1.7902430295944214,
      "learning_rate": 9.41420575178537e-06,
      "loss": 1.1623,
      "step": 10970
    },
    {
      "epoch": 2.1192819918934567,
      "grad_norm": 21.42052459716797,
      "learning_rate": 9.40455510519205e-06,
      "loss": 0.5233,
      "step": 10980
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 0.284784734249115,
      "learning_rate": 9.394904458598726e-06,
      "loss": 0.5153,
      "step": 10990
    },
    {
      "epoch": 2.1231422505307855,
      "grad_norm": 8.053227424621582,
      "learning_rate": 9.385253812005405e-06,
      "loss": 0.516,
      "step": 11000
    },
    {
      "epoch": 2.12507237984945,
      "grad_norm": 3.5991756916046143,
      "learning_rate": 9.375603165412084e-06,
      "loss": 0.9775,
      "step": 11010
    },
    {
      "epoch": 2.1270025091681144,
      "grad_norm": 47.57279586791992,
      "learning_rate": 9.365952518818761e-06,
      "loss": 0.8379,
      "step": 11020
    },
    {
      "epoch": 2.128932638486779,
      "grad_norm": 3.6179862022399902,
      "learning_rate": 9.35630187222544e-06,
      "loss": 0.507,
      "step": 11030
    },
    {
      "epoch": 2.130862767805443,
      "grad_norm": 10.708142280578613,
      "learning_rate": 9.346651225632119e-06,
      "loss": 1.7589,
      "step": 11040
    },
    {
      "epoch": 2.1327928971241072,
      "grad_norm": 29.7630558013916,
      "learning_rate": 9.337000579038796e-06,
      "loss": 1.3403,
      "step": 11050
    },
    {
      "epoch": 2.1347230264427717,
      "grad_norm": 2.5747454166412354,
      "learning_rate": 9.327349932445475e-06,
      "loss": 0.8789,
      "step": 11060
    },
    {
      "epoch": 2.136653155761436,
      "grad_norm": 1.846406102180481,
      "learning_rate": 9.317699285852154e-06,
      "loss": 0.8837,
      "step": 11070
    },
    {
      "epoch": 2.1385832850801005,
      "grad_norm": 5.074102878570557,
      "learning_rate": 9.30804863925883e-06,
      "loss": 0.8179,
      "step": 11080
    },
    {
      "epoch": 2.1405134143987645,
      "grad_norm": 1.5004826784133911,
      "learning_rate": 9.298397992665508e-06,
      "loss": 1.4071,
      "step": 11090
    },
    {
      "epoch": 2.142443543717429,
      "grad_norm": 3.575838088989258,
      "learning_rate": 9.288747346072189e-06,
      "loss": 0.8128,
      "step": 11100
    },
    {
      "epoch": 2.1443736730360934,
      "grad_norm": 4.3877644538879395,
      "learning_rate": 9.279096699478866e-06,
      "loss": 0.7968,
      "step": 11110
    },
    {
      "epoch": 2.146303802354758,
      "grad_norm": 176.07821655273438,
      "learning_rate": 9.269446052885544e-06,
      "loss": 1.0802,
      "step": 11120
    },
    {
      "epoch": 2.1482339316734222,
      "grad_norm": 225.66302490234375,
      "learning_rate": 9.259795406292222e-06,
      "loss": 1.5863,
      "step": 11130
    },
    {
      "epoch": 2.1501640609920862,
      "grad_norm": 14.852408409118652,
      "learning_rate": 9.2501447596989e-06,
      "loss": 1.33,
      "step": 11140
    },
    {
      "epoch": 2.1520941903107507,
      "grad_norm": 18.215837478637695,
      "learning_rate": 9.24049411310558e-06,
      "loss": 1.0971,
      "step": 11150
    },
    {
      "epoch": 2.154024319629415,
      "grad_norm": 37.100502014160156,
      "learning_rate": 9.230843466512256e-06,
      "loss": 0.8154,
      "step": 11160
    },
    {
      "epoch": 2.1559544489480795,
      "grad_norm": 4.255146503448486,
      "learning_rate": 9.221192819918935e-06,
      "loss": 0.955,
      "step": 11170
    },
    {
      "epoch": 2.157884578266744,
      "grad_norm": 1.7193673849105835,
      "learning_rate": 9.211542173325614e-06,
      "loss": 0.848,
      "step": 11180
    },
    {
      "epoch": 2.1598147075854084,
      "grad_norm": 11.738680839538574,
      "learning_rate": 9.201891526732291e-06,
      "loss": 1.4608,
      "step": 11190
    },
    {
      "epoch": 2.1617448369040724,
      "grad_norm": 15.966288566589355,
      "learning_rate": 9.19224088013897e-06,
      "loss": 0.6279,
      "step": 11200
    },
    {
      "epoch": 2.163674966222737,
      "grad_norm": 3.2074379920959473,
      "learning_rate": 9.182590233545649e-06,
      "loss": 1.0824,
      "step": 11210
    },
    {
      "epoch": 2.1656050955414012,
      "grad_norm": 118.26274871826172,
      "learning_rate": 9.172939586952326e-06,
      "loss": 1.6591,
      "step": 11220
    },
    {
      "epoch": 2.1675352248600657,
      "grad_norm": 23.46201515197754,
      "learning_rate": 9.163288940359005e-06,
      "loss": 0.8197,
      "step": 11230
    },
    {
      "epoch": 2.16946535417873,
      "grad_norm": 131.34104919433594,
      "learning_rate": 9.153638293765684e-06,
      "loss": 0.9523,
      "step": 11240
    },
    {
      "epoch": 2.1713954834973945,
      "grad_norm": 4.407690525054932,
      "learning_rate": 9.14398764717236e-06,
      "loss": 0.8686,
      "step": 11250
    },
    {
      "epoch": 2.1733256128160585,
      "grad_norm": 7.347929954528809,
      "learning_rate": 9.13433700057904e-06,
      "loss": 0.7688,
      "step": 11260
    },
    {
      "epoch": 2.175255742134723,
      "grad_norm": 2.639848232269287,
      "learning_rate": 9.124686353985719e-06,
      "loss": 0.8375,
      "step": 11270
    },
    {
      "epoch": 2.1771858714533874,
      "grad_norm": 4.224904537200928,
      "learning_rate": 9.115035707392396e-06,
      "loss": 1.3353,
      "step": 11280
    },
    {
      "epoch": 2.179116000772052,
      "grad_norm": 7.681166648864746,
      "learning_rate": 9.105385060799074e-06,
      "loss": 0.7248,
      "step": 11290
    },
    {
      "epoch": 2.1810461300907162,
      "grad_norm": 13.662412643432617,
      "learning_rate": 9.095734414205752e-06,
      "loss": 1.0672,
      "step": 11300
    },
    {
      "epoch": 2.1829762594093802,
      "grad_norm": 27.70073699951172,
      "learning_rate": 9.08608376761243e-06,
      "loss": 1.4443,
      "step": 11310
    },
    {
      "epoch": 2.1849063887280447,
      "grad_norm": 6.867692470550537,
      "learning_rate": 9.07643312101911e-06,
      "loss": 0.5702,
      "step": 11320
    },
    {
      "epoch": 2.186836518046709,
      "grad_norm": 10.48815631866455,
      "learning_rate": 9.066782474425786e-06,
      "loss": 0.9334,
      "step": 11330
    },
    {
      "epoch": 2.1887666473653735,
      "grad_norm": 84.435302734375,
      "learning_rate": 9.057131827832465e-06,
      "loss": 1.0253,
      "step": 11340
    },
    {
      "epoch": 2.190696776684038,
      "grad_norm": 6.1634063720703125,
      "learning_rate": 9.047481181239144e-06,
      "loss": 0.7651,
      "step": 11350
    },
    {
      "epoch": 2.1926269060027024,
      "grad_norm": 6.123706817626953,
      "learning_rate": 9.037830534645821e-06,
      "loss": 1.0617,
      "step": 11360
    },
    {
      "epoch": 2.1945570353213664,
      "grad_norm": 18.24344825744629,
      "learning_rate": 9.0281798880525e-06,
      "loss": 0.507,
      "step": 11370
    },
    {
      "epoch": 2.196487164640031,
      "grad_norm": 12.04784870147705,
      "learning_rate": 9.018529241459179e-06,
      "loss": 1.7854,
      "step": 11380
    },
    {
      "epoch": 2.1984172939586952,
      "grad_norm": 118.82364654541016,
      "learning_rate": 9.008878594865856e-06,
      "loss": 0.7334,
      "step": 11390
    },
    {
      "epoch": 2.2003474232773597,
      "grad_norm": 66.89726257324219,
      "learning_rate": 8.999227948272535e-06,
      "loss": 0.8098,
      "step": 11400
    },
    {
      "epoch": 2.202277552596024,
      "grad_norm": 62.56586837768555,
      "learning_rate": 8.989577301679214e-06,
      "loss": 1.6327,
      "step": 11410
    },
    {
      "epoch": 2.204207681914688,
      "grad_norm": 4.239198207855225,
      "learning_rate": 8.97992665508589e-06,
      "loss": 1.0803,
      "step": 11420
    },
    {
      "epoch": 2.2061378112333525,
      "grad_norm": 10.683871269226074,
      "learning_rate": 8.97027600849257e-06,
      "loss": 0.9794,
      "step": 11430
    },
    {
      "epoch": 2.208067940552017,
      "grad_norm": 13.617525100708008,
      "learning_rate": 8.960625361899249e-06,
      "loss": 1.3934,
      "step": 11440
    },
    {
      "epoch": 2.2099980698706814,
      "grad_norm": 2.912564277648926,
      "learning_rate": 8.950974715305927e-06,
      "loss": 0.9585,
      "step": 11450
    },
    {
      "epoch": 2.211928199189346,
      "grad_norm": 2.5843825340270996,
      "learning_rate": 8.941324068712604e-06,
      "loss": 1.524,
      "step": 11460
    },
    {
      "epoch": 2.2138583285080102,
      "grad_norm": 13.917201042175293,
      "learning_rate": 8.931673422119282e-06,
      "loss": 1.3962,
      "step": 11470
    },
    {
      "epoch": 2.215788457826674,
      "grad_norm": 84.43379211425781,
      "learning_rate": 8.922022775525962e-06,
      "loss": 1.3736,
      "step": 11480
    },
    {
      "epoch": 2.2177185871453386,
      "grad_norm": 15.029111862182617,
      "learning_rate": 8.91237212893264e-06,
      "loss": 1.7166,
      "step": 11490
    },
    {
      "epoch": 2.219648716464003,
      "grad_norm": 6.677371978759766,
      "learning_rate": 8.902721482339316e-06,
      "loss": 1.1633,
      "step": 11500
    },
    {
      "epoch": 2.2215788457826675,
      "grad_norm": 157.09474182128906,
      "learning_rate": 8.893070835745995e-06,
      "loss": 1.5728,
      "step": 11510
    },
    {
      "epoch": 2.223508975101332,
      "grad_norm": 30.881074905395508,
      "learning_rate": 8.883420189152674e-06,
      "loss": 1.3969,
      "step": 11520
    },
    {
      "epoch": 2.225439104419996,
      "grad_norm": 53.378387451171875,
      "learning_rate": 8.873769542559351e-06,
      "loss": 0.8126,
      "step": 11530
    },
    {
      "epoch": 2.2273692337386604,
      "grad_norm": 5.891091823577881,
      "learning_rate": 8.86411889596603e-06,
      "loss": 0.6895,
      "step": 11540
    },
    {
      "epoch": 2.229299363057325,
      "grad_norm": 24.989765167236328,
      "learning_rate": 8.854468249372709e-06,
      "loss": 0.8738,
      "step": 11550
    },
    {
      "epoch": 2.231229492375989,
      "grad_norm": 5.581233024597168,
      "learning_rate": 8.844817602779386e-06,
      "loss": 1.3286,
      "step": 11560
    },
    {
      "epoch": 2.2331596216946537,
      "grad_norm": 5.33955717086792,
      "learning_rate": 8.835166956186065e-06,
      "loss": 1.0552,
      "step": 11570
    },
    {
      "epoch": 2.235089751013318,
      "grad_norm": 0.12218547612428665,
      "learning_rate": 8.825516309592744e-06,
      "loss": 0.6549,
      "step": 11580
    },
    {
      "epoch": 2.237019880331982,
      "grad_norm": 0.4472268521785736,
      "learning_rate": 8.815865662999423e-06,
      "loss": 1.0943,
      "step": 11590
    },
    {
      "epoch": 2.2389500096506465,
      "grad_norm": 2.206681489944458,
      "learning_rate": 8.8062150164061e-06,
      "loss": 1.0502,
      "step": 11600
    },
    {
      "epoch": 2.240880138969311,
      "grad_norm": 13.215664863586426,
      "learning_rate": 8.796564369812779e-06,
      "loss": 1.4665,
      "step": 11610
    },
    {
      "epoch": 2.2428102682879754,
      "grad_norm": 6.623754024505615,
      "learning_rate": 8.786913723219457e-06,
      "loss": 0.8169,
      "step": 11620
    },
    {
      "epoch": 2.24474039760664,
      "grad_norm": 0.5930089354515076,
      "learning_rate": 8.777263076626134e-06,
      "loss": 0.7088,
      "step": 11630
    },
    {
      "epoch": 2.2466705269253042,
      "grad_norm": 2.3894920349121094,
      "learning_rate": 8.767612430032813e-06,
      "loss": 1.7391,
      "step": 11640
    },
    {
      "epoch": 2.248600656243968,
      "grad_norm": 0.7530078291893005,
      "learning_rate": 8.757961783439492e-06,
      "loss": 0.6959,
      "step": 11650
    },
    {
      "epoch": 2.2505307855626326,
      "grad_norm": 38.49658203125,
      "learning_rate": 8.74831113684617e-06,
      "loss": 1.1604,
      "step": 11660
    },
    {
      "epoch": 2.252460914881297,
      "grad_norm": 17.484088897705078,
      "learning_rate": 8.738660490252846e-06,
      "loss": 1.7547,
      "step": 11670
    },
    {
      "epoch": 2.2543910441999615,
      "grad_norm": 68.44469451904297,
      "learning_rate": 8.729009843659525e-06,
      "loss": 1.2012,
      "step": 11680
    },
    {
      "epoch": 2.256321173518626,
      "grad_norm": 55.821990966796875,
      "learning_rate": 8.719359197066204e-06,
      "loss": 1.55,
      "step": 11690
    },
    {
      "epoch": 2.25825130283729,
      "grad_norm": 5.922891616821289,
      "learning_rate": 8.709708550472883e-06,
      "loss": 1.4228,
      "step": 11700
    },
    {
      "epoch": 2.2601814321559544,
      "grad_norm": 6.502405166625977,
      "learning_rate": 8.70005790387956e-06,
      "loss": 0.8644,
      "step": 11710
    },
    {
      "epoch": 2.262111561474619,
      "grad_norm": 49.810646057128906,
      "learning_rate": 8.690407257286239e-06,
      "loss": 1.0892,
      "step": 11720
    },
    {
      "epoch": 2.264041690793283,
      "grad_norm": 3.002746820449829,
      "learning_rate": 8.680756610692918e-06,
      "loss": 0.6572,
      "step": 11730
    },
    {
      "epoch": 2.2659718201119476,
      "grad_norm": 0.5237608551979065,
      "learning_rate": 8.671105964099595e-06,
      "loss": 0.3412,
      "step": 11740
    },
    {
      "epoch": 2.2679019494306116,
      "grad_norm": 1.3102209568023682,
      "learning_rate": 8.661455317506274e-06,
      "loss": 1.1908,
      "step": 11750
    },
    {
      "epoch": 2.269832078749276,
      "grad_norm": 3.401231288909912,
      "learning_rate": 8.651804670912953e-06,
      "loss": 0.9892,
      "step": 11760
    },
    {
      "epoch": 2.2717622080679405,
      "grad_norm": 9.56171703338623,
      "learning_rate": 8.64215402431963e-06,
      "loss": 0.6978,
      "step": 11770
    },
    {
      "epoch": 2.273692337386605,
      "grad_norm": 55.253021240234375,
      "learning_rate": 8.632503377726309e-06,
      "loss": 0.7971,
      "step": 11780
    },
    {
      "epoch": 2.2756224667052694,
      "grad_norm": 8.989423751831055,
      "learning_rate": 8.622852731132987e-06,
      "loss": 1.3243,
      "step": 11790
    },
    {
      "epoch": 2.277552596023934,
      "grad_norm": 7.266956329345703,
      "learning_rate": 8.613202084539664e-06,
      "loss": 1.2304,
      "step": 11800
    },
    {
      "epoch": 2.2794827253425978,
      "grad_norm": 36.7454948425293,
      "learning_rate": 8.603551437946343e-06,
      "loss": 1.1607,
      "step": 11810
    },
    {
      "epoch": 2.281412854661262,
      "grad_norm": 53.97970199584961,
      "learning_rate": 8.593900791353022e-06,
      "loss": 1.8497,
      "step": 11820
    },
    {
      "epoch": 2.2833429839799266,
      "grad_norm": 9.515898704528809,
      "learning_rate": 8.5842501447597e-06,
      "loss": 0.5799,
      "step": 11830
    },
    {
      "epoch": 2.285273113298591,
      "grad_norm": 49.33531951904297,
      "learning_rate": 8.574599498166378e-06,
      "loss": 1.3174,
      "step": 11840
    },
    {
      "epoch": 2.2872032426172555,
      "grad_norm": 4.960328102111816,
      "learning_rate": 8.564948851573055e-06,
      "loss": 0.7822,
      "step": 11850
    },
    {
      "epoch": 2.28913337193592,
      "grad_norm": 366.3184509277344,
      "learning_rate": 8.555298204979734e-06,
      "loss": 1.3418,
      "step": 11860
    },
    {
      "epoch": 2.291063501254584,
      "grad_norm": 11.768957138061523,
      "learning_rate": 8.545647558386413e-06,
      "loss": 0.676,
      "step": 11870
    },
    {
      "epoch": 2.2929936305732483,
      "grad_norm": 13.045269012451172,
      "learning_rate": 8.53599691179309e-06,
      "loss": 1.0172,
      "step": 11880
    },
    {
      "epoch": 2.2949237598919128,
      "grad_norm": 2.708756446838379,
      "learning_rate": 8.526346265199769e-06,
      "loss": 1.0692,
      "step": 11890
    },
    {
      "epoch": 2.296853889210577,
      "grad_norm": 1.2066072225570679,
      "learning_rate": 8.516695618606448e-06,
      "loss": 0.8933,
      "step": 11900
    },
    {
      "epoch": 2.2987840185292416,
      "grad_norm": 8.10845947265625,
      "learning_rate": 8.507044972013125e-06,
      "loss": 1.2377,
      "step": 11910
    },
    {
      "epoch": 2.3007141478479056,
      "grad_norm": 49.35735321044922,
      "learning_rate": 8.497394325419804e-06,
      "loss": 0.8615,
      "step": 11920
    },
    {
      "epoch": 2.30264427716657,
      "grad_norm": 2.2436251640319824,
      "learning_rate": 8.487743678826483e-06,
      "loss": 0.8547,
      "step": 11930
    },
    {
      "epoch": 2.3045744064852345,
      "grad_norm": 21.095487594604492,
      "learning_rate": 8.47809303223316e-06,
      "loss": 1.3915,
      "step": 11940
    },
    {
      "epoch": 2.306504535803899,
      "grad_norm": 0.40182504057884216,
      "learning_rate": 8.468442385639839e-06,
      "loss": 1.4282,
      "step": 11950
    },
    {
      "epoch": 2.3084346651225633,
      "grad_norm": 8.568990707397461,
      "learning_rate": 8.458791739046517e-06,
      "loss": 0.7769,
      "step": 11960
    },
    {
      "epoch": 2.3103647944412273,
      "grad_norm": 4.82424259185791,
      "learning_rate": 8.449141092453195e-06,
      "loss": 0.8052,
      "step": 11970
    },
    {
      "epoch": 2.3122949237598918,
      "grad_norm": 80.43732452392578,
      "learning_rate": 8.439490445859873e-06,
      "loss": 1.7356,
      "step": 11980
    },
    {
      "epoch": 2.314225053078556,
      "grad_norm": 53.37574005126953,
      "learning_rate": 8.429839799266552e-06,
      "loss": 1.0541,
      "step": 11990
    },
    {
      "epoch": 2.3161551823972206,
      "grad_norm": 3.7425155639648438,
      "learning_rate": 8.42018915267323e-06,
      "loss": 0.6441,
      "step": 12000
    },
    {
      "epoch": 2.318085311715885,
      "grad_norm": 102.47161865234375,
      "learning_rate": 8.410538506079908e-06,
      "loss": 1.161,
      "step": 12010
    },
    {
      "epoch": 2.3200154410345495,
      "grad_norm": 24.77568244934082,
      "learning_rate": 8.400887859486587e-06,
      "loss": 0.9824,
      "step": 12020
    },
    {
      "epoch": 2.3219455703532135,
      "grad_norm": 0.7110438346862793,
      "learning_rate": 8.391237212893264e-06,
      "loss": 1.3897,
      "step": 12030
    },
    {
      "epoch": 2.323875699671878,
      "grad_norm": 7.037156581878662,
      "learning_rate": 8.381586566299943e-06,
      "loss": 1.0518,
      "step": 12040
    },
    {
      "epoch": 2.3258058289905423,
      "grad_norm": 4.614030838012695,
      "learning_rate": 8.37193591970662e-06,
      "loss": 1.3464,
      "step": 12050
    },
    {
      "epoch": 2.3277359583092068,
      "grad_norm": 1.690311312675476,
      "learning_rate": 8.362285273113299e-06,
      "loss": 0.9861,
      "step": 12060
    },
    {
      "epoch": 2.329666087627871,
      "grad_norm": 2.6444640159606934,
      "learning_rate": 8.352634626519978e-06,
      "loss": 1.1618,
      "step": 12070
    },
    {
      "epoch": 2.3315962169465356,
      "grad_norm": 10.354907035827637,
      "learning_rate": 8.342983979926655e-06,
      "loss": 1.0685,
      "step": 12080
    },
    {
      "epoch": 2.3335263462651996,
      "grad_norm": 1.959285855293274,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.4853,
      "step": 12090
    },
    {
      "epoch": 2.335456475583864,
      "grad_norm": 4.434115886688232,
      "learning_rate": 8.323682686740013e-06,
      "loss": 1.5945,
      "step": 12100
    },
    {
      "epoch": 2.3373866049025285,
      "grad_norm": 0.688952624797821,
      "learning_rate": 8.31403204014669e-06,
      "loss": 0.6672,
      "step": 12110
    },
    {
      "epoch": 2.339316734221193,
      "grad_norm": 58.416473388671875,
      "learning_rate": 8.304381393553369e-06,
      "loss": 0.7194,
      "step": 12120
    },
    {
      "epoch": 2.3412468635398573,
      "grad_norm": 2.285308361053467,
      "learning_rate": 8.294730746960047e-06,
      "loss": 0.8124,
      "step": 12130
    },
    {
      "epoch": 2.3431769928585213,
      "grad_norm": 77.8676986694336,
      "learning_rate": 8.285080100366725e-06,
      "loss": 1.2454,
      "step": 12140
    },
    {
      "epoch": 2.3451071221771858,
      "grad_norm": 2.7051727771759033,
      "learning_rate": 8.275429453773403e-06,
      "loss": 0.9849,
      "step": 12150
    },
    {
      "epoch": 2.34703725149585,
      "grad_norm": 2.9037318229675293,
      "learning_rate": 8.265778807180082e-06,
      "loss": 0.9586,
      "step": 12160
    },
    {
      "epoch": 2.3489673808145146,
      "grad_norm": 3.4001851081848145,
      "learning_rate": 8.256128160586761e-06,
      "loss": 0.8743,
      "step": 12170
    },
    {
      "epoch": 2.350897510133179,
      "grad_norm": 36.61991882324219,
      "learning_rate": 8.246477513993438e-06,
      "loss": 0.8501,
      "step": 12180
    },
    {
      "epoch": 2.352827639451843,
      "grad_norm": 21.359878540039062,
      "learning_rate": 8.236826867400117e-06,
      "loss": 1.68,
      "step": 12190
    },
    {
      "epoch": 2.3547577687705075,
      "grad_norm": 0.16435742378234863,
      "learning_rate": 8.227176220806796e-06,
      "loss": 1.1028,
      "step": 12200
    },
    {
      "epoch": 2.356687898089172,
      "grad_norm": 15.511855125427246,
      "learning_rate": 8.217525574213473e-06,
      "loss": 1.4623,
      "step": 12210
    },
    {
      "epoch": 2.3586180274078363,
      "grad_norm": 7.7133469581604,
      "learning_rate": 8.20787492762015e-06,
      "loss": 1.2158,
      "step": 12220
    },
    {
      "epoch": 2.3605481567265008,
      "grad_norm": 41.87059020996094,
      "learning_rate": 8.198224281026829e-06,
      "loss": 1.2363,
      "step": 12230
    },
    {
      "epoch": 2.362478286045165,
      "grad_norm": 39.6963005065918,
      "learning_rate": 8.188573634433508e-06,
      "loss": 1.5486,
      "step": 12240
    },
    {
      "epoch": 2.3644084153638296,
      "grad_norm": 3.705770969390869,
      "learning_rate": 8.178922987840185e-06,
      "loss": 1.5523,
      "step": 12250
    },
    {
      "epoch": 2.3663385446824936,
      "grad_norm": 15.05490779876709,
      "learning_rate": 8.169272341246864e-06,
      "loss": 1.0992,
      "step": 12260
    },
    {
      "epoch": 2.368268674001158,
      "grad_norm": 105.40153503417969,
      "learning_rate": 8.159621694653543e-06,
      "loss": 1.4294,
      "step": 12270
    },
    {
      "epoch": 2.3701988033198225,
      "grad_norm": 1.158822774887085,
      "learning_rate": 8.149971048060221e-06,
      "loss": 0.9958,
      "step": 12280
    },
    {
      "epoch": 2.372128932638487,
      "grad_norm": 1.791251301765442,
      "learning_rate": 8.140320401466899e-06,
      "loss": 0.7038,
      "step": 12290
    },
    {
      "epoch": 2.3740590619571513,
      "grad_norm": 9.686400413513184,
      "learning_rate": 8.130669754873577e-06,
      "loss": 0.6542,
      "step": 12300
    },
    {
      "epoch": 2.3759891912758153,
      "grad_norm": 1.960798740386963,
      "learning_rate": 8.121019108280256e-06,
      "loss": 0.7572,
      "step": 12310
    },
    {
      "epoch": 2.3779193205944797,
      "grad_norm": 8.73493480682373,
      "learning_rate": 8.111368461686933e-06,
      "loss": 1.206,
      "step": 12320
    },
    {
      "epoch": 2.379849449913144,
      "grad_norm": 0.8683571815490723,
      "learning_rate": 8.101717815093612e-06,
      "loss": 1.3507,
      "step": 12330
    },
    {
      "epoch": 2.3817795792318086,
      "grad_norm": 0.959216296672821,
      "learning_rate": 8.092067168500291e-06,
      "loss": 0.9949,
      "step": 12340
    },
    {
      "epoch": 2.383709708550473,
      "grad_norm": 2.2261834144592285,
      "learning_rate": 8.082416521906968e-06,
      "loss": 1.7198,
      "step": 12350
    },
    {
      "epoch": 2.385639837869137,
      "grad_norm": 10.06154727935791,
      "learning_rate": 8.072765875313647e-06,
      "loss": 1.4277,
      "step": 12360
    },
    {
      "epoch": 2.3875699671878015,
      "grad_norm": 15.106761932373047,
      "learning_rate": 8.063115228720326e-06,
      "loss": 1.0657,
      "step": 12370
    },
    {
      "epoch": 2.389500096506466,
      "grad_norm": 3.4739110469818115,
      "learning_rate": 8.053464582127003e-06,
      "loss": 1.2583,
      "step": 12380
    },
    {
      "epoch": 2.3914302258251303,
      "grad_norm": 15.694387435913086,
      "learning_rate": 8.043813935533682e-06,
      "loss": 1.388,
      "step": 12390
    },
    {
      "epoch": 2.3933603551437947,
      "grad_norm": 8.96507740020752,
      "learning_rate": 8.03416328894036e-06,
      "loss": 0.9016,
      "step": 12400
    },
    {
      "epoch": 2.3952904844624587,
      "grad_norm": 18.177907943725586,
      "learning_rate": 8.024512642347038e-06,
      "loss": 0.8063,
      "step": 12410
    },
    {
      "epoch": 2.397220613781123,
      "grad_norm": 50.20621871948242,
      "learning_rate": 8.014861995753717e-06,
      "loss": 1.2721,
      "step": 12420
    },
    {
      "epoch": 2.3991507430997876,
      "grad_norm": 19.111351013183594,
      "learning_rate": 8.005211349160394e-06,
      "loss": 0.9947,
      "step": 12430
    },
    {
      "epoch": 2.401080872418452,
      "grad_norm": 3.6027543544769287,
      "learning_rate": 7.995560702567073e-06,
      "loss": 0.5745,
      "step": 12440
    },
    {
      "epoch": 2.4030110017371165,
      "grad_norm": 38.70094680786133,
      "learning_rate": 7.985910055973751e-06,
      "loss": 1.6015,
      "step": 12450
    },
    {
      "epoch": 2.404941131055781,
      "grad_norm": 2.3081729412078857,
      "learning_rate": 7.976259409380429e-06,
      "loss": 1.4985,
      "step": 12460
    },
    {
      "epoch": 2.4068712603744453,
      "grad_norm": 57.078453063964844,
      "learning_rate": 7.966608762787107e-06,
      "loss": 1.3648,
      "step": 12470
    },
    {
      "epoch": 2.4088013896931093,
      "grad_norm": 11.058767318725586,
      "learning_rate": 7.956958116193786e-06,
      "loss": 1.0241,
      "step": 12480
    },
    {
      "epoch": 2.4107315190117737,
      "grad_norm": 14.80713176727295,
      "learning_rate": 7.947307469600463e-06,
      "loss": 1.1028,
      "step": 12490
    },
    {
      "epoch": 2.412661648330438,
      "grad_norm": 2.6526379585266113,
      "learning_rate": 7.937656823007142e-06,
      "loss": 0.7798,
      "step": 12500
    },
    {
      "epoch": 2.4145917776491026,
      "grad_norm": 2.0857481956481934,
      "learning_rate": 7.928006176413821e-06,
      "loss": 0.9969,
      "step": 12510
    },
    {
      "epoch": 2.416521906967767,
      "grad_norm": 3.4048476219177246,
      "learning_rate": 7.918355529820498e-06,
      "loss": 0.9818,
      "step": 12520
    },
    {
      "epoch": 2.418452036286431,
      "grad_norm": 1.3548731803894043,
      "learning_rate": 7.908704883227177e-06,
      "loss": 0.6137,
      "step": 12530
    },
    {
      "epoch": 2.4203821656050954,
      "grad_norm": 1.0677480697631836,
      "learning_rate": 7.899054236633856e-06,
      "loss": 0.4086,
      "step": 12540
    },
    {
      "epoch": 2.42231229492376,
      "grad_norm": 0.3910003900527954,
      "learning_rate": 7.889403590040533e-06,
      "loss": 0.7405,
      "step": 12550
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 7.532329559326172,
      "learning_rate": 7.879752943447212e-06,
      "loss": 0.5571,
      "step": 12560
    },
    {
      "epoch": 2.4261725535610887,
      "grad_norm": 24.50370216369629,
      "learning_rate": 7.87010229685389e-06,
      "loss": 1.3895,
      "step": 12570
    },
    {
      "epoch": 2.4281026828797527,
      "grad_norm": 5.500336647033691,
      "learning_rate": 7.860451650260568e-06,
      "loss": 1.3401,
      "step": 12580
    },
    {
      "epoch": 2.430032812198417,
      "grad_norm": 8.27955436706543,
      "learning_rate": 7.850801003667247e-06,
      "loss": 1.3401,
      "step": 12590
    },
    {
      "epoch": 2.4319629415170816,
      "grad_norm": 9.50247573852539,
      "learning_rate": 7.841150357073924e-06,
      "loss": 1.1723,
      "step": 12600
    },
    {
      "epoch": 2.433893070835746,
      "grad_norm": 3.9191365242004395,
      "learning_rate": 7.831499710480603e-06,
      "loss": 1.3457,
      "step": 12610
    },
    {
      "epoch": 2.4358232001544105,
      "grad_norm": 0.6504154205322266,
      "learning_rate": 7.821849063887281e-06,
      "loss": 1.0401,
      "step": 12620
    },
    {
      "epoch": 2.437753329473075,
      "grad_norm": 0.19214992225170135,
      "learning_rate": 7.812198417293959e-06,
      "loss": 1.2084,
      "step": 12630
    },
    {
      "epoch": 2.439683458791739,
      "grad_norm": 21.0004825592041,
      "learning_rate": 7.802547770700637e-06,
      "loss": 1.1346,
      "step": 12640
    },
    {
      "epoch": 2.4416135881104033,
      "grad_norm": 0.9618868231773376,
      "learning_rate": 7.792897124107316e-06,
      "loss": 1.1732,
      "step": 12650
    },
    {
      "epoch": 2.4435437174290677,
      "grad_norm": 61.66630935668945,
      "learning_rate": 7.783246477513993e-06,
      "loss": 1.1441,
      "step": 12660
    },
    {
      "epoch": 2.445473846747732,
      "grad_norm": 6.192601203918457,
      "learning_rate": 7.773595830920672e-06,
      "loss": 1.1295,
      "step": 12670
    },
    {
      "epoch": 2.4474039760663966,
      "grad_norm": 126.16576385498047,
      "learning_rate": 7.763945184327351e-06,
      "loss": 1.3113,
      "step": 12680
    },
    {
      "epoch": 2.449334105385061,
      "grad_norm": 30.639619827270508,
      "learning_rate": 7.754294537734028e-06,
      "loss": 1.0173,
      "step": 12690
    },
    {
      "epoch": 2.451264234703725,
      "grad_norm": 78.15121459960938,
      "learning_rate": 7.744643891140707e-06,
      "loss": 2.3036,
      "step": 12700
    },
    {
      "epoch": 2.4531943640223894,
      "grad_norm": 6.0232720375061035,
      "learning_rate": 7.734993244547386e-06,
      "loss": 1.0831,
      "step": 12710
    },
    {
      "epoch": 2.455124493341054,
      "grad_norm": 7.630908012390137,
      "learning_rate": 7.725342597954063e-06,
      "loss": 0.9772,
      "step": 12720
    },
    {
      "epoch": 2.4570546226597183,
      "grad_norm": 146.2101287841797,
      "learning_rate": 7.715691951360742e-06,
      "loss": 1.1195,
      "step": 12730
    },
    {
      "epoch": 2.4589847519783827,
      "grad_norm": 3.784959077835083,
      "learning_rate": 7.70604130476742e-06,
      "loss": 0.7219,
      "step": 12740
    },
    {
      "epoch": 2.4609148812970467,
      "grad_norm": 8.91682243347168,
      "learning_rate": 7.6963906581741e-06,
      "loss": 0.9579,
      "step": 12750
    },
    {
      "epoch": 2.462845010615711,
      "grad_norm": 1.6089450120925903,
      "learning_rate": 7.686740011580777e-06,
      "loss": 0.8953,
      "step": 12760
    },
    {
      "epoch": 2.4647751399343756,
      "grad_norm": 15.8243989944458,
      "learning_rate": 7.677089364987454e-06,
      "loss": 0.8709,
      "step": 12770
    },
    {
      "epoch": 2.46670526925304,
      "grad_norm": 4.5509772300720215,
      "learning_rate": 7.667438718394134e-06,
      "loss": 0.3469,
      "step": 12780
    },
    {
      "epoch": 2.4686353985717044,
      "grad_norm": 63.16758728027344,
      "learning_rate": 7.657788071800811e-06,
      "loss": 0.9643,
      "step": 12790
    },
    {
      "epoch": 2.4705655278903684,
      "grad_norm": 2.313528299331665,
      "learning_rate": 7.648137425207489e-06,
      "loss": 1.0709,
      "step": 12800
    },
    {
      "epoch": 2.472495657209033,
      "grad_norm": 0.4269333779811859,
      "learning_rate": 7.638486778614167e-06,
      "loss": 0.8989,
      "step": 12810
    },
    {
      "epoch": 2.4744257865276973,
      "grad_norm": 16.27206802368164,
      "learning_rate": 7.628836132020846e-06,
      "loss": 1.8388,
      "step": 12820
    },
    {
      "epoch": 2.4763559158463617,
      "grad_norm": 2.7859153747558594,
      "learning_rate": 7.619185485427523e-06,
      "loss": 1.1094,
      "step": 12830
    },
    {
      "epoch": 2.478286045165026,
      "grad_norm": 8.368215560913086,
      "learning_rate": 7.609534838834203e-06,
      "loss": 0.7884,
      "step": 12840
    },
    {
      "epoch": 2.4802161744836906,
      "grad_norm": 9.142816543579102,
      "learning_rate": 7.59988419224088e-06,
      "loss": 1.2415,
      "step": 12850
    },
    {
      "epoch": 2.4821463038023546,
      "grad_norm": 20.586036682128906,
      "learning_rate": 7.59023354564756e-06,
      "loss": 0.8363,
      "step": 12860
    },
    {
      "epoch": 2.484076433121019,
      "grad_norm": 0.9126584529876709,
      "learning_rate": 7.580582899054237e-06,
      "loss": 0.8056,
      "step": 12870
    },
    {
      "epoch": 2.4860065624396834,
      "grad_norm": 7.450992107391357,
      "learning_rate": 7.570932252460915e-06,
      "loss": 0.6225,
      "step": 12880
    },
    {
      "epoch": 2.487936691758348,
      "grad_norm": 9.94731616973877,
      "learning_rate": 7.561281605867594e-06,
      "loss": 0.8909,
      "step": 12890
    },
    {
      "epoch": 2.4898668210770123,
      "grad_norm": 8.46530818939209,
      "learning_rate": 7.551630959274272e-06,
      "loss": 1.4375,
      "step": 12900
    },
    {
      "epoch": 2.4917969503956767,
      "grad_norm": 6.481171607971191,
      "learning_rate": 7.54198031268095e-06,
      "loss": 0.9749,
      "step": 12910
    },
    {
      "epoch": 2.4937270797143407,
      "grad_norm": 10.695090293884277,
      "learning_rate": 7.532329666087629e-06,
      "loss": 0.7598,
      "step": 12920
    },
    {
      "epoch": 2.495657209033005,
      "grad_norm": 18.708711624145508,
      "learning_rate": 7.522679019494307e-06,
      "loss": 0.5793,
      "step": 12930
    },
    {
      "epoch": 2.4975873383516696,
      "grad_norm": 13.136709213256836,
      "learning_rate": 7.513028372900985e-06,
      "loss": 1.4957,
      "step": 12940
    },
    {
      "epoch": 2.499517467670334,
      "grad_norm": 28.007247924804688,
      "learning_rate": 7.5033777263076634e-06,
      "loss": 0.6671,
      "step": 12950
    },
    {
      "epoch": 2.5014475969889984,
      "grad_norm": 8.858991622924805,
      "learning_rate": 7.4937270797143414e-06,
      "loss": 1.2746,
      "step": 12960
    },
    {
      "epoch": 2.5033777263076624,
      "grad_norm": 21.547611236572266,
      "learning_rate": 7.484076433121019e-06,
      "loss": 0.6891,
      "step": 12970
    },
    {
      "epoch": 2.505307855626327,
      "grad_norm": 36.409324645996094,
      "learning_rate": 7.474425786527698e-06,
      "loss": 0.9225,
      "step": 12980
    },
    {
      "epoch": 2.5072379849449913,
      "grad_norm": 0.7382892966270447,
      "learning_rate": 7.464775139934376e-06,
      "loss": 1.016,
      "step": 12990
    },
    {
      "epoch": 2.5091681142636557,
      "grad_norm": 12.067386627197266,
      "learning_rate": 7.455124493341055e-06,
      "loss": 1.0328,
      "step": 13000
    },
    {
      "epoch": 2.51109824358232,
      "grad_norm": 7.0455403327941895,
      "learning_rate": 7.445473846747733e-06,
      "loss": 1.3632,
      "step": 13010
    },
    {
      "epoch": 2.513028372900984,
      "grad_norm": 1.6342524290084839,
      "learning_rate": 7.43582320015441e-06,
      "loss": 1.376,
      "step": 13020
    },
    {
      "epoch": 2.5149585022196486,
      "grad_norm": 38.014427185058594,
      "learning_rate": 7.42617255356109e-06,
      "loss": 0.8524,
      "step": 13030
    },
    {
      "epoch": 2.516888631538313,
      "grad_norm": 9.765813827514648,
      "learning_rate": 7.416521906967767e-06,
      "loss": 0.7838,
      "step": 13040
    },
    {
      "epoch": 2.5188187608569774,
      "grad_norm": 14.681615829467773,
      "learning_rate": 7.406871260374445e-06,
      "loss": 0.94,
      "step": 13050
    },
    {
      "epoch": 2.520748890175642,
      "grad_norm": 1.567814826965332,
      "learning_rate": 7.397220613781124e-06,
      "loss": 0.8465,
      "step": 13060
    },
    {
      "epoch": 2.522679019494306,
      "grad_norm": 0.5401426553726196,
      "learning_rate": 7.387569967187802e-06,
      "loss": 1.1042,
      "step": 13070
    },
    {
      "epoch": 2.5246091488129707,
      "grad_norm": 6.2050700187683105,
      "learning_rate": 7.37791932059448e-06,
      "loss": 0.7162,
      "step": 13080
    },
    {
      "epoch": 2.5265392781316347,
      "grad_norm": 17.401018142700195,
      "learning_rate": 7.368268674001159e-06,
      "loss": 1.0816,
      "step": 13090
    },
    {
      "epoch": 2.528469407450299,
      "grad_norm": 2.0195183753967285,
      "learning_rate": 7.358618027407837e-06,
      "loss": 1.0864,
      "step": 13100
    },
    {
      "epoch": 2.5303995367689636,
      "grad_norm": 11.30167293548584,
      "learning_rate": 7.3489673808145155e-06,
      "loss": 1.3141,
      "step": 13110
    },
    {
      "epoch": 2.532329666087628,
      "grad_norm": 12.892783164978027,
      "learning_rate": 7.3393167342211935e-06,
      "loss": 1.0152,
      "step": 13120
    },
    {
      "epoch": 2.5342597954062924,
      "grad_norm": 9.847455978393555,
      "learning_rate": 7.3296660876278714e-06,
      "loss": 0.4492,
      "step": 13130
    },
    {
      "epoch": 2.5361899247249564,
      "grad_norm": 15.930928230285645,
      "learning_rate": 7.32001544103455e-06,
      "loss": 0.6155,
      "step": 13140
    },
    {
      "epoch": 2.538120054043621,
      "grad_norm": 7.39434814453125,
      "learning_rate": 7.310364794441228e-06,
      "loss": 0.9261,
      "step": 13150
    },
    {
      "epoch": 2.5400501833622853,
      "grad_norm": 5.8842620849609375,
      "learning_rate": 7.300714147847906e-06,
      "loss": 1.0729,
      "step": 13160
    },
    {
      "epoch": 2.5419803126809497,
      "grad_norm": 0.39544054865837097,
      "learning_rate": 7.291063501254585e-06,
      "loss": 0.5139,
      "step": 13170
    },
    {
      "epoch": 2.543910441999614,
      "grad_norm": 0.2462531477212906,
      "learning_rate": 7.281412854661263e-06,
      "loss": 1.2668,
      "step": 13180
    },
    {
      "epoch": 2.545840571318278,
      "grad_norm": 0.6063128113746643,
      "learning_rate": 7.271762208067941e-06,
      "loss": 1.1944,
      "step": 13190
    },
    {
      "epoch": 2.5477707006369426,
      "grad_norm": 30.2177791595459,
      "learning_rate": 7.26211156147462e-06,
      "loss": 1.403,
      "step": 13200
    },
    {
      "epoch": 2.549700829955607,
      "grad_norm": 1.8725236654281616,
      "learning_rate": 7.252460914881297e-06,
      "loss": 1.2221,
      "step": 13210
    },
    {
      "epoch": 2.5516309592742714,
      "grad_norm": 46.50960159301758,
      "learning_rate": 7.242810268287977e-06,
      "loss": 1.0025,
      "step": 13220
    },
    {
      "epoch": 2.553561088592936,
      "grad_norm": 1.1395964622497559,
      "learning_rate": 7.233159621694654e-06,
      "loss": 0.492,
      "step": 13230
    },
    {
      "epoch": 2.5554912179116,
      "grad_norm": 2.3761754035949707,
      "learning_rate": 7.223508975101332e-06,
      "loss": 1.0976,
      "step": 13240
    },
    {
      "epoch": 2.5574213472302643,
      "grad_norm": 4.236466407775879,
      "learning_rate": 7.213858328508011e-06,
      "loss": 0.7766,
      "step": 13250
    },
    {
      "epoch": 2.5593514765489287,
      "grad_norm": 1.473411202430725,
      "learning_rate": 7.204207681914689e-06,
      "loss": 0.6742,
      "step": 13260
    },
    {
      "epoch": 2.561281605867593,
      "grad_norm": 8.53915023803711,
      "learning_rate": 7.194557035321367e-06,
      "loss": 0.512,
      "step": 13270
    },
    {
      "epoch": 2.5632117351862576,
      "grad_norm": 2.089212656021118,
      "learning_rate": 7.1849063887280455e-06,
      "loss": 0.677,
      "step": 13280
    },
    {
      "epoch": 2.565141864504922,
      "grad_norm": 22.59674835205078,
      "learning_rate": 7.1752557421347235e-06,
      "loss": 1.0216,
      "step": 13290
    },
    {
      "epoch": 2.5670719938235864,
      "grad_norm": 1.3298228979110718,
      "learning_rate": 7.1656050955414014e-06,
      "loss": 0.8275,
      "step": 13300
    },
    {
      "epoch": 2.5690021231422504,
      "grad_norm": 0.3795088529586792,
      "learning_rate": 7.15595444894808e-06,
      "loss": 1.6763,
      "step": 13310
    },
    {
      "epoch": 2.570932252460915,
      "grad_norm": 5.705696105957031,
      "learning_rate": 7.146303802354758e-06,
      "loss": 1.0623,
      "step": 13320
    },
    {
      "epoch": 2.5728623817795793,
      "grad_norm": 65.55107116699219,
      "learning_rate": 7.136653155761437e-06,
      "loss": 0.7217,
      "step": 13330
    },
    {
      "epoch": 2.5747925110982437,
      "grad_norm": 314.4554748535156,
      "learning_rate": 7.127002509168115e-06,
      "loss": 0.9986,
      "step": 13340
    },
    {
      "epoch": 2.576722640416908,
      "grad_norm": 2.729384660720825,
      "learning_rate": 7.117351862574793e-06,
      "loss": 1.2567,
      "step": 13350
    },
    {
      "epoch": 2.578652769735572,
      "grad_norm": 59.061729431152344,
      "learning_rate": 7.107701215981472e-06,
      "loss": 0.6234,
      "step": 13360
    },
    {
      "epoch": 2.5805828990542365,
      "grad_norm": 28.505849838256836,
      "learning_rate": 7.09805056938815e-06,
      "loss": 0.905,
      "step": 13370
    },
    {
      "epoch": 2.582513028372901,
      "grad_norm": 9.449257850646973,
      "learning_rate": 7.088399922794828e-06,
      "loss": 1.0049,
      "step": 13380
    },
    {
      "epoch": 2.5844431576915654,
      "grad_norm": 85.7002944946289,
      "learning_rate": 7.078749276201507e-06,
      "loss": 1.116,
      "step": 13390
    },
    {
      "epoch": 2.58637328701023,
      "grad_norm": 7.320760250091553,
      "learning_rate": 7.069098629608184e-06,
      "loss": 1.3962,
      "step": 13400
    },
    {
      "epoch": 2.588303416328894,
      "grad_norm": 12.198376655578613,
      "learning_rate": 7.059447983014862e-06,
      "loss": 0.6287,
      "step": 13410
    },
    {
      "epoch": 2.5902335456475583,
      "grad_norm": 0.30273446440696716,
      "learning_rate": 7.049797336421541e-06,
      "loss": 0.8319,
      "step": 13420
    },
    {
      "epoch": 2.5921636749662227,
      "grad_norm": 4.013638019561768,
      "learning_rate": 7.040146689828219e-06,
      "loss": 0.439,
      "step": 13430
    },
    {
      "epoch": 2.594093804284887,
      "grad_norm": 121.49607849121094,
      "learning_rate": 7.030496043234897e-06,
      "loss": 0.9945,
      "step": 13440
    },
    {
      "epoch": 2.5960239336035515,
      "grad_norm": 32.151676177978516,
      "learning_rate": 7.0208453966415755e-06,
      "loss": 1.1467,
      "step": 13450
    },
    {
      "epoch": 2.5979540629222155,
      "grad_norm": 37.318904876708984,
      "learning_rate": 7.0111947500482535e-06,
      "loss": 0.5243,
      "step": 13460
    },
    {
      "epoch": 2.5998841922408804,
      "grad_norm": 4.824168682098389,
      "learning_rate": 7.001544103454932e-06,
      "loss": 1.1151,
      "step": 13470
    },
    {
      "epoch": 2.6018143215595444,
      "grad_norm": 88.2439956665039,
      "learning_rate": 6.99189345686161e-06,
      "loss": 1.5416,
      "step": 13480
    },
    {
      "epoch": 2.603744450878209,
      "grad_norm": 11.282652854919434,
      "learning_rate": 6.982242810268288e-06,
      "loss": 1.67,
      "step": 13490
    },
    {
      "epoch": 2.6056745801968733,
      "grad_norm": 8.516386985778809,
      "learning_rate": 6.972592163674967e-06,
      "loss": 0.6023,
      "step": 13500
    },
    {
      "epoch": 2.6076047095155377,
      "grad_norm": 4.72945499420166,
      "learning_rate": 6.962941517081645e-06,
      "loss": 1.0032,
      "step": 13510
    },
    {
      "epoch": 2.609534838834202,
      "grad_norm": 50.53861999511719,
      "learning_rate": 6.953290870488323e-06,
      "loss": 0.4865,
      "step": 13520
    },
    {
      "epoch": 2.611464968152866,
      "grad_norm": 96.42640686035156,
      "learning_rate": 6.943640223895002e-06,
      "loss": 0.7145,
      "step": 13530
    },
    {
      "epoch": 2.6133950974715305,
      "grad_norm": 0.6667097806930542,
      "learning_rate": 6.93398957730168e-06,
      "loss": 0.7761,
      "step": 13540
    },
    {
      "epoch": 2.615325226790195,
      "grad_norm": 99.60198974609375,
      "learning_rate": 6.924338930708358e-06,
      "loss": 1.4738,
      "step": 13550
    },
    {
      "epoch": 2.6172553561088594,
      "grad_norm": 12.315961837768555,
      "learning_rate": 6.914688284115037e-06,
      "loss": 0.596,
      "step": 13560
    },
    {
      "epoch": 2.619185485427524,
      "grad_norm": 4.4911627769470215,
      "learning_rate": 6.905037637521715e-06,
      "loss": 0.5795,
      "step": 13570
    },
    {
      "epoch": 2.621115614746188,
      "grad_norm": 0.8165052533149719,
      "learning_rate": 6.8953869909283935e-06,
      "loss": 1.0819,
      "step": 13580
    },
    {
      "epoch": 2.6230457440648522,
      "grad_norm": 0.772085964679718,
      "learning_rate": 6.885736344335071e-06,
      "loss": 1.4768,
      "step": 13590
    },
    {
      "epoch": 2.6249758733835167,
      "grad_norm": 0.6475985050201416,
      "learning_rate": 6.876085697741749e-06,
      "loss": 1.7515,
      "step": 13600
    },
    {
      "epoch": 2.626906002702181,
      "grad_norm": 119.99900817871094,
      "learning_rate": 6.8664350511484275e-06,
      "loss": 1.3065,
      "step": 13610
    },
    {
      "epoch": 2.6288361320208455,
      "grad_norm": 13.012531280517578,
      "learning_rate": 6.8567844045551055e-06,
      "loss": 0.8696,
      "step": 13620
    },
    {
      "epoch": 2.6307662613395095,
      "grad_norm": 0.7478753328323364,
      "learning_rate": 6.8471337579617835e-06,
      "loss": 1.0209,
      "step": 13630
    },
    {
      "epoch": 2.632696390658174,
      "grad_norm": 7.232962131500244,
      "learning_rate": 6.837483111368462e-06,
      "loss": 0.7599,
      "step": 13640
    },
    {
      "epoch": 2.6346265199768384,
      "grad_norm": 29.052915573120117,
      "learning_rate": 6.82783246477514e-06,
      "loss": 0.666,
      "step": 13650
    },
    {
      "epoch": 2.636556649295503,
      "grad_norm": 11.815967559814453,
      "learning_rate": 6.818181818181818e-06,
      "loss": 0.8024,
      "step": 13660
    },
    {
      "epoch": 2.6384867786141672,
      "grad_norm": 8.960729598999023,
      "learning_rate": 6.808531171588497e-06,
      "loss": 0.803,
      "step": 13670
    },
    {
      "epoch": 2.6404169079328312,
      "grad_norm": 9.19530200958252,
      "learning_rate": 6.798880524995175e-06,
      "loss": 0.5902,
      "step": 13680
    },
    {
      "epoch": 2.642347037251496,
      "grad_norm": 0.5116904377937317,
      "learning_rate": 6.789229878401854e-06,
      "loss": 0.5696,
      "step": 13690
    },
    {
      "epoch": 2.64427716657016,
      "grad_norm": 86.35163116455078,
      "learning_rate": 6.779579231808532e-06,
      "loss": 1.0201,
      "step": 13700
    },
    {
      "epoch": 2.6462072958888245,
      "grad_norm": 50.59503936767578,
      "learning_rate": 6.76992858521521e-06,
      "loss": 1.0854,
      "step": 13710
    },
    {
      "epoch": 2.648137425207489,
      "grad_norm": 20.520112991333008,
      "learning_rate": 6.760277938621889e-06,
      "loss": 0.7268,
      "step": 13720
    },
    {
      "epoch": 2.6500675545261534,
      "grad_norm": 14.548959732055664,
      "learning_rate": 6.750627292028567e-06,
      "loss": 1.0603,
      "step": 13730
    },
    {
      "epoch": 2.651997683844818,
      "grad_norm": 9.889354705810547,
      "learning_rate": 6.740976645435245e-06,
      "loss": 1.1478,
      "step": 13740
    },
    {
      "epoch": 2.653927813163482,
      "grad_norm": 7.2136454582214355,
      "learning_rate": 6.7313259988419235e-06,
      "loss": 0.8225,
      "step": 13750
    },
    {
      "epoch": 2.6558579424821462,
      "grad_norm": 4.002645969390869,
      "learning_rate": 6.7216753522486015e-06,
      "loss": 0.5155,
      "step": 13760
    },
    {
      "epoch": 2.6577880718008107,
      "grad_norm": 8.142547607421875,
      "learning_rate": 6.712024705655279e-06,
      "loss": 0.5383,
      "step": 13770
    },
    {
      "epoch": 2.659718201119475,
      "grad_norm": 16.605968475341797,
      "learning_rate": 6.7023740590619575e-06,
      "loss": 0.8949,
      "step": 13780
    },
    {
      "epoch": 2.6616483304381395,
      "grad_norm": 3.5221235752105713,
      "learning_rate": 6.6927234124686355e-06,
      "loss": 0.9657,
      "step": 13790
    },
    {
      "epoch": 2.6635784597568035,
      "grad_norm": 0.7888225317001343,
      "learning_rate": 6.683072765875314e-06,
      "loss": 0.3164,
      "step": 13800
    },
    {
      "epoch": 2.665508589075468,
      "grad_norm": 8.458369255065918,
      "learning_rate": 6.673422119281992e-06,
      "loss": 0.4771,
      "step": 13810
    },
    {
      "epoch": 2.6674387183941324,
      "grad_norm": 58.43794631958008,
      "learning_rate": 6.66377147268867e-06,
      "loss": 1.7052,
      "step": 13820
    },
    {
      "epoch": 2.669368847712797,
      "grad_norm": 0.6914430856704712,
      "learning_rate": 6.654120826095349e-06,
      "loss": 1.1657,
      "step": 13830
    },
    {
      "epoch": 2.6712989770314612,
      "grad_norm": 109.15274047851562,
      "learning_rate": 6.644470179502027e-06,
      "loss": 0.6798,
      "step": 13840
    },
    {
      "epoch": 2.6732291063501252,
      "grad_norm": 133.40235900878906,
      "learning_rate": 6.634819532908705e-06,
      "loss": 0.8857,
      "step": 13850
    },
    {
      "epoch": 2.6751592356687897,
      "grad_norm": 3.7955870628356934,
      "learning_rate": 6.625168886315384e-06,
      "loss": 0.9823,
      "step": 13860
    },
    {
      "epoch": 2.677089364987454,
      "grad_norm": 1.14378023147583,
      "learning_rate": 6.615518239722062e-06,
      "loss": 0.9677,
      "step": 13870
    },
    {
      "epoch": 2.6790194943061185,
      "grad_norm": 0.5389754176139832,
      "learning_rate": 6.60586759312874e-06,
      "loss": 0.7518,
      "step": 13880
    },
    {
      "epoch": 2.680949623624783,
      "grad_norm": 33.603939056396484,
      "learning_rate": 6.596216946535419e-06,
      "loss": 1.1128,
      "step": 13890
    },
    {
      "epoch": 2.682879752943447,
      "grad_norm": 9.548067092895508,
      "learning_rate": 6.586566299942097e-06,
      "loss": 0.6729,
      "step": 13900
    },
    {
      "epoch": 2.684809882262112,
      "grad_norm": 3.6901865005493164,
      "learning_rate": 6.576915653348775e-06,
      "loss": 0.9326,
      "step": 13910
    },
    {
      "epoch": 2.686740011580776,
      "grad_norm": 0.4775453209877014,
      "learning_rate": 6.5672650067554536e-06,
      "loss": 0.4783,
      "step": 13920
    },
    {
      "epoch": 2.6886701408994402,
      "grad_norm": 6.280099868774414,
      "learning_rate": 6.5576143601621315e-06,
      "loss": 0.9113,
      "step": 13930
    },
    {
      "epoch": 2.6906002702181047,
      "grad_norm": 88.25436401367188,
      "learning_rate": 6.54796371356881e-06,
      "loss": 0.5425,
      "step": 13940
    },
    {
      "epoch": 2.692530399536769,
      "grad_norm": 0.6902898550033569,
      "learning_rate": 6.538313066975488e-06,
      "loss": 0.5055,
      "step": 13950
    },
    {
      "epoch": 2.6944605288554335,
      "grad_norm": 0.4674272835254669,
      "learning_rate": 6.5286624203821655e-06,
      "loss": 1.3178,
      "step": 13960
    },
    {
      "epoch": 2.6963906581740975,
      "grad_norm": 1.708443284034729,
      "learning_rate": 6.519011773788845e-06,
      "loss": 1.3242,
      "step": 13970
    },
    {
      "epoch": 2.698320787492762,
      "grad_norm": 4.932424068450928,
      "learning_rate": 6.509361127195522e-06,
      "loss": 0.9558,
      "step": 13980
    },
    {
      "epoch": 2.7002509168114264,
      "grad_norm": 15.954225540161133,
      "learning_rate": 6.4997104806022e-06,
      "loss": 1.2752,
      "step": 13990
    },
    {
      "epoch": 2.702181046130091,
      "grad_norm": 8.213580131530762,
      "learning_rate": 6.490059834008879e-06,
      "loss": 0.5708,
      "step": 14000
    },
    {
      "epoch": 2.7041111754487552,
      "grad_norm": 16.970884323120117,
      "learning_rate": 6.480409187415557e-06,
      "loss": 1.2024,
      "step": 14010
    },
    {
      "epoch": 2.706041304767419,
      "grad_norm": 6.291445255279541,
      "learning_rate": 6.470758540822235e-06,
      "loss": 1.7459,
      "step": 14020
    },
    {
      "epoch": 2.7079714340860837,
      "grad_norm": 43.101234436035156,
      "learning_rate": 6.461107894228914e-06,
      "loss": 1.0297,
      "step": 14030
    },
    {
      "epoch": 2.709901563404748,
      "grad_norm": 3.79811429977417,
      "learning_rate": 6.451457247635592e-06,
      "loss": 1.3391,
      "step": 14040
    },
    {
      "epoch": 2.7118316927234125,
      "grad_norm": 1.658678412437439,
      "learning_rate": 6.441806601042271e-06,
      "loss": 0.8837,
      "step": 14050
    },
    {
      "epoch": 2.713761822042077,
      "grad_norm": 5.351208686828613,
      "learning_rate": 6.432155954448949e-06,
      "loss": 0.4528,
      "step": 14060
    },
    {
      "epoch": 2.715691951360741,
      "grad_norm": 0.6197509169578552,
      "learning_rate": 6.422505307855627e-06,
      "loss": 1.0185,
      "step": 14070
    },
    {
      "epoch": 2.7176220806794054,
      "grad_norm": 7.301795482635498,
      "learning_rate": 6.4128546612623056e-06,
      "loss": 0.503,
      "step": 14080
    },
    {
      "epoch": 2.71955220999807,
      "grad_norm": 14.905562400817871,
      "learning_rate": 6.4032040146689836e-06,
      "loss": 0.9111,
      "step": 14090
    },
    {
      "epoch": 2.7214823393167342,
      "grad_norm": 37.36772155761719,
      "learning_rate": 6.3935533680756615e-06,
      "loss": 0.7376,
      "step": 14100
    },
    {
      "epoch": 2.7234124686353987,
      "grad_norm": 0.22101996839046478,
      "learning_rate": 6.38390272148234e-06,
      "loss": 0.6035,
      "step": 14110
    },
    {
      "epoch": 2.725342597954063,
      "grad_norm": 1.4106491804122925,
      "learning_rate": 6.374252074889018e-06,
      "loss": 0.6093,
      "step": 14120
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 0.20738667249679565,
      "learning_rate": 6.3646014282956955e-06,
      "loss": 0.3456,
      "step": 14130
    },
    {
      "epoch": 2.7292028565913915,
      "grad_norm": 9.726449012756348,
      "learning_rate": 6.354950781702375e-06,
      "loss": 1.2827,
      "step": 14140
    },
    {
      "epoch": 2.731132985910056,
      "grad_norm": 1.1892226934432983,
      "learning_rate": 6.345300135109052e-06,
      "loss": 0.9563,
      "step": 14150
    },
    {
      "epoch": 2.7330631152287204,
      "grad_norm": 2.5494797229766846,
      "learning_rate": 6.335649488515732e-06,
      "loss": 0.6849,
      "step": 14160
    },
    {
      "epoch": 2.734993244547385,
      "grad_norm": 9.927159309387207,
      "learning_rate": 6.325998841922409e-06,
      "loss": 0.5201,
      "step": 14170
    },
    {
      "epoch": 2.7369233738660492,
      "grad_norm": 0.617080569267273,
      "learning_rate": 6.316348195329087e-06,
      "loss": 1.2885,
      "step": 14180
    },
    {
      "epoch": 2.738853503184713,
      "grad_norm": 6.661830425262451,
      "learning_rate": 6.306697548735766e-06,
      "loss": 1.4664,
      "step": 14190
    },
    {
      "epoch": 2.7407836325033776,
      "grad_norm": 0.4252451956272125,
      "learning_rate": 6.297046902142444e-06,
      "loss": 1.9319,
      "step": 14200
    },
    {
      "epoch": 2.742713761822042,
      "grad_norm": 7.579573631286621,
      "learning_rate": 6.287396255549122e-06,
      "loss": 1.2627,
      "step": 14210
    },
    {
      "epoch": 2.7446438911407065,
      "grad_norm": 16.810287475585938,
      "learning_rate": 6.277745608955801e-06,
      "loss": 1.1575,
      "step": 14220
    },
    {
      "epoch": 2.746574020459371,
      "grad_norm": 15.742704391479492,
      "learning_rate": 6.268094962362479e-06,
      "loss": 1.6358,
      "step": 14230
    },
    {
      "epoch": 2.748504149778035,
      "grad_norm": 2.7983815670013428,
      "learning_rate": 6.258444315769157e-06,
      "loss": 0.7916,
      "step": 14240
    },
    {
      "epoch": 2.7504342790966994,
      "grad_norm": 36.08818817138672,
      "learning_rate": 6.248793669175836e-06,
      "loss": 0.4386,
      "step": 14250
    },
    {
      "epoch": 2.752364408415364,
      "grad_norm": 22.83018684387207,
      "learning_rate": 6.2391430225825136e-06,
      "loss": 1.1099,
      "step": 14260
    },
    {
      "epoch": 2.754294537734028,
      "grad_norm": 2.162731170654297,
      "learning_rate": 6.229492375989192e-06,
      "loss": 1.4065,
      "step": 14270
    },
    {
      "epoch": 2.7562246670526926,
      "grad_norm": 74.47081756591797,
      "learning_rate": 6.21984172939587e-06,
      "loss": 1.2282,
      "step": 14280
    },
    {
      "epoch": 2.7581547963713566,
      "grad_norm": 116.76090240478516,
      "learning_rate": 6.210191082802548e-06,
      "loss": 1.5996,
      "step": 14290
    },
    {
      "epoch": 2.7600849256900215,
      "grad_norm": 6.622755527496338,
      "learning_rate": 6.200540436209227e-06,
      "loss": 0.6753,
      "step": 14300
    },
    {
      "epoch": 2.7620150550086855,
      "grad_norm": 8.076554298400879,
      "learning_rate": 6.190889789615905e-06,
      "loss": 1.731,
      "step": 14310
    },
    {
      "epoch": 2.76394518432735,
      "grad_norm": 11.20789909362793,
      "learning_rate": 6.181239143022582e-06,
      "loss": 0.5653,
      "step": 14320
    },
    {
      "epoch": 2.7658753136460144,
      "grad_norm": 5.348842620849609,
      "learning_rate": 6.171588496429262e-06,
      "loss": 0.7229,
      "step": 14330
    },
    {
      "epoch": 2.767805442964679,
      "grad_norm": 157.64439392089844,
      "learning_rate": 6.161937849835939e-06,
      "loss": 0.8889,
      "step": 14340
    },
    {
      "epoch": 2.769735572283343,
      "grad_norm": 35.62127685546875,
      "learning_rate": 6.152287203242617e-06,
      "loss": 0.8298,
      "step": 14350
    },
    {
      "epoch": 2.771665701602007,
      "grad_norm": 6.479985237121582,
      "learning_rate": 6.142636556649296e-06,
      "loss": 0.7819,
      "step": 14360
    },
    {
      "epoch": 2.7735958309206716,
      "grad_norm": 0.46451297402381897,
      "learning_rate": 6.132985910055974e-06,
      "loss": 0.7744,
      "step": 14370
    },
    {
      "epoch": 2.775525960239336,
      "grad_norm": 5.106220245361328,
      "learning_rate": 6.123335263462652e-06,
      "loss": 0.8077,
      "step": 14380
    },
    {
      "epoch": 2.7774560895580005,
      "grad_norm": 3.381115674972534,
      "learning_rate": 6.113684616869331e-06,
      "loss": 0.7048,
      "step": 14390
    },
    {
      "epoch": 2.779386218876665,
      "grad_norm": 6.592851638793945,
      "learning_rate": 6.104033970276009e-06,
      "loss": 1.1103,
      "step": 14400
    },
    {
      "epoch": 2.781316348195329,
      "grad_norm": 28.63041877746582,
      "learning_rate": 6.094383323682688e-06,
      "loss": 0.7883,
      "step": 14410
    },
    {
      "epoch": 2.7832464775139933,
      "grad_norm": 1.3037587404251099,
      "learning_rate": 6.084732677089366e-06,
      "loss": 0.9176,
      "step": 14420
    },
    {
      "epoch": 2.7851766068326578,
      "grad_norm": 10.848785400390625,
      "learning_rate": 6.0750820304960436e-06,
      "loss": 1.0147,
      "step": 14430
    },
    {
      "epoch": 2.787106736151322,
      "grad_norm": 3.230794906616211,
      "learning_rate": 6.065431383902722e-06,
      "loss": 1.2718,
      "step": 14440
    },
    {
      "epoch": 2.7890368654699866,
      "grad_norm": 0.9966545701026917,
      "learning_rate": 6.0557807373094e-06,
      "loss": 1.2332,
      "step": 14450
    },
    {
      "epoch": 2.7909669947886506,
      "grad_norm": 1.0588008165359497,
      "learning_rate": 6.046130090716078e-06,
      "loss": 0.7273,
      "step": 14460
    },
    {
      "epoch": 2.792897124107315,
      "grad_norm": 2.2596232891082764,
      "learning_rate": 6.036479444122757e-06,
      "loss": 0.9815,
      "step": 14470
    },
    {
      "epoch": 2.7948272534259795,
      "grad_norm": 48.26445388793945,
      "learning_rate": 6.026828797529435e-06,
      "loss": 0.9269,
      "step": 14480
    },
    {
      "epoch": 2.796757382744644,
      "grad_norm": 1.2573158740997314,
      "learning_rate": 6.017178150936112e-06,
      "loss": 1.2133,
      "step": 14490
    },
    {
      "epoch": 2.7986875120633083,
      "grad_norm": 7.942381381988525,
      "learning_rate": 6.007527504342792e-06,
      "loss": 0.6448,
      "step": 14500
    },
    {
      "epoch": 2.8006176413819723,
      "grad_norm": 4.059282302856445,
      "learning_rate": 5.997876857749469e-06,
      "loss": 1.2369,
      "step": 14510
    },
    {
      "epoch": 2.802547770700637,
      "grad_norm": 32.1055793762207,
      "learning_rate": 5.988226211156149e-06,
      "loss": 0.5627,
      "step": 14520
    },
    {
      "epoch": 2.804477900019301,
      "grad_norm": 12.2713623046875,
      "learning_rate": 5.978575564562826e-06,
      "loss": 1.029,
      "step": 14530
    },
    {
      "epoch": 2.8064080293379656,
      "grad_norm": 8.38637638092041,
      "learning_rate": 5.968924917969504e-06,
      "loss": 1.4221,
      "step": 14540
    },
    {
      "epoch": 2.80833815865663,
      "grad_norm": 44.82981872558594,
      "learning_rate": 5.959274271376183e-06,
      "loss": 1.1904,
      "step": 14550
    },
    {
      "epoch": 2.8102682879752945,
      "grad_norm": 0.8476757407188416,
      "learning_rate": 5.949623624782861e-06,
      "loss": 0.5882,
      "step": 14560
    },
    {
      "epoch": 2.812198417293959,
      "grad_norm": 4.206515789031982,
      "learning_rate": 5.939972978189539e-06,
      "loss": 1.3948,
      "step": 14570
    },
    {
      "epoch": 2.814128546612623,
      "grad_norm": 113.77513885498047,
      "learning_rate": 5.930322331596218e-06,
      "loss": 0.8985,
      "step": 14580
    },
    {
      "epoch": 2.8160586759312873,
      "grad_norm": 4.000624656677246,
      "learning_rate": 5.920671685002896e-06,
      "loss": 0.6474,
      "step": 14590
    },
    {
      "epoch": 2.8179888052499518,
      "grad_norm": 1.089165210723877,
      "learning_rate": 5.911021038409574e-06,
      "loss": 0.9293,
      "step": 14600
    },
    {
      "epoch": 2.819918934568616,
      "grad_norm": 0.31590309739112854,
      "learning_rate": 5.901370391816252e-06,
      "loss": 1.1147,
      "step": 14610
    },
    {
      "epoch": 2.8218490638872806,
      "grad_norm": 46.79145812988281,
      "learning_rate": 5.89171974522293e-06,
      "loss": 0.3912,
      "step": 14620
    },
    {
      "epoch": 2.8237791932059446,
      "grad_norm": 8.49980354309082,
      "learning_rate": 5.882069098629609e-06,
      "loss": 0.8535,
      "step": 14630
    },
    {
      "epoch": 2.825709322524609,
      "grad_norm": 24.860515594482422,
      "learning_rate": 5.872418452036287e-06,
      "loss": 1.4653,
      "step": 14640
    },
    {
      "epoch": 2.8276394518432735,
      "grad_norm": 1.0480903387069702,
      "learning_rate": 5.862767805442965e-06,
      "loss": 0.3375,
      "step": 14650
    },
    {
      "epoch": 2.829569581161938,
      "grad_norm": 21.039520263671875,
      "learning_rate": 5.853117158849644e-06,
      "loss": 1.133,
      "step": 14660
    },
    {
      "epoch": 2.8314997104806023,
      "grad_norm": 4.185657978057861,
      "learning_rate": 5.843466512256322e-06,
      "loss": 1.8472,
      "step": 14670
    },
    {
      "epoch": 2.8334298397992663,
      "grad_norm": 82.51986694335938,
      "learning_rate": 5.833815865662999e-06,
      "loss": 0.7387,
      "step": 14680
    },
    {
      "epoch": 2.8353599691179308,
      "grad_norm": 1.3715367317199707,
      "learning_rate": 5.824165219069679e-06,
      "loss": 1.2402,
      "step": 14690
    },
    {
      "epoch": 2.837290098436595,
      "grad_norm": 2.446007251739502,
      "learning_rate": 5.814514572476356e-06,
      "loss": 1.0825,
      "step": 14700
    },
    {
      "epoch": 2.8392202277552596,
      "grad_norm": 17.26995277404785,
      "learning_rate": 5.804863925883034e-06,
      "loss": 0.9283,
      "step": 14710
    },
    {
      "epoch": 2.841150357073924,
      "grad_norm": 0.21662378311157227,
      "learning_rate": 5.795213279289713e-06,
      "loss": 0.4642,
      "step": 14720
    },
    {
      "epoch": 2.843080486392588,
      "grad_norm": 12.167155265808105,
      "learning_rate": 5.785562632696391e-06,
      "loss": 1.2832,
      "step": 14730
    },
    {
      "epoch": 2.845010615711253,
      "grad_norm": 1.0757205486297607,
      "learning_rate": 5.77591198610307e-06,
      "loss": 1.1145,
      "step": 14740
    },
    {
      "epoch": 2.846940745029917,
      "grad_norm": 4.926102161407471,
      "learning_rate": 5.766261339509748e-06,
      "loss": 0.9595,
      "step": 14750
    },
    {
      "epoch": 2.8488708743485813,
      "grad_norm": 7.347893714904785,
      "learning_rate": 5.756610692916426e-06,
      "loss": 1.113,
      "step": 14760
    },
    {
      "epoch": 2.8508010036672458,
      "grad_norm": 9.205131530761719,
      "learning_rate": 5.7469600463231044e-06,
      "loss": 0.9573,
      "step": 14770
    },
    {
      "epoch": 2.85273113298591,
      "grad_norm": 0.39034584164619446,
      "learning_rate": 5.737309399729782e-06,
      "loss": 1.1803,
      "step": 14780
    },
    {
      "epoch": 2.8546612623045746,
      "grad_norm": 8.02646541595459,
      "learning_rate": 5.72765875313646e-06,
      "loss": 0.6812,
      "step": 14790
    },
    {
      "epoch": 2.8565913916232386,
      "grad_norm": 38.07777404785156,
      "learning_rate": 5.718008106543139e-06,
      "loss": 1.3303,
      "step": 14800
    },
    {
      "epoch": 2.858521520941903,
      "grad_norm": 16.317138671875,
      "learning_rate": 5.708357459949817e-06,
      "loss": 1.2818,
      "step": 14810
    },
    {
      "epoch": 2.8604516502605675,
      "grad_norm": 5.709144115447998,
      "learning_rate": 5.698706813356495e-06,
      "loss": 1.057,
      "step": 14820
    },
    {
      "epoch": 2.862381779579232,
      "grad_norm": 13.91159439086914,
      "learning_rate": 5.689056166763174e-06,
      "loss": 1.3412,
      "step": 14830
    },
    {
      "epoch": 2.8643119088978963,
      "grad_norm": 19.8203067779541,
      "learning_rate": 5.679405520169852e-06,
      "loss": 0.6716,
      "step": 14840
    },
    {
      "epoch": 2.8662420382165603,
      "grad_norm": 11.41010570526123,
      "learning_rate": 5.669754873576529e-06,
      "loss": 0.5342,
      "step": 14850
    },
    {
      "epoch": 2.8681721675352247,
      "grad_norm": 5.333387851715088,
      "learning_rate": 5.660104226983209e-06,
      "loss": 1.0784,
      "step": 14860
    },
    {
      "epoch": 2.870102296853889,
      "grad_norm": 74.71604919433594,
      "learning_rate": 5.650453580389886e-06,
      "loss": 0.8911,
      "step": 14870
    },
    {
      "epoch": 2.8720324261725536,
      "grad_norm": 13.491657257080078,
      "learning_rate": 5.640802933796566e-06,
      "loss": 0.7964,
      "step": 14880
    },
    {
      "epoch": 2.873962555491218,
      "grad_norm": 13.494026184082031,
      "learning_rate": 5.631152287203243e-06,
      "loss": 0.823,
      "step": 14890
    },
    {
      "epoch": 2.875892684809882,
      "grad_norm": 29.346086502075195,
      "learning_rate": 5.621501640609921e-06,
      "loss": 0.8301,
      "step": 14900
    },
    {
      "epoch": 2.8778228141285465,
      "grad_norm": 6.150303840637207,
      "learning_rate": 5.6118509940166e-06,
      "loss": 0.613,
      "step": 14910
    },
    {
      "epoch": 2.879752943447211,
      "grad_norm": 0.6184297800064087,
      "learning_rate": 5.602200347423278e-06,
      "loss": 1.2356,
      "step": 14920
    },
    {
      "epoch": 2.8816830727658753,
      "grad_norm": 11.58712387084961,
      "learning_rate": 5.592549700829956e-06,
      "loss": 0.6732,
      "step": 14930
    },
    {
      "epoch": 2.8836132020845398,
      "grad_norm": 93.74922180175781,
      "learning_rate": 5.5828990542366344e-06,
      "loss": 1.1567,
      "step": 14940
    },
    {
      "epoch": 2.8855433314032037,
      "grad_norm": 162.56790161132812,
      "learning_rate": 5.573248407643312e-06,
      "loss": 0.5644,
      "step": 14950
    },
    {
      "epoch": 2.8874734607218686,
      "grad_norm": 15.58378791809082,
      "learning_rate": 5.56359776104999e-06,
      "loss": 1.0959,
      "step": 14960
    },
    {
      "epoch": 2.8894035900405326,
      "grad_norm": 45.28349685668945,
      "learning_rate": 5.553947114456669e-06,
      "loss": 1.8536,
      "step": 14970
    },
    {
      "epoch": 2.891333719359197,
      "grad_norm": 4.082364082336426,
      "learning_rate": 5.544296467863347e-06,
      "loss": 0.9515,
      "step": 14980
    },
    {
      "epoch": 2.8932638486778615,
      "grad_norm": 2.601813793182373,
      "learning_rate": 5.534645821270026e-06,
      "loss": 0.5435,
      "step": 14990
    },
    {
      "epoch": 2.895193977996526,
      "grad_norm": 1.8016583919525146,
      "learning_rate": 5.524995174676704e-06,
      "loss": 1.0982,
      "step": 15000
    },
    {
      "epoch": 2.8971241073151903,
      "grad_norm": 36.549774169921875,
      "learning_rate": 5.515344528083382e-06,
      "loss": 0.8267,
      "step": 15010
    },
    {
      "epoch": 2.8990542366338543,
      "grad_norm": 5.847691059112549,
      "learning_rate": 5.505693881490061e-06,
      "loss": 0.9452,
      "step": 15020
    },
    {
      "epoch": 2.9009843659525187,
      "grad_norm": 7.896668434143066,
      "learning_rate": 5.496043234896739e-06,
      "loss": 0.4648,
      "step": 15030
    },
    {
      "epoch": 2.902914495271183,
      "grad_norm": 0.1634557545185089,
      "learning_rate": 5.486392588303416e-06,
      "loss": 0.8896,
      "step": 15040
    },
    {
      "epoch": 2.9048446245898476,
      "grad_norm": 129.48736572265625,
      "learning_rate": 5.476741941710096e-06,
      "loss": 2.1986,
      "step": 15050
    },
    {
      "epoch": 2.906774753908512,
      "grad_norm": 7.39813756942749,
      "learning_rate": 5.467091295116773e-06,
      "loss": 0.479,
      "step": 15060
    },
    {
      "epoch": 2.908704883227176,
      "grad_norm": 1.1378862857818604,
      "learning_rate": 5.457440648523451e-06,
      "loss": 1.2647,
      "step": 15070
    },
    {
      "epoch": 2.9106350125458404,
      "grad_norm": 2.609147071838379,
      "learning_rate": 5.44779000193013e-06,
      "loss": 1.4,
      "step": 15080
    },
    {
      "epoch": 2.912565141864505,
      "grad_norm": 8.167851448059082,
      "learning_rate": 5.438139355336808e-06,
      "loss": 0.7789,
      "step": 15090
    },
    {
      "epoch": 2.9144952711831693,
      "grad_norm": 17.604169845581055,
      "learning_rate": 5.4284887087434865e-06,
      "loss": 0.6696,
      "step": 15100
    },
    {
      "epoch": 2.9164254005018337,
      "grad_norm": 12.80713939666748,
      "learning_rate": 5.4188380621501644e-06,
      "loss": 0.9769,
      "step": 15110
    },
    {
      "epoch": 2.9183555298204977,
      "grad_norm": 1.6630839109420776,
      "learning_rate": 5.4091874155568424e-06,
      "loss": 0.6168,
      "step": 15120
    },
    {
      "epoch": 2.920285659139162,
      "grad_norm": 8.127511024475098,
      "learning_rate": 5.399536768963521e-06,
      "loss": 0.3785,
      "step": 15130
    },
    {
      "epoch": 2.9222157884578266,
      "grad_norm": 15.310152053833008,
      "learning_rate": 5.389886122370199e-06,
      "loss": 1.0148,
      "step": 15140
    },
    {
      "epoch": 2.924145917776491,
      "grad_norm": 0.6501838564872742,
      "learning_rate": 5.380235475776877e-06,
      "loss": 0.8019,
      "step": 15150
    },
    {
      "epoch": 2.9260760470951555,
      "grad_norm": 12.623298645019531,
      "learning_rate": 5.370584829183556e-06,
      "loss": 1.1544,
      "step": 15160
    },
    {
      "epoch": 2.92800617641382,
      "grad_norm": 14.480874061584473,
      "learning_rate": 5.360934182590234e-06,
      "loss": 1.54,
      "step": 15170
    },
    {
      "epoch": 2.9299363057324843,
      "grad_norm": 20.485048294067383,
      "learning_rate": 5.351283535996912e-06,
      "loss": 0.5266,
      "step": 15180
    },
    {
      "epoch": 2.9318664350511483,
      "grad_norm": 1.189569115638733,
      "learning_rate": 5.341632889403591e-06,
      "loss": 0.7353,
      "step": 15190
    },
    {
      "epoch": 2.9337965643698127,
      "grad_norm": 6.903083801269531,
      "learning_rate": 5.331982242810269e-06,
      "loss": 0.8756,
      "step": 15200
    },
    {
      "epoch": 2.935726693688477,
      "grad_norm": 6.025068759918213,
      "learning_rate": 5.322331596216948e-06,
      "loss": 1.0206,
      "step": 15210
    },
    {
      "epoch": 2.9376568230071416,
      "grad_norm": 2.3930437564849854,
      "learning_rate": 5.312680949623626e-06,
      "loss": 0.4587,
      "step": 15220
    },
    {
      "epoch": 2.939586952325806,
      "grad_norm": 5.6077985763549805,
      "learning_rate": 5.303030303030303e-06,
      "loss": 1.5265,
      "step": 15230
    },
    {
      "epoch": 2.94151708164447,
      "grad_norm": 80.4455337524414,
      "learning_rate": 5.2933796564369825e-06,
      "loss": 0.8004,
      "step": 15240
    },
    {
      "epoch": 2.9434472109631344,
      "grad_norm": 45.07389831542969,
      "learning_rate": 5.28372900984366e-06,
      "loss": 1.7127,
      "step": 15250
    },
    {
      "epoch": 2.945377340281799,
      "grad_norm": 1.1610537767410278,
      "learning_rate": 5.274078363250338e-06,
      "loss": 0.6323,
      "step": 15260
    },
    {
      "epoch": 2.9473074696004633,
      "grad_norm": 2.1908271312713623,
      "learning_rate": 5.2644277166570165e-06,
      "loss": 1.6554,
      "step": 15270
    },
    {
      "epoch": 2.9492375989191277,
      "grad_norm": 3.288444757461548,
      "learning_rate": 5.2547770700636944e-06,
      "loss": 1.3459,
      "step": 15280
    },
    {
      "epoch": 2.9511677282377917,
      "grad_norm": 1.5628741979599,
      "learning_rate": 5.2451264234703724e-06,
      "loss": 0.1768,
      "step": 15290
    },
    {
      "epoch": 2.953097857556456,
      "grad_norm": 5.832451820373535,
      "learning_rate": 5.235475776877051e-06,
      "loss": 1.0268,
      "step": 15300
    },
    {
      "epoch": 2.9550279868751206,
      "grad_norm": 12.650599479675293,
      "learning_rate": 5.225825130283729e-06,
      "loss": 1.2047,
      "step": 15310
    },
    {
      "epoch": 2.956958116193785,
      "grad_norm": 7.460720539093018,
      "learning_rate": 5.216174483690407e-06,
      "loss": 0.9129,
      "step": 15320
    },
    {
      "epoch": 2.9588882455124494,
      "grad_norm": 2.4482085704803467,
      "learning_rate": 5.206523837097086e-06,
      "loss": 0.7832,
      "step": 15330
    },
    {
      "epoch": 2.9608183748311134,
      "grad_norm": 40.503990173339844,
      "learning_rate": 5.196873190503764e-06,
      "loss": 0.9766,
      "step": 15340
    },
    {
      "epoch": 2.9627485041497783,
      "grad_norm": 4.924970626831055,
      "learning_rate": 5.187222543910443e-06,
      "loss": 1.0289,
      "step": 15350
    },
    {
      "epoch": 2.9646786334684423,
      "grad_norm": 104.31356048583984,
      "learning_rate": 5.177571897317121e-06,
      "loss": 0.7373,
      "step": 15360
    },
    {
      "epoch": 2.9666087627871067,
      "grad_norm": 19.080554962158203,
      "learning_rate": 5.167921250723799e-06,
      "loss": 0.6267,
      "step": 15370
    },
    {
      "epoch": 2.968538892105771,
      "grad_norm": 2.9598288536071777,
      "learning_rate": 5.158270604130478e-06,
      "loss": 0.7502,
      "step": 15380
    },
    {
      "epoch": 2.9704690214244356,
      "grad_norm": 3.0664117336273193,
      "learning_rate": 5.148619957537156e-06,
      "loss": 0.7627,
      "step": 15390
    },
    {
      "epoch": 2.9723991507431,
      "grad_norm": 1.8904544115066528,
      "learning_rate": 5.138969310943833e-06,
      "loss": 0.8138,
      "step": 15400
    },
    {
      "epoch": 2.974329280061764,
      "grad_norm": 5.437111854553223,
      "learning_rate": 5.1293186643505125e-06,
      "loss": 0.8615,
      "step": 15410
    },
    {
      "epoch": 2.9762594093804284,
      "grad_norm": 195.5040283203125,
      "learning_rate": 5.11966801775719e-06,
      "loss": 0.5446,
      "step": 15420
    },
    {
      "epoch": 2.978189538699093,
      "grad_norm": 54.30030059814453,
      "learning_rate": 5.110017371163868e-06,
      "loss": 2.4359,
      "step": 15430
    },
    {
      "epoch": 2.9801196680177573,
      "grad_norm": 14.345706939697266,
      "learning_rate": 5.1003667245705465e-06,
      "loss": 0.9307,
      "step": 15440
    },
    {
      "epoch": 2.9820497973364217,
      "grad_norm": 73.05831146240234,
      "learning_rate": 5.0907160779772245e-06,
      "loss": 0.6857,
      "step": 15450
    },
    {
      "epoch": 2.9839799266550857,
      "grad_norm": 5.421019554138184,
      "learning_rate": 5.081065431383903e-06,
      "loss": 0.3877,
      "step": 15460
    },
    {
      "epoch": 2.98591005597375,
      "grad_norm": 1.1178828477859497,
      "learning_rate": 5.071414784790581e-06,
      "loss": 0.9428,
      "step": 15470
    },
    {
      "epoch": 2.9878401852924146,
      "grad_norm": 0.2061605453491211,
      "learning_rate": 5.061764138197259e-06,
      "loss": 1.2345,
      "step": 15480
    },
    {
      "epoch": 2.989770314611079,
      "grad_norm": 9.312531471252441,
      "learning_rate": 5.052113491603938e-06,
      "loss": 0.7591,
      "step": 15490
    },
    {
      "epoch": 2.9917004439297434,
      "grad_norm": 13.12600040435791,
      "learning_rate": 5.042462845010616e-06,
      "loss": 1.0322,
      "step": 15500
    },
    {
      "epoch": 2.9936305732484074,
      "grad_norm": 109.1051254272461,
      "learning_rate": 5.032812198417294e-06,
      "loss": 1.4803,
      "step": 15510
    },
    {
      "epoch": 2.995560702567072,
      "grad_norm": 8.245319366455078,
      "learning_rate": 5.023161551823973e-06,
      "loss": 1.2629,
      "step": 15520
    },
    {
      "epoch": 2.9974908318857363,
      "grad_norm": 1.9114980697631836,
      "learning_rate": 5.013510905230651e-06,
      "loss": 1.0767,
      "step": 15530
    },
    {
      "epoch": 2.9994209612044007,
      "grad_norm": 29.876754760742188,
      "learning_rate": 5.003860258637329e-06,
      "loss": 0.8931,
      "step": 15540
    },
    {
      "epoch": 3.0,
      "eval_intent_acc": 0.8446180555555556,
      "eval_loss": 1.3325026035308838,
      "eval_runtime": 22.8711,
      "eval_samples_per_second": 50.369,
      "eval_slot_f1": 0.6194837635303914,
      "eval_slot_precision": 0.5881422924901186,
      "eval_slot_recall": 0.6543535620052771,
      "eval_steps_per_second": 25.185,
      "step": 15543
    },
    {
      "epoch": 3.001351090523065,
      "grad_norm": 0.7214230895042419,
      "learning_rate": 4.994209612044008e-06,
      "loss": 0.3943,
      "step": 15550
    },
    {
      "epoch": 3.0032812198417296,
      "grad_norm": 5.051427364349365,
      "learning_rate": 4.984558965450686e-06,
      "loss": 0.5396,
      "step": 15560
    },
    {
      "epoch": 3.0052113491603936,
      "grad_norm": 51.20978546142578,
      "learning_rate": 4.974908318857364e-06,
      "loss": 0.4851,
      "step": 15570
    },
    {
      "epoch": 3.007141478479058,
      "grad_norm": 3.7181942462921143,
      "learning_rate": 4.9652576722640425e-06,
      "loss": 0.9135,
      "step": 15580
    },
    {
      "epoch": 3.0090716077977224,
      "grad_norm": 1.1153829097747803,
      "learning_rate": 4.9556070256707205e-06,
      "loss": 0.7041,
      "step": 15590
    },
    {
      "epoch": 3.011001737116387,
      "grad_norm": 0.4479788839817047,
      "learning_rate": 4.9459563790773985e-06,
      "loss": 0.3107,
      "step": 15600
    },
    {
      "epoch": 3.0129318664350513,
      "grad_norm": 3.0202856063842773,
      "learning_rate": 4.9363057324840765e-06,
      "loss": 0.6691,
      "step": 15610
    },
    {
      "epoch": 3.0148619957537157,
      "grad_norm": 0.3478333055973053,
      "learning_rate": 4.926655085890755e-06,
      "loss": 0.5459,
      "step": 15620
    },
    {
      "epoch": 3.0167921250723797,
      "grad_norm": 46.017574310302734,
      "learning_rate": 4.917004439297433e-06,
      "loss": 0.4046,
      "step": 15630
    },
    {
      "epoch": 3.018722254391044,
      "grad_norm": 1.5718916654586792,
      "learning_rate": 4.907353792704111e-06,
      "loss": 0.8225,
      "step": 15640
    },
    {
      "epoch": 3.0206523837097086,
      "grad_norm": 77.5649642944336,
      "learning_rate": 4.89770314611079e-06,
      "loss": 0.8761,
      "step": 15650
    },
    {
      "epoch": 3.022582513028373,
      "grad_norm": 4.430008411407471,
      "learning_rate": 4.888052499517468e-06,
      "loss": 0.653,
      "step": 15660
    },
    {
      "epoch": 3.0245126423470374,
      "grad_norm": 8.474444389343262,
      "learning_rate": 4.878401852924146e-06,
      "loss": 0.6401,
      "step": 15670
    },
    {
      "epoch": 3.0264427716657014,
      "grad_norm": 3.5711376667022705,
      "learning_rate": 4.868751206330825e-06,
      "loss": 0.8566,
      "step": 15680
    },
    {
      "epoch": 3.028372900984366,
      "grad_norm": 8.756133079528809,
      "learning_rate": 4.859100559737503e-06,
      "loss": 0.7949,
      "step": 15690
    },
    {
      "epoch": 3.0303030303030303,
      "grad_norm": 4.380042552947998,
      "learning_rate": 4.849449913144181e-06,
      "loss": 0.6463,
      "step": 15700
    },
    {
      "epoch": 3.0322331596216947,
      "grad_norm": 8.789305686950684,
      "learning_rate": 4.839799266550859e-06,
      "loss": 0.7096,
      "step": 15710
    },
    {
      "epoch": 3.034163288940359,
      "grad_norm": 1.627289891242981,
      "learning_rate": 4.830148619957538e-06,
      "loss": 0.6729,
      "step": 15720
    },
    {
      "epoch": 3.0360934182590236,
      "grad_norm": 140.32879638671875,
      "learning_rate": 4.820497973364216e-06,
      "loss": 1.3446,
      "step": 15730
    },
    {
      "epoch": 3.0380235475776876,
      "grad_norm": 4.744457721710205,
      "learning_rate": 4.810847326770894e-06,
      "loss": 1.2959,
      "step": 15740
    },
    {
      "epoch": 3.039953676896352,
      "grad_norm": 0.2392895519733429,
      "learning_rate": 4.8011966801775725e-06,
      "loss": 0.8844,
      "step": 15750
    },
    {
      "epoch": 3.0418838062150164,
      "grad_norm": 3.6681935787200928,
      "learning_rate": 4.7915460335842505e-06,
      "loss": 0.6951,
      "step": 15760
    },
    {
      "epoch": 3.043813935533681,
      "grad_norm": 45.6630744934082,
      "learning_rate": 4.781895386990929e-06,
      "loss": 0.4808,
      "step": 15770
    },
    {
      "epoch": 3.0457440648523453,
      "grad_norm": 16.720359802246094,
      "learning_rate": 4.7722447403976065e-06,
      "loss": 0.2007,
      "step": 15780
    },
    {
      "epoch": 3.0476741941710093,
      "grad_norm": 8.305829048156738,
      "learning_rate": 4.762594093804285e-06,
      "loss": 0.6006,
      "step": 15790
    },
    {
      "epoch": 3.0496043234896737,
      "grad_norm": 1.0548675060272217,
      "learning_rate": 4.752943447210963e-06,
      "loss": 0.5603,
      "step": 15800
    },
    {
      "epoch": 3.051534452808338,
      "grad_norm": 0.9720868468284607,
      "learning_rate": 4.743292800617641e-06,
      "loss": 0.3813,
      "step": 15810
    },
    {
      "epoch": 3.0534645821270026,
      "grad_norm": 5.320306777954102,
      "learning_rate": 4.73364215402432e-06,
      "loss": 0.5569,
      "step": 15820
    },
    {
      "epoch": 3.055394711445667,
      "grad_norm": 2.4140007495880127,
      "learning_rate": 4.723991507430998e-06,
      "loss": 0.4794,
      "step": 15830
    },
    {
      "epoch": 3.0573248407643314,
      "grad_norm": 51.283546447753906,
      "learning_rate": 4.714340860837677e-06,
      "loss": 0.7885,
      "step": 15840
    },
    {
      "epoch": 3.0592549700829954,
      "grad_norm": 2.270050048828125,
      "learning_rate": 4.704690214244355e-06,
      "loss": 0.9865,
      "step": 15850
    },
    {
      "epoch": 3.06118509940166,
      "grad_norm": 9.58587646484375,
      "learning_rate": 4.695039567651033e-06,
      "loss": 0.4174,
      "step": 15860
    },
    {
      "epoch": 3.0631152287203243,
      "grad_norm": 0.8940389752388,
      "learning_rate": 4.685388921057712e-06,
      "loss": 1.169,
      "step": 15870
    },
    {
      "epoch": 3.0650453580389887,
      "grad_norm": 131.90443420410156,
      "learning_rate": 4.67573827446439e-06,
      "loss": 1.4676,
      "step": 15880
    },
    {
      "epoch": 3.066975487357653,
      "grad_norm": 3.6566286087036133,
      "learning_rate": 4.666087627871068e-06,
      "loss": 0.7018,
      "step": 15890
    },
    {
      "epoch": 3.068905616676317,
      "grad_norm": 3.9672200679779053,
      "learning_rate": 4.656436981277746e-06,
      "loss": 1.5454,
      "step": 15900
    },
    {
      "epoch": 3.0708357459949815,
      "grad_norm": 17.58473777770996,
      "learning_rate": 4.6467863346844245e-06,
      "loss": 0.6802,
      "step": 15910
    },
    {
      "epoch": 3.072765875313646,
      "grad_norm": 6.014474391937256,
      "learning_rate": 4.6371356880911025e-06,
      "loss": 0.9837,
      "step": 15920
    },
    {
      "epoch": 3.0746960046323104,
      "grad_norm": 1.6555366516113281,
      "learning_rate": 4.6274850414977805e-06,
      "loss": 0.9563,
      "step": 15930
    },
    {
      "epoch": 3.076626133950975,
      "grad_norm": 0.822530210018158,
      "learning_rate": 4.617834394904459e-06,
      "loss": 0.9886,
      "step": 15940
    },
    {
      "epoch": 3.0785562632696393,
      "grad_norm": 60.26913070678711,
      "learning_rate": 4.608183748311137e-06,
      "loss": 0.733,
      "step": 15950
    },
    {
      "epoch": 3.0804863925883033,
      "grad_norm": 34.31299591064453,
      "learning_rate": 4.598533101717815e-06,
      "loss": 0.7119,
      "step": 15960
    },
    {
      "epoch": 3.0824165219069677,
      "grad_norm": 35.56551742553711,
      "learning_rate": 4.588882455124493e-06,
      "loss": 0.6711,
      "step": 15970
    },
    {
      "epoch": 3.084346651225632,
      "grad_norm": 2.9199345111846924,
      "learning_rate": 4.579231808531172e-06,
      "loss": 0.9928,
      "step": 15980
    },
    {
      "epoch": 3.0862767805442965,
      "grad_norm": 3.50571608543396,
      "learning_rate": 4.56958116193785e-06,
      "loss": 1.0086,
      "step": 15990
    },
    {
      "epoch": 3.088206909862961,
      "grad_norm": 0.23161941766738892,
      "learning_rate": 4.559930515344528e-06,
      "loss": 0.6877,
      "step": 16000
    },
    {
      "epoch": 3.090137039181625,
      "grad_norm": 0.47485780715942383,
      "learning_rate": 4.550279868751207e-06,
      "loss": 0.932,
      "step": 16010
    },
    {
      "epoch": 3.0920671685002894,
      "grad_norm": 0.30014774203300476,
      "learning_rate": 4.540629222157885e-06,
      "loss": 0.6624,
      "step": 16020
    },
    {
      "epoch": 3.093997297818954,
      "grad_norm": 0.6402385234832764,
      "learning_rate": 4.530978575564563e-06,
      "loss": 0.8516,
      "step": 16030
    },
    {
      "epoch": 3.0959274271376183,
      "grad_norm": 2.9012393951416016,
      "learning_rate": 4.521327928971242e-06,
      "loss": 0.7652,
      "step": 16040
    },
    {
      "epoch": 3.0978575564562827,
      "grad_norm": 8.012624740600586,
      "learning_rate": 4.51167728237792e-06,
      "loss": 1.0553,
      "step": 16050
    },
    {
      "epoch": 3.099787685774947,
      "grad_norm": 44.906089782714844,
      "learning_rate": 4.5020266357845986e-06,
      "loss": 1.1519,
      "step": 16060
    },
    {
      "epoch": 3.101717815093611,
      "grad_norm": 1.5561602115631104,
      "learning_rate": 4.492375989191276e-06,
      "loss": 0.8431,
      "step": 16070
    },
    {
      "epoch": 3.1036479444122755,
      "grad_norm": 73.04546356201172,
      "learning_rate": 4.4827253425979545e-06,
      "loss": 0.9263,
      "step": 16080
    },
    {
      "epoch": 3.10557807373094,
      "grad_norm": 5.659934997558594,
      "learning_rate": 4.4730746960046325e-06,
      "loss": 0.4331,
      "step": 16090
    },
    {
      "epoch": 3.1075082030496044,
      "grad_norm": 140.09481811523438,
      "learning_rate": 4.4634240494113105e-06,
      "loss": 1.1743,
      "step": 16100
    },
    {
      "epoch": 3.109438332368269,
      "grad_norm": 3.9766030311584473,
      "learning_rate": 4.453773402817989e-06,
      "loss": 0.6889,
      "step": 16110
    },
    {
      "epoch": 3.111368461686933,
      "grad_norm": 2.9540743827819824,
      "learning_rate": 4.444122756224667e-06,
      "loss": 0.7353,
      "step": 16120
    },
    {
      "epoch": 3.1132985910055972,
      "grad_norm": 5.54327392578125,
      "learning_rate": 4.434472109631346e-06,
      "loss": 0.3944,
      "step": 16130
    },
    {
      "epoch": 3.1152287203242617,
      "grad_norm": 2.2527027130126953,
      "learning_rate": 4.424821463038023e-06,
      "loss": 0.6513,
      "step": 16140
    },
    {
      "epoch": 3.117158849642926,
      "grad_norm": 3.6728241443634033,
      "learning_rate": 4.415170816444702e-06,
      "loss": 0.957,
      "step": 16150
    },
    {
      "epoch": 3.1190889789615905,
      "grad_norm": 5.32325553894043,
      "learning_rate": 4.40552016985138e-06,
      "loss": 0.4261,
      "step": 16160
    },
    {
      "epoch": 3.121019108280255,
      "grad_norm": 1.7014330625534058,
      "learning_rate": 4.395869523258058e-06,
      "loss": 0.2335,
      "step": 16170
    },
    {
      "epoch": 3.122949237598919,
      "grad_norm": 0.1356104016304016,
      "learning_rate": 4.386218876664737e-06,
      "loss": 1.1243,
      "step": 16180
    },
    {
      "epoch": 3.1248793669175834,
      "grad_norm": 8.675358772277832,
      "learning_rate": 4.376568230071415e-06,
      "loss": 0.6624,
      "step": 16190
    },
    {
      "epoch": 3.126809496236248,
      "grad_norm": 26.933536529541016,
      "learning_rate": 4.366917583478094e-06,
      "loss": 0.7446,
      "step": 16200
    },
    {
      "epoch": 3.1287396255549123,
      "grad_norm": 12.288716316223145,
      "learning_rate": 4.357266936884772e-06,
      "loss": 0.6195,
      "step": 16210
    },
    {
      "epoch": 3.1306697548735767,
      "grad_norm": 86.84742736816406,
      "learning_rate": 4.34761629029145e-06,
      "loss": 1.133,
      "step": 16220
    },
    {
      "epoch": 3.132599884192241,
      "grad_norm": 12.67698860168457,
      "learning_rate": 4.337965643698129e-06,
      "loss": 0.7372,
      "step": 16230
    },
    {
      "epoch": 3.134530013510905,
      "grad_norm": 1.9578052759170532,
      "learning_rate": 4.3283149971048066e-06,
      "loss": 0.7112,
      "step": 16240
    },
    {
      "epoch": 3.1364601428295695,
      "grad_norm": 1.0448122024536133,
      "learning_rate": 4.3186643505114846e-06,
      "loss": 0.4333,
      "step": 16250
    },
    {
      "epoch": 3.138390272148234,
      "grad_norm": 0.8767863512039185,
      "learning_rate": 4.3090137039181625e-06,
      "loss": 0.8798,
      "step": 16260
    },
    {
      "epoch": 3.1403204014668984,
      "grad_norm": 9.632244110107422,
      "learning_rate": 4.299363057324841e-06,
      "loss": 0.3767,
      "step": 16270
    },
    {
      "epoch": 3.142250530785563,
      "grad_norm": 86.51559448242188,
      "learning_rate": 4.289712410731519e-06,
      "loss": 1.0474,
      "step": 16280
    },
    {
      "epoch": 3.144180660104227,
      "grad_norm": 2.0078492164611816,
      "learning_rate": 4.280061764138197e-06,
      "loss": 0.3063,
      "step": 16290
    },
    {
      "epoch": 3.1461107894228912,
      "grad_norm": 0.23707734048366547,
      "learning_rate": 4.270411117544876e-06,
      "loss": 1.0117,
      "step": 16300
    },
    {
      "epoch": 3.1480409187415557,
      "grad_norm": 14.640483856201172,
      "learning_rate": 4.260760470951554e-06,
      "loss": 1.1067,
      "step": 16310
    },
    {
      "epoch": 3.14997104806022,
      "grad_norm": 3.080348491668701,
      "learning_rate": 4.251109824358232e-06,
      "loss": 0.769,
      "step": 16320
    },
    {
      "epoch": 3.1519011773788845,
      "grad_norm": 0.1873287856578827,
      "learning_rate": 4.24145917776491e-06,
      "loss": 0.5234,
      "step": 16330
    },
    {
      "epoch": 3.1538313066975485,
      "grad_norm": 13.428139686584473,
      "learning_rate": 4.231808531171589e-06,
      "loss": 0.4108,
      "step": 16340
    },
    {
      "epoch": 3.155761436016213,
      "grad_norm": 13.756068229675293,
      "learning_rate": 4.222157884578267e-06,
      "loss": 1.1982,
      "step": 16350
    },
    {
      "epoch": 3.1576915653348774,
      "grad_norm": 4.769608497619629,
      "learning_rate": 4.212507237984945e-06,
      "loss": 0.2468,
      "step": 16360
    },
    {
      "epoch": 3.159621694653542,
      "grad_norm": 8.574517250061035,
      "learning_rate": 4.202856591391624e-06,
      "loss": 1.4194,
      "step": 16370
    },
    {
      "epoch": 3.1615518239722062,
      "grad_norm": 29.777877807617188,
      "learning_rate": 4.193205944798302e-06,
      "loss": 1.6048,
      "step": 16380
    },
    {
      "epoch": 3.1634819532908707,
      "grad_norm": 2.0500264167785645,
      "learning_rate": 4.18355529820498e-06,
      "loss": 0.3867,
      "step": 16390
    },
    {
      "epoch": 3.1654120826095347,
      "grad_norm": 9.475553512573242,
      "learning_rate": 4.173904651611659e-06,
      "loss": 0.309,
      "step": 16400
    },
    {
      "epoch": 3.167342211928199,
      "grad_norm": 0.738065242767334,
      "learning_rate": 4.1642540050183366e-06,
      "loss": 0.4312,
      "step": 16410
    },
    {
      "epoch": 3.1692723412468635,
      "grad_norm": 2.8593428134918213,
      "learning_rate": 4.154603358425015e-06,
      "loss": 0.9134,
      "step": 16420
    },
    {
      "epoch": 3.171202470565528,
      "grad_norm": 3.935802459716797,
      "learning_rate": 4.1449527118316925e-06,
      "loss": 0.6202,
      "step": 16430
    },
    {
      "epoch": 3.1731325998841924,
      "grad_norm": 72.84033966064453,
      "learning_rate": 4.135302065238371e-06,
      "loss": 0.6408,
      "step": 16440
    },
    {
      "epoch": 3.175062729202857,
      "grad_norm": 1.0698742866516113,
      "learning_rate": 4.125651418645049e-06,
      "loss": 0.501,
      "step": 16450
    },
    {
      "epoch": 3.176992858521521,
      "grad_norm": 3.1047286987304688,
      "learning_rate": 4.116000772051727e-06,
      "loss": 0.264,
      "step": 16460
    },
    {
      "epoch": 3.1789229878401852,
      "grad_norm": 26.398582458496094,
      "learning_rate": 4.106350125458406e-06,
      "loss": 0.7457,
      "step": 16470
    },
    {
      "epoch": 3.1808531171588497,
      "grad_norm": 23.77861213684082,
      "learning_rate": 4.096699478865084e-06,
      "loss": 0.7712,
      "step": 16480
    },
    {
      "epoch": 3.182783246477514,
      "grad_norm": 19.02921485900879,
      "learning_rate": 4.087048832271763e-06,
      "loss": 0.5558,
      "step": 16490
    },
    {
      "epoch": 3.1847133757961785,
      "grad_norm": 0.8103765249252319,
      "learning_rate": 4.077398185678441e-06,
      "loss": 0.9768,
      "step": 16500
    },
    {
      "epoch": 3.1866435051148425,
      "grad_norm": 6.13883638381958,
      "learning_rate": 4.067747539085119e-06,
      "loss": 0.543,
      "step": 16510
    },
    {
      "epoch": 3.188573634433507,
      "grad_norm": 3.305222272872925,
      "learning_rate": 4.058096892491797e-06,
      "loss": 0.9639,
      "step": 16520
    },
    {
      "epoch": 3.1905037637521714,
      "grad_norm": 1.2075494527816772,
      "learning_rate": 4.048446245898476e-06,
      "loss": 0.7331,
      "step": 16530
    },
    {
      "epoch": 3.192433893070836,
      "grad_norm": 11.467942237854004,
      "learning_rate": 4.038795599305154e-06,
      "loss": 1.1781,
      "step": 16540
    },
    {
      "epoch": 3.1943640223895002,
      "grad_norm": 2.7221150398254395,
      "learning_rate": 4.029144952711832e-06,
      "loss": 1.2865,
      "step": 16550
    },
    {
      "epoch": 3.196294151708164,
      "grad_norm": 10.433573722839355,
      "learning_rate": 4.019494306118511e-06,
      "loss": 0.5911,
      "step": 16560
    },
    {
      "epoch": 3.1982242810268287,
      "grad_norm": 0.13726961612701416,
      "learning_rate": 4.009843659525189e-06,
      "loss": 0.4916,
      "step": 16570
    },
    {
      "epoch": 3.200154410345493,
      "grad_norm": 3.2725234031677246,
      "learning_rate": 4.000193012931867e-06,
      "loss": 0.9548,
      "step": 16580
    },
    {
      "epoch": 3.2020845396641575,
      "grad_norm": 0.2519473433494568,
      "learning_rate": 3.990542366338545e-06,
      "loss": 0.1859,
      "step": 16590
    },
    {
      "epoch": 3.204014668982822,
      "grad_norm": 162.45126342773438,
      "learning_rate": 3.980891719745223e-06,
      "loss": 0.6452,
      "step": 16600
    },
    {
      "epoch": 3.2059447983014864,
      "grad_norm": 19.73065948486328,
      "learning_rate": 3.971241073151901e-06,
      "loss": 0.6829,
      "step": 16610
    },
    {
      "epoch": 3.2078749276201504,
      "grad_norm": 7.866395950317383,
      "learning_rate": 3.961590426558579e-06,
      "loss": 1.0641,
      "step": 16620
    },
    {
      "epoch": 3.209805056938815,
      "grad_norm": 0.35607829689979553,
      "learning_rate": 3.951939779965258e-06,
      "loss": 0.8298,
      "step": 16630
    },
    {
      "epoch": 3.2117351862574792,
      "grad_norm": 1.121538758277893,
      "learning_rate": 3.942289133371936e-06,
      "loss": 0.5601,
      "step": 16640
    },
    {
      "epoch": 3.2136653155761437,
      "grad_norm": 2.076916217803955,
      "learning_rate": 3.932638486778614e-06,
      "loss": 0.9816,
      "step": 16650
    },
    {
      "epoch": 3.215595444894808,
      "grad_norm": 0.644430935382843,
      "learning_rate": 3.922987840185293e-06,
      "loss": 1.1518,
      "step": 16660
    },
    {
      "epoch": 3.2175255742134725,
      "grad_norm": 2.9479689598083496,
      "learning_rate": 3.913337193591971e-06,
      "loss": 0.5831,
      "step": 16670
    },
    {
      "epoch": 3.2194557035321365,
      "grad_norm": 0.09069845080375671,
      "learning_rate": 3.903686546998649e-06,
      "loss": 0.3687,
      "step": 16680
    },
    {
      "epoch": 3.221385832850801,
      "grad_norm": 49.9088020324707,
      "learning_rate": 3.894035900405328e-06,
      "loss": 0.6549,
      "step": 16690
    },
    {
      "epoch": 3.2233159621694654,
      "grad_norm": 9.876072883605957,
      "learning_rate": 3.884385253812006e-06,
      "loss": 1.3558,
      "step": 16700
    },
    {
      "epoch": 3.22524609148813,
      "grad_norm": 0.05415089800953865,
      "learning_rate": 3.874734607218685e-06,
      "loss": 0.3773,
      "step": 16710
    },
    {
      "epoch": 3.2271762208067942,
      "grad_norm": 9.680463790893555,
      "learning_rate": 3.865083960625362e-06,
      "loss": 0.6565,
      "step": 16720
    },
    {
      "epoch": 3.229106350125458,
      "grad_norm": 84.01913452148438,
      "learning_rate": 3.855433314032041e-06,
      "loss": 1.1556,
      "step": 16730
    },
    {
      "epoch": 3.2310364794441226,
      "grad_norm": 1.6719279289245605,
      "learning_rate": 3.845782667438719e-06,
      "loss": 0.7174,
      "step": 16740
    },
    {
      "epoch": 3.232966608762787,
      "grad_norm": 12.283784866333008,
      "learning_rate": 3.836132020845397e-06,
      "loss": 0.9462,
      "step": 16750
    },
    {
      "epoch": 3.2348967380814515,
      "grad_norm": 8.199928283691406,
      "learning_rate": 3.826481374252075e-06,
      "loss": 0.61,
      "step": 16760
    },
    {
      "epoch": 3.236826867400116,
      "grad_norm": 28.485130310058594,
      "learning_rate": 3.816830727658753e-06,
      "loss": 1.1778,
      "step": 16770
    },
    {
      "epoch": 3.23875699671878,
      "grad_norm": 9.992707252502441,
      "learning_rate": 3.807180081065432e-06,
      "loss": 0.6814,
      "step": 16780
    },
    {
      "epoch": 3.2406871260374444,
      "grad_norm": 11.183938026428223,
      "learning_rate": 3.79752943447211e-06,
      "loss": 1.0693,
      "step": 16790
    },
    {
      "epoch": 3.242617255356109,
      "grad_norm": 9.053420066833496,
      "learning_rate": 3.7878787878787882e-06,
      "loss": 0.5592,
      "step": 16800
    },
    {
      "epoch": 3.244547384674773,
      "grad_norm": 18.236392974853516,
      "learning_rate": 3.7782281412854666e-06,
      "loss": 0.9355,
      "step": 16810
    },
    {
      "epoch": 3.2464775139934376,
      "grad_norm": 0.48931974172592163,
      "learning_rate": 3.768577494692145e-06,
      "loss": 0.2932,
      "step": 16820
    },
    {
      "epoch": 3.248407643312102,
      "grad_norm": 0.5020133852958679,
      "learning_rate": 3.7589268480988226e-06,
      "loss": 0.6466,
      "step": 16830
    },
    {
      "epoch": 3.2503377726307665,
      "grad_norm": 3.662400484085083,
      "learning_rate": 3.749276201505501e-06,
      "loss": 0.8678,
      "step": 16840
    },
    {
      "epoch": 3.2522679019494305,
      "grad_norm": 0.2622429132461548,
      "learning_rate": 3.7396255549121794e-06,
      "loss": 0.5794,
      "step": 16850
    },
    {
      "epoch": 3.254198031268095,
      "grad_norm": 9.14972972869873,
      "learning_rate": 3.7299749083188574e-06,
      "loss": 0.705,
      "step": 16860
    },
    {
      "epoch": 3.2561281605867594,
      "grad_norm": 2.700115203857422,
      "learning_rate": 3.720324261725536e-06,
      "loss": 0.3266,
      "step": 16870
    },
    {
      "epoch": 3.258058289905424,
      "grad_norm": 8.712120056152344,
      "learning_rate": 3.7106736151322142e-06,
      "loss": 0.5851,
      "step": 16880
    },
    {
      "epoch": 3.259988419224088,
      "grad_norm": 0.876803457736969,
      "learning_rate": 3.7010229685388926e-06,
      "loss": 0.6775,
      "step": 16890
    },
    {
      "epoch": 3.261918548542752,
      "grad_norm": 0.0407327376306057,
      "learning_rate": 3.6913723219455706e-06,
      "loss": 0.3638,
      "step": 16900
    },
    {
      "epoch": 3.2638486778614166,
      "grad_norm": 0.10343382507562637,
      "learning_rate": 3.681721675352249e-06,
      "loss": 0.4989,
      "step": 16910
    },
    {
      "epoch": 3.265778807180081,
      "grad_norm": 11.70749282836914,
      "learning_rate": 3.6720710287589274e-06,
      "loss": 0.5803,
      "step": 16920
    },
    {
      "epoch": 3.2677089364987455,
      "grad_norm": 16.471458435058594,
      "learning_rate": 3.662420382165605e-06,
      "loss": 0.5248,
      "step": 16930
    },
    {
      "epoch": 3.26963906581741,
      "grad_norm": 4.415524482727051,
      "learning_rate": 3.6527697355722834e-06,
      "loss": 0.5582,
      "step": 16940
    },
    {
      "epoch": 3.271569195136074,
      "grad_norm": 21.118270874023438,
      "learning_rate": 3.643119088978962e-06,
      "loss": 1.1826,
      "step": 16950
    },
    {
      "epoch": 3.2734993244547383,
      "grad_norm": 0.5315293073654175,
      "learning_rate": 3.6334684423856402e-06,
      "loss": 0.8341,
      "step": 16960
    },
    {
      "epoch": 3.2754294537734028,
      "grad_norm": 0.5875406861305237,
      "learning_rate": 3.6238177957923182e-06,
      "loss": 1.3347,
      "step": 16970
    },
    {
      "epoch": 3.277359583092067,
      "grad_norm": 7.732180595397949,
      "learning_rate": 3.6141671491989966e-06,
      "loss": 0.5927,
      "step": 16980
    },
    {
      "epoch": 3.2792897124107316,
      "grad_norm": 11.350166320800781,
      "learning_rate": 3.604516502605675e-06,
      "loss": 0.4555,
      "step": 16990
    },
    {
      "epoch": 3.2812198417293956,
      "grad_norm": 8.380501747131348,
      "learning_rate": 3.5948658560123535e-06,
      "loss": 0.5679,
      "step": 17000
    },
    {
      "epoch": 3.28314997104806,
      "grad_norm": 1.141339898109436,
      "learning_rate": 3.5852152094190314e-06,
      "loss": 0.7918,
      "step": 17010
    },
    {
      "epoch": 3.2850801003667245,
      "grad_norm": 8.600641250610352,
      "learning_rate": 3.5755645628257094e-06,
      "loss": 0.3892,
      "step": 17020
    },
    {
      "epoch": 3.287010229685389,
      "grad_norm": 0.5599185228347778,
      "learning_rate": 3.565913916232388e-06,
      "loss": 1.4375,
      "step": 17030
    },
    {
      "epoch": 3.2889403590040533,
      "grad_norm": 1.894639015197754,
      "learning_rate": 3.556263269639066e-06,
      "loss": 0.4532,
      "step": 17040
    },
    {
      "epoch": 3.290870488322718,
      "grad_norm": 37.093017578125,
      "learning_rate": 3.5466126230457442e-06,
      "loss": 1.7119,
      "step": 17050
    },
    {
      "epoch": 3.292800617641382,
      "grad_norm": 4.392537593841553,
      "learning_rate": 3.5369619764524226e-06,
      "loss": 0.3929,
      "step": 17060
    },
    {
      "epoch": 3.294730746960046,
      "grad_norm": 126.09510040283203,
      "learning_rate": 3.527311329859101e-06,
      "loss": 0.7233,
      "step": 17070
    },
    {
      "epoch": 3.2966608762787106,
      "grad_norm": 14.20681095123291,
      "learning_rate": 3.517660683265779e-06,
      "loss": 0.5688,
      "step": 17080
    },
    {
      "epoch": 3.298591005597375,
      "grad_norm": 0.8269808888435364,
      "learning_rate": 3.5080100366724574e-06,
      "loss": 0.5507,
      "step": 17090
    },
    {
      "epoch": 3.3005211349160395,
      "grad_norm": 1.4368075132369995,
      "learning_rate": 3.498359390079136e-06,
      "loss": 1.6608,
      "step": 17100
    },
    {
      "epoch": 3.302451264234704,
      "grad_norm": 3.2665505409240723,
      "learning_rate": 3.4887087434858134e-06,
      "loss": 0.7842,
      "step": 17110
    },
    {
      "epoch": 3.304381393553368,
      "grad_norm": 4.750354766845703,
      "learning_rate": 3.479058096892492e-06,
      "loss": 0.6516,
      "step": 17120
    },
    {
      "epoch": 3.3063115228720323,
      "grad_norm": 12.191856384277344,
      "learning_rate": 3.4694074502991702e-06,
      "loss": 0.6056,
      "step": 17130
    },
    {
      "epoch": 3.3082416521906968,
      "grad_norm": 10.919473648071289,
      "learning_rate": 3.4597568037058487e-06,
      "loss": 0.9035,
      "step": 17140
    },
    {
      "epoch": 3.310171781509361,
      "grad_norm": 0.8730136156082153,
      "learning_rate": 3.4501061571125266e-06,
      "loss": 1.161,
      "step": 17150
    },
    {
      "epoch": 3.3121019108280256,
      "grad_norm": 12.070833206176758,
      "learning_rate": 3.440455510519205e-06,
      "loss": 1.4305,
      "step": 17160
    },
    {
      "epoch": 3.3140320401466896,
      "grad_norm": 0.5133339166641235,
      "learning_rate": 3.4308048639258835e-06,
      "loss": 0.6353,
      "step": 17170
    },
    {
      "epoch": 3.315962169465354,
      "grad_norm": 1.8011404275894165,
      "learning_rate": 3.421154217332562e-06,
      "loss": 1.099,
      "step": 17180
    },
    {
      "epoch": 3.3178922987840185,
      "grad_norm": 2.587790012359619,
      "learning_rate": 3.41150357073924e-06,
      "loss": 0.4219,
      "step": 17190
    },
    {
      "epoch": 3.319822428102683,
      "grad_norm": 6.579780578613281,
      "learning_rate": 3.4018529241459183e-06,
      "loss": 0.2745,
      "step": 17200
    },
    {
      "epoch": 3.3217525574213473,
      "grad_norm": 28.28647804260254,
      "learning_rate": 3.3922022775525963e-06,
      "loss": 1.0062,
      "step": 17210
    },
    {
      "epoch": 3.3236826867400118,
      "grad_norm": 60.79775619506836,
      "learning_rate": 3.3825516309592742e-06,
      "loss": 0.7358,
      "step": 17220
    },
    {
      "epoch": 3.3256128160586758,
      "grad_norm": 1.4893170595169067,
      "learning_rate": 3.3729009843659526e-06,
      "loss": 0.6047,
      "step": 17230
    },
    {
      "epoch": 3.32754294537734,
      "grad_norm": 8.801939010620117,
      "learning_rate": 3.363250337772631e-06,
      "loss": 1.3539,
      "step": 17240
    },
    {
      "epoch": 3.3294730746960046,
      "grad_norm": 63.311405181884766,
      "learning_rate": 3.3535996911793095e-06,
      "loss": 1.2922,
      "step": 17250
    },
    {
      "epoch": 3.331403204014669,
      "grad_norm": 0.7553258538246155,
      "learning_rate": 3.3439490445859875e-06,
      "loss": 0.8685,
      "step": 17260
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 7.573069095611572,
      "learning_rate": 3.334298397992666e-06,
      "loss": 0.8599,
      "step": 17270
    },
    {
      "epoch": 3.335263462651998,
      "grad_norm": 40.8726921081543,
      "learning_rate": 3.3246477513993443e-06,
      "loss": 0.913,
      "step": 17280
    },
    {
      "epoch": 3.337193591970662,
      "grad_norm": 31.3011531829834,
      "learning_rate": 3.3149971048060227e-06,
      "loss": 1.1993,
      "step": 17290
    },
    {
      "epoch": 3.3391237212893263,
      "grad_norm": 0.1466207355260849,
      "learning_rate": 3.3053464582127002e-06,
      "loss": 0.2682,
      "step": 17300
    },
    {
      "epoch": 3.3410538506079908,
      "grad_norm": 7.025270938873291,
      "learning_rate": 3.2956958116193787e-06,
      "loss": 0.7511,
      "step": 17310
    },
    {
      "epoch": 3.342983979926655,
      "grad_norm": 1.2277274131774902,
      "learning_rate": 3.286045165026057e-06,
      "loss": 0.9783,
      "step": 17320
    },
    {
      "epoch": 3.3449141092453196,
      "grad_norm": 54.0211181640625,
      "learning_rate": 3.276394518432735e-06,
      "loss": 1.6108,
      "step": 17330
    },
    {
      "epoch": 3.3468442385639836,
      "grad_norm": 17.33617401123047,
      "learning_rate": 3.2667438718394135e-06,
      "loss": 0.6629,
      "step": 17340
    },
    {
      "epoch": 3.348774367882648,
      "grad_norm": 4.98238468170166,
      "learning_rate": 3.257093225246092e-06,
      "loss": 0.7251,
      "step": 17350
    },
    {
      "epoch": 3.3507044972013125,
      "grad_norm": 16.534841537475586,
      "learning_rate": 3.2474425786527703e-06,
      "loss": 0.7231,
      "step": 17360
    },
    {
      "epoch": 3.352634626519977,
      "grad_norm": 0.3349573016166687,
      "learning_rate": 3.2377919320594483e-06,
      "loss": 1.0726,
      "step": 17370
    },
    {
      "epoch": 3.3545647558386413,
      "grad_norm": 39.87957000732422,
      "learning_rate": 3.2281412854661267e-06,
      "loss": 0.5979,
      "step": 17380
    },
    {
      "epoch": 3.3564948851573053,
      "grad_norm": 7.265590667724609,
      "learning_rate": 3.218490638872805e-06,
      "loss": 1.1251,
      "step": 17390
    },
    {
      "epoch": 3.3584250144759697,
      "grad_norm": 4.187770366668701,
      "learning_rate": 3.2088399922794827e-06,
      "loss": 0.8844,
      "step": 17400
    },
    {
      "epoch": 3.360355143794634,
      "grad_norm": 3.2526755332946777,
      "learning_rate": 3.199189345686161e-06,
      "loss": 0.5705,
      "step": 17410
    },
    {
      "epoch": 3.3622852731132986,
      "grad_norm": 2.4680144786834717,
      "learning_rate": 3.1895386990928395e-06,
      "loss": 0.6842,
      "step": 17420
    },
    {
      "epoch": 3.364215402431963,
      "grad_norm": 9.925376892089844,
      "learning_rate": 3.179888052499518e-06,
      "loss": 1.6986,
      "step": 17430
    },
    {
      "epoch": 3.3661455317506275,
      "grad_norm": 0.48789748549461365,
      "learning_rate": 3.170237405906196e-06,
      "loss": 1.3873,
      "step": 17440
    },
    {
      "epoch": 3.3680756610692915,
      "grad_norm": 2.215561628341675,
      "learning_rate": 3.1605867593128743e-06,
      "loss": 0.5066,
      "step": 17450
    },
    {
      "epoch": 3.370005790387956,
      "grad_norm": 94.34059143066406,
      "learning_rate": 3.1509361127195527e-06,
      "loss": 1.2111,
      "step": 17460
    },
    {
      "epoch": 3.3719359197066203,
      "grad_norm": 3.0818188190460205,
      "learning_rate": 3.141285466126231e-06,
      "loss": 0.6193,
      "step": 17470
    },
    {
      "epoch": 3.3738660490252848,
      "grad_norm": 7.814475059509277,
      "learning_rate": 3.1316348195329087e-06,
      "loss": 0.9889,
      "step": 17480
    },
    {
      "epoch": 3.375796178343949,
      "grad_norm": 2.8459227085113525,
      "learning_rate": 3.121984172939587e-06,
      "loss": 1.3915,
      "step": 17490
    },
    {
      "epoch": 3.3777263076626136,
      "grad_norm": 17.224971771240234,
      "learning_rate": 3.1123335263462655e-06,
      "loss": 0.9574,
      "step": 17500
    },
    {
      "epoch": 3.3796564369812776,
      "grad_norm": 0.5131667852401733,
      "learning_rate": 3.1026828797529435e-06,
      "loss": 0.8439,
      "step": 17510
    },
    {
      "epoch": 3.381586566299942,
      "grad_norm": 0.1672687828540802,
      "learning_rate": 3.093032233159622e-06,
      "loss": 0.8114,
      "step": 17520
    },
    {
      "epoch": 3.3835166956186065,
      "grad_norm": 0.11602452397346497,
      "learning_rate": 3.0833815865663003e-06,
      "loss": 0.56,
      "step": 17530
    },
    {
      "epoch": 3.385446824937271,
      "grad_norm": 0.24980893731117249,
      "learning_rate": 3.0737309399729787e-06,
      "loss": 0.5634,
      "step": 17540
    },
    {
      "epoch": 3.3873769542559353,
      "grad_norm": 0.18496422469615936,
      "learning_rate": 3.0640802933796567e-06,
      "loss": 1.1029,
      "step": 17550
    },
    {
      "epoch": 3.3893070835745993,
      "grad_norm": 3.3999738693237305,
      "learning_rate": 3.054429646786335e-06,
      "loss": 0.8197,
      "step": 17560
    },
    {
      "epoch": 3.3912372128932637,
      "grad_norm": 0.3007871210575104,
      "learning_rate": 3.0447790001930135e-06,
      "loss": 0.4979,
      "step": 17570
    },
    {
      "epoch": 3.393167342211928,
      "grad_norm": 1.1030638217926025,
      "learning_rate": 3.035128353599691e-06,
      "loss": 0.6453,
      "step": 17580
    },
    {
      "epoch": 3.3950974715305926,
      "grad_norm": 0.6675356030464172,
      "learning_rate": 3.0254777070063695e-06,
      "loss": 1.3985,
      "step": 17590
    },
    {
      "epoch": 3.397027600849257,
      "grad_norm": 0.6767277121543884,
      "learning_rate": 3.015827060413048e-06,
      "loss": 0.4331,
      "step": 17600
    },
    {
      "epoch": 3.398957730167921,
      "grad_norm": 5.728600025177002,
      "learning_rate": 3.0061764138197263e-06,
      "loss": 0.9274,
      "step": 17610
    },
    {
      "epoch": 3.4008878594865855,
      "grad_norm": 0.19679339230060577,
      "learning_rate": 2.9965257672264043e-06,
      "loss": 0.2451,
      "step": 17620
    },
    {
      "epoch": 3.40281798880525,
      "grad_norm": 30.89256477355957,
      "learning_rate": 2.9868751206330827e-06,
      "loss": 1.0688,
      "step": 17630
    },
    {
      "epoch": 3.4047481181239143,
      "grad_norm": 6.129980564117432,
      "learning_rate": 2.977224474039761e-06,
      "loss": 0.7098,
      "step": 17640
    },
    {
      "epoch": 3.4066782474425787,
      "grad_norm": 4.371479034423828,
      "learning_rate": 2.9675738274464395e-06,
      "loss": 0.4101,
      "step": 17650
    },
    {
      "epoch": 3.408608376761243,
      "grad_norm": 8.095247268676758,
      "learning_rate": 2.957923180853117e-06,
      "loss": 0.8946,
      "step": 17660
    },
    {
      "epoch": 3.410538506079907,
      "grad_norm": 7.006259441375732,
      "learning_rate": 2.9482725342597955e-06,
      "loss": 0.3945,
      "step": 17670
    },
    {
      "epoch": 3.4124686353985716,
      "grad_norm": 4.991399765014648,
      "learning_rate": 2.938621887666474e-06,
      "loss": 0.7562,
      "step": 17680
    },
    {
      "epoch": 3.414398764717236,
      "grad_norm": 2.870727777481079,
      "learning_rate": 2.928971241073152e-06,
      "loss": 1.4491,
      "step": 17690
    },
    {
      "epoch": 3.4163288940359005,
      "grad_norm": 101.01264953613281,
      "learning_rate": 2.9193205944798303e-06,
      "loss": 0.9044,
      "step": 17700
    },
    {
      "epoch": 3.418259023354565,
      "grad_norm": 14.620192527770996,
      "learning_rate": 2.9096699478865087e-06,
      "loss": 0.5406,
      "step": 17710
    },
    {
      "epoch": 3.4201891526732293,
      "grad_norm": 30.24992561340332,
      "learning_rate": 2.900019301293187e-06,
      "loss": 0.724,
      "step": 17720
    },
    {
      "epoch": 3.4221192819918933,
      "grad_norm": 2.300724983215332,
      "learning_rate": 2.890368654699865e-06,
      "loss": 1.066,
      "step": 17730
    },
    {
      "epoch": 3.4240494113105577,
      "grad_norm": 0.07395398616790771,
      "learning_rate": 2.8807180081065435e-06,
      "loss": 0.6059,
      "step": 17740
    },
    {
      "epoch": 3.425979540629222,
      "grad_norm": 0.4242907166481018,
      "learning_rate": 2.871067361513222e-06,
      "loss": 0.6703,
      "step": 17750
    },
    {
      "epoch": 3.4279096699478866,
      "grad_norm": 0.4492005407810211,
      "learning_rate": 2.8614167149199003e-06,
      "loss": 0.7929,
      "step": 17760
    },
    {
      "epoch": 3.429839799266551,
      "grad_norm": 13.404031753540039,
      "learning_rate": 2.851766068326578e-06,
      "loss": 1.8728,
      "step": 17770
    },
    {
      "epoch": 3.431769928585215,
      "grad_norm": 5.518136978149414,
      "learning_rate": 2.8421154217332563e-06,
      "loss": 1.0203,
      "step": 17780
    },
    {
      "epoch": 3.4337000579038794,
      "grad_norm": 10.31529426574707,
      "learning_rate": 2.8324647751399347e-06,
      "loss": 0.6486,
      "step": 17790
    },
    {
      "epoch": 3.435630187222544,
      "grad_norm": 6.26971960067749,
      "learning_rate": 2.8228141285466127e-06,
      "loss": 0.6327,
      "step": 17800
    },
    {
      "epoch": 3.4375603165412083,
      "grad_norm": 2.4822287559509277,
      "learning_rate": 2.813163481953291e-06,
      "loss": 0.8994,
      "step": 17810
    },
    {
      "epoch": 3.4394904458598727,
      "grad_norm": 1.3160072565078735,
      "learning_rate": 2.8035128353599695e-06,
      "loss": 0.1558,
      "step": 17820
    },
    {
      "epoch": 3.4414205751785367,
      "grad_norm": 18.38629722595215,
      "learning_rate": 2.793862188766648e-06,
      "loss": 0.661,
      "step": 17830
    },
    {
      "epoch": 3.443350704497201,
      "grad_norm": 61.3969841003418,
      "learning_rate": 2.7842115421733255e-06,
      "loss": 1.14,
      "step": 17840
    },
    {
      "epoch": 3.4452808338158656,
      "grad_norm": 6.4403204917907715,
      "learning_rate": 2.774560895580004e-06,
      "loss": 0.4042,
      "step": 17850
    },
    {
      "epoch": 3.44721096313453,
      "grad_norm": 19.525190353393555,
      "learning_rate": 2.7649102489866823e-06,
      "loss": 0.5027,
      "step": 17860
    },
    {
      "epoch": 3.4491410924531944,
      "grad_norm": 0.819718062877655,
      "learning_rate": 2.7552596023933603e-06,
      "loss": 0.4294,
      "step": 17870
    },
    {
      "epoch": 3.451071221771859,
      "grad_norm": 14.38803768157959,
      "learning_rate": 2.7456089558000387e-06,
      "loss": 0.9723,
      "step": 17880
    },
    {
      "epoch": 3.4530013510905233,
      "grad_norm": 67.78121948242188,
      "learning_rate": 2.735958309206717e-06,
      "loss": 0.8318,
      "step": 17890
    },
    {
      "epoch": 3.4549314804091873,
      "grad_norm": 9.26087474822998,
      "learning_rate": 2.7263076626133955e-06,
      "loss": 0.6856,
      "step": 17900
    },
    {
      "epoch": 3.4568616097278517,
      "grad_norm": 2.7427496910095215,
      "learning_rate": 2.7166570160200735e-06,
      "loss": 0.2661,
      "step": 17910
    },
    {
      "epoch": 3.458791739046516,
      "grad_norm": 8.75226879119873,
      "learning_rate": 2.707006369426752e-06,
      "loss": 0.57,
      "step": 17920
    },
    {
      "epoch": 3.4607218683651806,
      "grad_norm": 0.6206011772155762,
      "learning_rate": 2.6973557228334303e-06,
      "loss": 0.535,
      "step": 17930
    },
    {
      "epoch": 3.462651997683845,
      "grad_norm": 6.987580299377441,
      "learning_rate": 2.6877050762401087e-06,
      "loss": 0.2356,
      "step": 17940
    },
    {
      "epoch": 3.464582127002509,
      "grad_norm": 2.663546323776245,
      "learning_rate": 2.6780544296467863e-06,
      "loss": 0.5245,
      "step": 17950
    },
    {
      "epoch": 3.4665122563211734,
      "grad_norm": 6.218016624450684,
      "learning_rate": 2.6684037830534647e-06,
      "loss": 0.461,
      "step": 17960
    },
    {
      "epoch": 3.468442385639838,
      "grad_norm": 24.286544799804688,
      "learning_rate": 2.658753136460143e-06,
      "loss": 0.6809,
      "step": 17970
    },
    {
      "epoch": 3.4703725149585023,
      "grad_norm": 2.2878692150115967,
      "learning_rate": 2.649102489866821e-06,
      "loss": 0.3173,
      "step": 17980
    },
    {
      "epoch": 3.4723026442771667,
      "grad_norm": 4.7396087646484375,
      "learning_rate": 2.6394518432734995e-06,
      "loss": 0.497,
      "step": 17990
    },
    {
      "epoch": 3.4742327735958307,
      "grad_norm": 26.63905906677246,
      "learning_rate": 2.629801196680178e-06,
      "loss": 1.0516,
      "step": 18000
    },
    {
      "epoch": 3.476162902914495,
      "grad_norm": 3.975194215774536,
      "learning_rate": 2.6201505500868563e-06,
      "loss": 0.8874,
      "step": 18010
    },
    {
      "epoch": 3.4780930322331596,
      "grad_norm": 14.124027252197266,
      "learning_rate": 2.610499903493534e-06,
      "loss": 0.5997,
      "step": 18020
    },
    {
      "epoch": 3.480023161551824,
      "grad_norm": 1.2513426542282104,
      "learning_rate": 2.6008492569002123e-06,
      "loss": 0.3526,
      "step": 18030
    },
    {
      "epoch": 3.4819532908704884,
      "grad_norm": 15.38004207611084,
      "learning_rate": 2.5911986103068907e-06,
      "loss": 0.8526,
      "step": 18040
    },
    {
      "epoch": 3.4838834201891524,
      "grad_norm": 39.23562240600586,
      "learning_rate": 2.5815479637135687e-06,
      "loss": 1.1522,
      "step": 18050
    },
    {
      "epoch": 3.485813549507817,
      "grad_norm": 0.08167091757059097,
      "learning_rate": 2.571897317120247e-06,
      "loss": 1.1396,
      "step": 18060
    },
    {
      "epoch": 3.4877436788264813,
      "grad_norm": 7.974835395812988,
      "learning_rate": 2.5622466705269255e-06,
      "loss": 0.5925,
      "step": 18070
    },
    {
      "epoch": 3.4896738081451457,
      "grad_norm": 0.5134607553482056,
      "learning_rate": 2.552596023933604e-06,
      "loss": 0.9556,
      "step": 18080
    },
    {
      "epoch": 3.49160393746381,
      "grad_norm": 6.6275858879089355,
      "learning_rate": 2.542945377340282e-06,
      "loss": 1.4286,
      "step": 18090
    },
    {
      "epoch": 3.4935340667824746,
      "grad_norm": 3.0613045692443848,
      "learning_rate": 2.5332947307469603e-06,
      "loss": 0.2756,
      "step": 18100
    },
    {
      "epoch": 3.495464196101139,
      "grad_norm": 81.80423736572266,
      "learning_rate": 2.5236440841536388e-06,
      "loss": 0.4881,
      "step": 18110
    },
    {
      "epoch": 3.497394325419803,
      "grad_norm": 0.8065200448036194,
      "learning_rate": 2.513993437560317e-06,
      "loss": 0.8458,
      "step": 18120
    },
    {
      "epoch": 3.4993244547384674,
      "grad_norm": 4.7186174392700195,
      "learning_rate": 2.5043427909669947e-06,
      "loss": 0.6982,
      "step": 18130
    },
    {
      "epoch": 3.501254584057132,
      "grad_norm": 0.40476205945014954,
      "learning_rate": 2.494692144373673e-06,
      "loss": 0.6098,
      "step": 18140
    },
    {
      "epoch": 3.5031847133757963,
      "grad_norm": 1.258367657661438,
      "learning_rate": 2.4850414977803515e-06,
      "loss": 0.9381,
      "step": 18150
    },
    {
      "epoch": 3.5051148426944607,
      "grad_norm": 65.16006469726562,
      "learning_rate": 2.47539085118703e-06,
      "loss": 1.1446,
      "step": 18160
    },
    {
      "epoch": 3.5070449720131247,
      "grad_norm": 13.427918434143066,
      "learning_rate": 2.465740204593708e-06,
      "loss": 1.5418,
      "step": 18170
    },
    {
      "epoch": 3.508975101331789,
      "grad_norm": 99.89350891113281,
      "learning_rate": 2.4560895580003864e-06,
      "loss": 1.3647,
      "step": 18180
    },
    {
      "epoch": 3.5109052306504536,
      "grad_norm": 0.7819852232933044,
      "learning_rate": 2.4464389114070643e-06,
      "loss": 1.5189,
      "step": 18190
    },
    {
      "epoch": 3.512835359969118,
      "grad_norm": 0.2569308876991272,
      "learning_rate": 2.4367882648137428e-06,
      "loss": 0.9379,
      "step": 18200
    },
    {
      "epoch": 3.5147654892877824,
      "grad_norm": 26.195384979248047,
      "learning_rate": 2.4271376182204207e-06,
      "loss": 1.3186,
      "step": 18210
    },
    {
      "epoch": 3.5166956186064464,
      "grad_norm": 7.356010913848877,
      "learning_rate": 2.417486971627099e-06,
      "loss": 0.4753,
      "step": 18220
    },
    {
      "epoch": 3.518625747925111,
      "grad_norm": 5.848624229431152,
      "learning_rate": 2.4078363250337776e-06,
      "loss": 1.0471,
      "step": 18230
    },
    {
      "epoch": 3.5205558772437753,
      "grad_norm": 2.6416807174682617,
      "learning_rate": 2.3981856784404555e-06,
      "loss": 1.2429,
      "step": 18240
    },
    {
      "epoch": 3.5224860065624397,
      "grad_norm": 1.7088228464126587,
      "learning_rate": 2.388535031847134e-06,
      "loss": 0.6815,
      "step": 18250
    },
    {
      "epoch": 3.524416135881104,
      "grad_norm": 7.9195404052734375,
      "learning_rate": 2.378884385253812e-06,
      "loss": 0.7471,
      "step": 18260
    },
    {
      "epoch": 3.526346265199768,
      "grad_norm": 1.0042333602905273,
      "learning_rate": 2.3692337386604904e-06,
      "loss": 0.5023,
      "step": 18270
    },
    {
      "epoch": 3.528276394518433,
      "grad_norm": 5.453758239746094,
      "learning_rate": 2.3595830920671688e-06,
      "loss": 0.377,
      "step": 18280
    },
    {
      "epoch": 3.530206523837097,
      "grad_norm": 29.393516540527344,
      "learning_rate": 2.349932445473847e-06,
      "loss": 1.2853,
      "step": 18290
    },
    {
      "epoch": 3.5321366531557614,
      "grad_norm": 111.2380599975586,
      "learning_rate": 2.340281798880525e-06,
      "loss": 1.2046,
      "step": 18300
    },
    {
      "epoch": 3.534066782474426,
      "grad_norm": 0.14248262345790863,
      "learning_rate": 2.3306311522872036e-06,
      "loss": 0.7005,
      "step": 18310
    },
    {
      "epoch": 3.5359969117930903,
      "grad_norm": 53.845558166503906,
      "learning_rate": 2.3209805056938816e-06,
      "loss": 1.1254,
      "step": 18320
    },
    {
      "epoch": 3.5379270411117547,
      "grad_norm": 79.98316192626953,
      "learning_rate": 2.31132985910056e-06,
      "loss": 0.6498,
      "step": 18330
    },
    {
      "epoch": 3.5398571704304187,
      "grad_norm": 12.662714004516602,
      "learning_rate": 2.3016792125072384e-06,
      "loss": 0.6532,
      "step": 18340
    },
    {
      "epoch": 3.541787299749083,
      "grad_norm": 3.871461868286133,
      "learning_rate": 2.2920285659139164e-06,
      "loss": 0.4872,
      "step": 18350
    },
    {
      "epoch": 3.5437174290677476,
      "grad_norm": 4.713068008422852,
      "learning_rate": 2.2823779193205948e-06,
      "loss": 1.0035,
      "step": 18360
    },
    {
      "epoch": 3.545647558386412,
      "grad_norm": 2.562101364135742,
      "learning_rate": 2.2727272727272728e-06,
      "loss": 0.8789,
      "step": 18370
    },
    {
      "epoch": 3.5475776877050764,
      "grad_norm": 64.32212829589844,
      "learning_rate": 2.263076626133951e-06,
      "loss": 0.7424,
      "step": 18380
    },
    {
      "epoch": 3.5495078170237404,
      "grad_norm": 71.74766540527344,
      "learning_rate": 2.253425979540629e-06,
      "loss": 1.16,
      "step": 18390
    },
    {
      "epoch": 3.551437946342405,
      "grad_norm": 0.22438646852970123,
      "learning_rate": 2.2437753329473076e-06,
      "loss": 0.3294,
      "step": 18400
    },
    {
      "epoch": 3.5533680756610693,
      "grad_norm": 1.324407935142517,
      "learning_rate": 2.234124686353986e-06,
      "loss": 1.057,
      "step": 18410
    },
    {
      "epoch": 3.5552982049797337,
      "grad_norm": 7.039165496826172,
      "learning_rate": 2.2244740397606644e-06,
      "loss": 0.8473,
      "step": 18420
    },
    {
      "epoch": 3.557228334298398,
      "grad_norm": 1.5136966705322266,
      "learning_rate": 2.2148233931673424e-06,
      "loss": 0.4372,
      "step": 18430
    },
    {
      "epoch": 3.559158463617062,
      "grad_norm": 0.16307906806468964,
      "learning_rate": 2.2051727465740204e-06,
      "loss": 1.1451,
      "step": 18440
    },
    {
      "epoch": 3.5610885929357265,
      "grad_norm": 8.899567604064941,
      "learning_rate": 2.1955220999806988e-06,
      "loss": 0.6947,
      "step": 18450
    },
    {
      "epoch": 3.563018722254391,
      "grad_norm": 16.648466110229492,
      "learning_rate": 2.185871453387377e-06,
      "loss": 1.1795,
      "step": 18460
    },
    {
      "epoch": 3.5649488515730554,
      "grad_norm": 0.6386141180992126,
      "learning_rate": 2.1762208067940556e-06,
      "loss": 0.8914,
      "step": 18470
    },
    {
      "epoch": 3.56687898089172,
      "grad_norm": 67.76244354248047,
      "learning_rate": 2.1665701602007336e-06,
      "loss": 0.4248,
      "step": 18480
    },
    {
      "epoch": 3.568809110210384,
      "grad_norm": 5.787372589111328,
      "learning_rate": 2.156919513607412e-06,
      "loss": 0.9438,
      "step": 18490
    },
    {
      "epoch": 3.5707392395290487,
      "grad_norm": 4.3274102210998535,
      "learning_rate": 2.14726886701409e-06,
      "loss": 0.5184,
      "step": 18500
    },
    {
      "epoch": 3.5726693688477127,
      "grad_norm": 2.956367254257202,
      "learning_rate": 2.1376182204207684e-06,
      "loss": 1.209,
      "step": 18510
    },
    {
      "epoch": 3.574599498166377,
      "grad_norm": 75.37281799316406,
      "learning_rate": 2.127967573827447e-06,
      "loss": 0.6598,
      "step": 18520
    },
    {
      "epoch": 3.5765296274850416,
      "grad_norm": 0.19547684490680695,
      "learning_rate": 2.1183169272341248e-06,
      "loss": 0.5551,
      "step": 18530
    },
    {
      "epoch": 3.578459756803706,
      "grad_norm": 22.841951370239258,
      "learning_rate": 2.108666280640803e-06,
      "loss": 1.0323,
      "step": 18540
    },
    {
      "epoch": 3.5803898861223704,
      "grad_norm": 13.228389739990234,
      "learning_rate": 2.099015634047481e-06,
      "loss": 0.8148,
      "step": 18550
    },
    {
      "epoch": 3.5823200154410344,
      "grad_norm": 9.520888328552246,
      "learning_rate": 2.0893649874541596e-06,
      "loss": 1.2211,
      "step": 18560
    },
    {
      "epoch": 3.584250144759699,
      "grad_norm": 0.14653410017490387,
      "learning_rate": 2.079714340860838e-06,
      "loss": 0.5817,
      "step": 18570
    },
    {
      "epoch": 3.5861802740783633,
      "grad_norm": 3.7747855186462402,
      "learning_rate": 2.070063694267516e-06,
      "loss": 1.5245,
      "step": 18580
    },
    {
      "epoch": 3.5881104033970277,
      "grad_norm": 17.1690616607666,
      "learning_rate": 2.0604130476741944e-06,
      "loss": 0.6949,
      "step": 18590
    },
    {
      "epoch": 3.590040532715692,
      "grad_norm": 1.7178709506988525,
      "learning_rate": 2.050762401080873e-06,
      "loss": 0.9178,
      "step": 18600
    },
    {
      "epoch": 3.591970662034356,
      "grad_norm": 31.383180618286133,
      "learning_rate": 2.041111754487551e-06,
      "loss": 0.4332,
      "step": 18610
    },
    {
      "epoch": 3.5939007913530205,
      "grad_norm": 17.36649513244629,
      "learning_rate": 2.0314611078942288e-06,
      "loss": 0.5432,
      "step": 18620
    },
    {
      "epoch": 3.595830920671685,
      "grad_norm": 10.929035186767578,
      "learning_rate": 2.021810461300907e-06,
      "loss": 0.5321,
      "step": 18630
    },
    {
      "epoch": 3.5977610499903494,
      "grad_norm": 30.050085067749023,
      "learning_rate": 2.0121598147075856e-06,
      "loss": 0.8361,
      "step": 18640
    },
    {
      "epoch": 3.599691179309014,
      "grad_norm": 0.16379287838935852,
      "learning_rate": 2.002509168114264e-06,
      "loss": 1.1586,
      "step": 18650
    },
    {
      "epoch": 3.601621308627678,
      "grad_norm": 1.9609066247940063,
      "learning_rate": 1.992858521520942e-06,
      "loss": 0.4635,
      "step": 18660
    },
    {
      "epoch": 3.6035514379463423,
      "grad_norm": 1.7486664056777954,
      "learning_rate": 1.9832078749276204e-06,
      "loss": 0.4141,
      "step": 18670
    },
    {
      "epoch": 3.6054815672650067,
      "grad_norm": 3.1096789836883545,
      "learning_rate": 1.9735572283342984e-06,
      "loss": 0.3348,
      "step": 18680
    },
    {
      "epoch": 3.607411696583671,
      "grad_norm": 4.554452896118164,
      "learning_rate": 1.963906581740977e-06,
      "loss": 0.9572,
      "step": 18690
    },
    {
      "epoch": 3.6093418259023355,
      "grad_norm": 1.0054961442947388,
      "learning_rate": 1.954255935147655e-06,
      "loss": 0.5537,
      "step": 18700
    },
    {
      "epoch": 3.6112719552209995,
      "grad_norm": 1.1911628246307373,
      "learning_rate": 1.944605288554333e-06,
      "loss": 0.4388,
      "step": 18710
    },
    {
      "epoch": 3.6132020845396644,
      "grad_norm": 4.772181034088135,
      "learning_rate": 1.9349546419610116e-06,
      "loss": 0.1425,
      "step": 18720
    },
    {
      "epoch": 3.6151322138583284,
      "grad_norm": 36.46574020385742,
      "learning_rate": 1.9253039953676896e-06,
      "loss": 1.1842,
      "step": 18730
    },
    {
      "epoch": 3.617062343176993,
      "grad_norm": 4.859887599945068,
      "learning_rate": 1.915653348774368e-06,
      "loss": 0.8273,
      "step": 18740
    },
    {
      "epoch": 3.6189924724956573,
      "grad_norm": 1.617749571800232,
      "learning_rate": 1.9060027021810462e-06,
      "loss": 1.4353,
      "step": 18750
    },
    {
      "epoch": 3.6209226018143217,
      "grad_norm": 10.090197563171387,
      "learning_rate": 1.8963520555877246e-06,
      "loss": 0.4076,
      "step": 18760
    },
    {
      "epoch": 3.622852731132986,
      "grad_norm": 3.132392168045044,
      "learning_rate": 1.8867014089944028e-06,
      "loss": 0.6236,
      "step": 18770
    },
    {
      "epoch": 3.62478286045165,
      "grad_norm": 0.3820561170578003,
      "learning_rate": 1.8770507624010812e-06,
      "loss": 1.0951,
      "step": 18780
    },
    {
      "epoch": 3.6267129897703145,
      "grad_norm": 3.1955647468566895,
      "learning_rate": 1.8674001158077592e-06,
      "loss": 1.6313,
      "step": 18790
    },
    {
      "epoch": 3.628643119088979,
      "grad_norm": 13.996153831481934,
      "learning_rate": 1.8577494692144374e-06,
      "loss": 0.7315,
      "step": 18800
    },
    {
      "epoch": 3.6305732484076434,
      "grad_norm": 0.3214467763900757,
      "learning_rate": 1.8480988226211158e-06,
      "loss": 1.2918,
      "step": 18810
    },
    {
      "epoch": 3.632503377726308,
      "grad_norm": 80.47469329833984,
      "learning_rate": 1.838448176027794e-06,
      "loss": 0.6374,
      "step": 18820
    },
    {
      "epoch": 3.634433507044972,
      "grad_norm": 134.99502563476562,
      "learning_rate": 1.8287975294344722e-06,
      "loss": 0.5604,
      "step": 18830
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 133.9414825439453,
      "learning_rate": 1.8191468828411504e-06,
      "loss": 1.1352,
      "step": 18840
    },
    {
      "epoch": 3.6382937656823007,
      "grad_norm": 0.8356935977935791,
      "learning_rate": 1.8094962362478288e-06,
      "loss": 0.7987,
      "step": 18850
    },
    {
      "epoch": 3.640223895000965,
      "grad_norm": 0.25849318504333496,
      "learning_rate": 1.799845589654507e-06,
      "loss": 0.3143,
      "step": 18860
    },
    {
      "epoch": 3.6421540243196295,
      "grad_norm": 2.9802749156951904,
      "learning_rate": 1.7901949430611854e-06,
      "loss": 0.453,
      "step": 18870
    },
    {
      "epoch": 3.6440841536382935,
      "grad_norm": 0.3722412884235382,
      "learning_rate": 1.7805442964678634e-06,
      "loss": 0.5011,
      "step": 18880
    },
    {
      "epoch": 3.646014282956958,
      "grad_norm": 4.864349365234375,
      "learning_rate": 1.7708936498745418e-06,
      "loss": 0.3809,
      "step": 18890
    },
    {
      "epoch": 3.6479444122756224,
      "grad_norm": 3.866041421890259,
      "learning_rate": 1.76124300328122e-06,
      "loss": 1.2144,
      "step": 18900
    },
    {
      "epoch": 3.649874541594287,
      "grad_norm": 11.787447929382324,
      "learning_rate": 1.7515923566878982e-06,
      "loss": 0.2413,
      "step": 18910
    },
    {
      "epoch": 3.6518046709129512,
      "grad_norm": 36.281429290771484,
      "learning_rate": 1.7419417100945764e-06,
      "loss": 0.8411,
      "step": 18920
    },
    {
      "epoch": 3.6537348002316157,
      "grad_norm": 57.24393081665039,
      "learning_rate": 1.7322910635012546e-06,
      "loss": 0.8555,
      "step": 18930
    },
    {
      "epoch": 3.65566492955028,
      "grad_norm": 6.12138557434082,
      "learning_rate": 1.722640416907933e-06,
      "loss": 0.5326,
      "step": 18940
    },
    {
      "epoch": 3.657595058868944,
      "grad_norm": 11.798190116882324,
      "learning_rate": 1.7129897703146112e-06,
      "loss": 0.4651,
      "step": 18950
    },
    {
      "epoch": 3.6595251881876085,
      "grad_norm": 2.6475605964660645,
      "learning_rate": 1.7033391237212896e-06,
      "loss": 0.4216,
      "step": 18960
    },
    {
      "epoch": 3.661455317506273,
      "grad_norm": 30.247371673583984,
      "learning_rate": 1.6936884771279676e-06,
      "loss": 0.639,
      "step": 18970
    },
    {
      "epoch": 3.6633854468249374,
      "grad_norm": 0.7254568338394165,
      "learning_rate": 1.684037830534646e-06,
      "loss": 0.8006,
      "step": 18980
    },
    {
      "epoch": 3.665315576143602,
      "grad_norm": 10.861536979675293,
      "learning_rate": 1.6743871839413242e-06,
      "loss": 1.0626,
      "step": 18990
    },
    {
      "epoch": 3.667245705462266,
      "grad_norm": 60.97142028808594,
      "learning_rate": 1.6647365373480024e-06,
      "loss": 1.0575,
      "step": 19000
    },
    {
      "epoch": 3.6691758347809302,
      "grad_norm": 0.5114267468452454,
      "learning_rate": 1.6550858907546808e-06,
      "loss": 0.8187,
      "step": 19010
    },
    {
      "epoch": 3.6711059640995947,
      "grad_norm": 1.83943772315979,
      "learning_rate": 1.6454352441613588e-06,
      "loss": 0.4612,
      "step": 19020
    },
    {
      "epoch": 3.673036093418259,
      "grad_norm": 10.56955337524414,
      "learning_rate": 1.6357845975680372e-06,
      "loss": 0.7182,
      "step": 19030
    },
    {
      "epoch": 3.6749662227369235,
      "grad_norm": 34.680667877197266,
      "learning_rate": 1.6261339509747154e-06,
      "loss": 0.5496,
      "step": 19040
    },
    {
      "epoch": 3.6768963520555875,
      "grad_norm": 0.23957403004169464,
      "learning_rate": 1.6164833043813938e-06,
      "loss": 0.6641,
      "step": 19050
    },
    {
      "epoch": 3.678826481374252,
      "grad_norm": 0.7228547930717468,
      "learning_rate": 1.6068326577880718e-06,
      "loss": 0.85,
      "step": 19060
    },
    {
      "epoch": 3.6807566106929164,
      "grad_norm": 8.167375564575195,
      "learning_rate": 1.5971820111947502e-06,
      "loss": 0.7028,
      "step": 19070
    },
    {
      "epoch": 3.682686740011581,
      "grad_norm": 10.046494483947754,
      "learning_rate": 1.5875313646014284e-06,
      "loss": 0.5609,
      "step": 19080
    },
    {
      "epoch": 3.6846168693302452,
      "grad_norm": 63.60027313232422,
      "learning_rate": 1.5778807180081066e-06,
      "loss": 1.3019,
      "step": 19090
    },
    {
      "epoch": 3.6865469986489092,
      "grad_norm": 5.179193496704102,
      "learning_rate": 1.568230071414785e-06,
      "loss": 0.5682,
      "step": 19100
    },
    {
      "epoch": 3.688477127967574,
      "grad_norm": 1.5595979690551758,
      "learning_rate": 1.558579424821463e-06,
      "loss": 0.6057,
      "step": 19110
    },
    {
      "epoch": 3.690407257286238,
      "grad_norm": 15.602588653564453,
      "learning_rate": 1.5489287782281414e-06,
      "loss": 0.3305,
      "step": 19120
    },
    {
      "epoch": 3.6923373866049025,
      "grad_norm": 1.3495516777038574,
      "learning_rate": 1.5392781316348196e-06,
      "loss": 1.4927,
      "step": 19130
    },
    {
      "epoch": 3.694267515923567,
      "grad_norm": 5.447523593902588,
      "learning_rate": 1.529627485041498e-06,
      "loss": 0.5863,
      "step": 19140
    },
    {
      "epoch": 3.6961976452422314,
      "grad_norm": 0.21044868230819702,
      "learning_rate": 1.519976838448176e-06,
      "loss": 0.9474,
      "step": 19150
    },
    {
      "epoch": 3.698127774560896,
      "grad_norm": 6.261119842529297,
      "learning_rate": 1.5103261918548544e-06,
      "loss": 0.658,
      "step": 19160
    },
    {
      "epoch": 3.70005790387956,
      "grad_norm": 70.4229965209961,
      "learning_rate": 1.5006755452615326e-06,
      "loss": 0.8618,
      "step": 19170
    },
    {
      "epoch": 3.7019880331982242,
      "grad_norm": 0.25311192870140076,
      "learning_rate": 1.4910248986682108e-06,
      "loss": 0.8617,
      "step": 19180
    },
    {
      "epoch": 3.7039181625168887,
      "grad_norm": 10.98318099975586,
      "learning_rate": 1.4813742520748893e-06,
      "loss": 0.5946,
      "step": 19190
    },
    {
      "epoch": 3.705848291835553,
      "grad_norm": 78.79969787597656,
      "learning_rate": 1.4717236054815672e-06,
      "loss": 0.929,
      "step": 19200
    },
    {
      "epoch": 3.7077784211542175,
      "grad_norm": 7.92359733581543,
      "learning_rate": 1.4620729588882457e-06,
      "loss": 0.6778,
      "step": 19210
    },
    {
      "epoch": 3.7097085504728815,
      "grad_norm": 0.2733045220375061,
      "learning_rate": 1.4524223122949238e-06,
      "loss": 0.8592,
      "step": 19220
    },
    {
      "epoch": 3.711638679791546,
      "grad_norm": 3.1427981853485107,
      "learning_rate": 1.4427716657016023e-06,
      "loss": 1.0165,
      "step": 19230
    },
    {
      "epoch": 3.7135688091102104,
      "grad_norm": 7.9647674560546875,
      "learning_rate": 1.4331210191082802e-06,
      "loss": 1.3593,
      "step": 19240
    },
    {
      "epoch": 3.715498938428875,
      "grad_norm": 5.372964382171631,
      "learning_rate": 1.4234703725149587e-06,
      "loss": 0.3732,
      "step": 19250
    },
    {
      "epoch": 3.7174290677475392,
      "grad_norm": 0.7838075160980225,
      "learning_rate": 1.4138197259216369e-06,
      "loss": 1.0256,
      "step": 19260
    },
    {
      "epoch": 3.719359197066203,
      "grad_norm": 3.0450046062469482,
      "learning_rate": 1.404169079328315e-06,
      "loss": 1.096,
      "step": 19270
    },
    {
      "epoch": 3.7212893263848676,
      "grad_norm": 24.23821258544922,
      "learning_rate": 1.3945184327349935e-06,
      "loss": 0.6883,
      "step": 19280
    },
    {
      "epoch": 3.723219455703532,
      "grad_norm": 87.91792297363281,
      "learning_rate": 1.3848677861416714e-06,
      "loss": 1.047,
      "step": 19290
    },
    {
      "epoch": 3.7251495850221965,
      "grad_norm": 17.86789321899414,
      "learning_rate": 1.3752171395483499e-06,
      "loss": 0.5133,
      "step": 19300
    },
    {
      "epoch": 3.727079714340861,
      "grad_norm": 10.952488899230957,
      "learning_rate": 1.365566492955028e-06,
      "loss": 0.4331,
      "step": 19310
    },
    {
      "epoch": 3.729009843659525,
      "grad_norm": 49.020572662353516,
      "learning_rate": 1.3559158463617065e-06,
      "loss": 0.9025,
      "step": 19320
    },
    {
      "epoch": 3.73093997297819,
      "grad_norm": 74.61106872558594,
      "learning_rate": 1.3462651997683845e-06,
      "loss": 0.4832,
      "step": 19330
    },
    {
      "epoch": 3.732870102296854,
      "grad_norm": 6.5517354011535645,
      "learning_rate": 1.3366145531750629e-06,
      "loss": 1.0046,
      "step": 19340
    },
    {
      "epoch": 3.734800231615518,
      "grad_norm": 10.76579475402832,
      "learning_rate": 1.326963906581741e-06,
      "loss": 1.3194,
      "step": 19350
    },
    {
      "epoch": 3.7367303609341826,
      "grad_norm": 2.1535000801086426,
      "learning_rate": 1.3173132599884195e-06,
      "loss": 0.6865,
      "step": 19360
    },
    {
      "epoch": 3.738660490252847,
      "grad_norm": 11.158509254455566,
      "learning_rate": 1.3076626133950977e-06,
      "loss": 1.6066,
      "step": 19370
    },
    {
      "epoch": 3.7405906195715115,
      "grad_norm": 309.971923828125,
      "learning_rate": 1.2980119668017757e-06,
      "loss": 0.5921,
      "step": 19380
    },
    {
      "epoch": 3.7425207488901755,
      "grad_norm": 3.6600687503814697,
      "learning_rate": 1.288361320208454e-06,
      "loss": 0.5964,
      "step": 19390
    },
    {
      "epoch": 3.74445087820884,
      "grad_norm": 41.96127700805664,
      "learning_rate": 1.2787106736151323e-06,
      "loss": 0.6843,
      "step": 19400
    },
    {
      "epoch": 3.7463810075275044,
      "grad_norm": 0.2813389003276825,
      "learning_rate": 1.2690600270218107e-06,
      "loss": 0.4657,
      "step": 19410
    },
    {
      "epoch": 3.748311136846169,
      "grad_norm": 1.5770714282989502,
      "learning_rate": 1.2594093804284887e-06,
      "loss": 0.8794,
      "step": 19420
    },
    {
      "epoch": 3.750241266164833,
      "grad_norm": 13.204646110534668,
      "learning_rate": 1.249758733835167e-06,
      "loss": 1.067,
      "step": 19430
    },
    {
      "epoch": 3.752171395483497,
      "grad_norm": 129.13504028320312,
      "learning_rate": 1.2401080872418453e-06,
      "loss": 1.3731,
      "step": 19440
    },
    {
      "epoch": 3.7541015248021616,
      "grad_norm": 7.21926736831665,
      "learning_rate": 1.2304574406485235e-06,
      "loss": 0.9167,
      "step": 19450
    },
    {
      "epoch": 3.756031654120826,
      "grad_norm": 1.6739696264266968,
      "learning_rate": 1.2208067940552019e-06,
      "loss": 0.5723,
      "step": 19460
    },
    {
      "epoch": 3.7579617834394905,
      "grad_norm": 0.804961085319519,
      "learning_rate": 1.21115614746188e-06,
      "loss": 0.6447,
      "step": 19470
    },
    {
      "epoch": 3.759891912758155,
      "grad_norm": 1.7205331325531006,
      "learning_rate": 1.2015055008685583e-06,
      "loss": 1.3646,
      "step": 19480
    },
    {
      "epoch": 3.761822042076819,
      "grad_norm": 6.494638919830322,
      "learning_rate": 1.1918548542752367e-06,
      "loss": 1.0007,
      "step": 19490
    },
    {
      "epoch": 3.7637521713954833,
      "grad_norm": 22.83042335510254,
      "learning_rate": 1.1822042076819149e-06,
      "loss": 0.7125,
      "step": 19500
    },
    {
      "epoch": 3.765682300714148,
      "grad_norm": 0.09601504355669022,
      "learning_rate": 1.1725535610885929e-06,
      "loss": 0.8161,
      "step": 19510
    },
    {
      "epoch": 3.767612430032812,
      "grad_norm": 11.41783618927002,
      "learning_rate": 1.1629029144952713e-06,
      "loss": 1.2974,
      "step": 19520
    },
    {
      "epoch": 3.7695425593514766,
      "grad_norm": 0.5310224890708923,
      "learning_rate": 1.1532522679019495e-06,
      "loss": 0.8881,
      "step": 19530
    },
    {
      "epoch": 3.7714726886701406,
      "grad_norm": 1.623948097229004,
      "learning_rate": 1.1436016213086277e-06,
      "loss": 2.0744,
      "step": 19540
    },
    {
      "epoch": 3.7734028179888055,
      "grad_norm": 59.754940032958984,
      "learning_rate": 1.133950974715306e-06,
      "loss": 0.4949,
      "step": 19550
    },
    {
      "epoch": 3.7753329473074695,
      "grad_norm": 11.25915241241455,
      "learning_rate": 1.1243003281219843e-06,
      "loss": 1.7375,
      "step": 19560
    },
    {
      "epoch": 3.777263076626134,
      "grad_norm": 5.973437786102295,
      "learning_rate": 1.1146496815286625e-06,
      "loss": 1.0921,
      "step": 19570
    },
    {
      "epoch": 3.7791932059447984,
      "grad_norm": 1.9096558094024658,
      "learning_rate": 1.104999034935341e-06,
      "loss": 1.1648,
      "step": 19580
    },
    {
      "epoch": 3.781123335263463,
      "grad_norm": 0.3811849057674408,
      "learning_rate": 1.095348388342019e-06,
      "loss": 0.3238,
      "step": 19590
    },
    {
      "epoch": 3.783053464582127,
      "grad_norm": 12.65445613861084,
      "learning_rate": 1.0856977417486973e-06,
      "loss": 0.839,
      "step": 19600
    },
    {
      "epoch": 3.784983593900791,
      "grad_norm": 31.10231590270996,
      "learning_rate": 1.0760470951553755e-06,
      "loss": 1.0293,
      "step": 19610
    },
    {
      "epoch": 3.7869137232194556,
      "grad_norm": 11.312286376953125,
      "learning_rate": 1.0663964485620537e-06,
      "loss": 0.675,
      "step": 19620
    },
    {
      "epoch": 3.78884385253812,
      "grad_norm": 69.06166076660156,
      "learning_rate": 1.0567458019687319e-06,
      "loss": 0.8186,
      "step": 19630
    },
    {
      "epoch": 3.7907739818567845,
      "grad_norm": 69.79890441894531,
      "learning_rate": 1.0470951553754103e-06,
      "loss": 0.9698,
      "step": 19640
    },
    {
      "epoch": 3.792704111175449,
      "grad_norm": 1.3790284395217896,
      "learning_rate": 1.0374445087820885e-06,
      "loss": 0.514,
      "step": 19650
    },
    {
      "epoch": 3.794634240494113,
      "grad_norm": 5.2942633628845215,
      "learning_rate": 1.0277938621887667e-06,
      "loss": 1.0482,
      "step": 19660
    },
    {
      "epoch": 3.7965643698127773,
      "grad_norm": 11.066947937011719,
      "learning_rate": 1.018143215595445e-06,
      "loss": 0.5312,
      "step": 19670
    },
    {
      "epoch": 3.7984944991314418,
      "grad_norm": 105.3844223022461,
      "learning_rate": 1.0084925690021233e-06,
      "loss": 1.5959,
      "step": 19680
    },
    {
      "epoch": 3.800424628450106,
      "grad_norm": 17.589984893798828,
      "learning_rate": 9.988419224088015e-07,
      "loss": 0.4259,
      "step": 19690
    },
    {
      "epoch": 3.8023547577687706,
      "grad_norm": 10.793399810791016,
      "learning_rate": 9.891912758154797e-07,
      "loss": 0.4432,
      "step": 19700
    },
    {
      "epoch": 3.8042848870874346,
      "grad_norm": 41.246185302734375,
      "learning_rate": 9.79540629222158e-07,
      "loss": 0.6462,
      "step": 19710
    },
    {
      "epoch": 3.806215016406099,
      "grad_norm": 116.45458984375,
      "learning_rate": 9.69889982628836e-07,
      "loss": 0.9565,
      "step": 19720
    },
    {
      "epoch": 3.8081451457247635,
      "grad_norm": 2.612032651901245,
      "learning_rate": 9.602393360355145e-07,
      "loss": 0.8765,
      "step": 19730
    },
    {
      "epoch": 3.810075275043428,
      "grad_norm": 8.829353332519531,
      "learning_rate": 9.505886894421927e-07,
      "loss": 1.0306,
      "step": 19740
    },
    {
      "epoch": 3.8120054043620923,
      "grad_norm": 1.5347485542297363,
      "learning_rate": 9.40938042848871e-07,
      "loss": 0.6545,
      "step": 19750
    },
    {
      "epoch": 3.8139355336807568,
      "grad_norm": 3.8834803104400635,
      "learning_rate": 9.312873962555492e-07,
      "loss": 0.5269,
      "step": 19760
    },
    {
      "epoch": 3.815865662999421,
      "grad_norm": 97.82020568847656,
      "learning_rate": 9.216367496622275e-07,
      "loss": 0.7749,
      "step": 19770
    },
    {
      "epoch": 3.817795792318085,
      "grad_norm": 4.446467876434326,
      "learning_rate": 9.119861030689057e-07,
      "loss": 1.2224,
      "step": 19780
    },
    {
      "epoch": 3.8197259216367496,
      "grad_norm": 1.210486888885498,
      "learning_rate": 9.023354564755839e-07,
      "loss": 0.4958,
      "step": 19790
    },
    {
      "epoch": 3.821656050955414,
      "grad_norm": 0.8786914348602295,
      "learning_rate": 8.926848098822621e-07,
      "loss": 0.2247,
      "step": 19800
    },
    {
      "epoch": 3.8235861802740785,
      "grad_norm": 6.287317752838135,
      "learning_rate": 8.830341632889404e-07,
      "loss": 0.6603,
      "step": 19810
    },
    {
      "epoch": 3.825516309592743,
      "grad_norm": 3.547084331512451,
      "learning_rate": 8.733835166956187e-07,
      "loss": 0.556,
      "step": 19820
    },
    {
      "epoch": 3.827446438911407,
      "grad_norm": 3.232740879058838,
      "learning_rate": 8.637328701022969e-07,
      "loss": 0.2839,
      "step": 19830
    },
    {
      "epoch": 3.8293765682300713,
      "grad_norm": 148.73065185546875,
      "learning_rate": 8.540822235089752e-07,
      "loss": 1.215,
      "step": 19840
    },
    {
      "epoch": 3.8313066975487358,
      "grad_norm": 20.529773712158203,
      "learning_rate": 8.444315769156534e-07,
      "loss": 0.9506,
      "step": 19850
    },
    {
      "epoch": 3.8332368268674,
      "grad_norm": 8.185235977172852,
      "learning_rate": 8.347809303223317e-07,
      "loss": 0.4443,
      "step": 19860
    },
    {
      "epoch": 3.8351669561860646,
      "grad_norm": 26.64179801940918,
      "learning_rate": 8.251302837290099e-07,
      "loss": 1.1338,
      "step": 19870
    },
    {
      "epoch": 3.8370970855047286,
      "grad_norm": 0.8173359632492065,
      "learning_rate": 8.154796371356881e-07,
      "loss": 0.4912,
      "step": 19880
    },
    {
      "epoch": 3.839027214823393,
      "grad_norm": 0.6329636573791504,
      "learning_rate": 8.058289905423663e-07,
      "loss": 0.9405,
      "step": 19890
    },
    {
      "epoch": 3.8409573441420575,
      "grad_norm": 4.936476707458496,
      "learning_rate": 7.961783439490446e-07,
      "loss": 1.0688,
      "step": 19900
    },
    {
      "epoch": 3.842887473460722,
      "grad_norm": 0.11035311222076416,
      "learning_rate": 7.865276973557229e-07,
      "loss": 0.4557,
      "step": 19910
    },
    {
      "epoch": 3.8448176027793863,
      "grad_norm": 99.60895538330078,
      "learning_rate": 7.768770507624011e-07,
      "loss": 0.7402,
      "step": 19920
    },
    {
      "epoch": 3.8467477320980503,
      "grad_norm": 21.448490142822266,
      "learning_rate": 7.672264041690794e-07,
      "loss": 0.5772,
      "step": 19930
    },
    {
      "epoch": 3.848677861416715,
      "grad_norm": 0.6216779351234436,
      "learning_rate": 7.575757575757576e-07,
      "loss": 0.4625,
      "step": 19940
    },
    {
      "epoch": 3.850607990735379,
      "grad_norm": 0.33211004734039307,
      "learning_rate": 7.479251109824359e-07,
      "loss": 0.3425,
      "step": 19950
    },
    {
      "epoch": 3.8525381200540436,
      "grad_norm": 6.455738067626953,
      "learning_rate": 7.382744643891142e-07,
      "loss": 0.5978,
      "step": 19960
    },
    {
      "epoch": 3.854468249372708,
      "grad_norm": 0.697789192199707,
      "learning_rate": 7.286238177957924e-07,
      "loss": 0.778,
      "step": 19970
    },
    {
      "epoch": 3.8563983786913725,
      "grad_norm": 0.6291429996490479,
      "learning_rate": 7.189731712024705e-07,
      "loss": 0.9495,
      "step": 19980
    },
    {
      "epoch": 3.858328508010037,
      "grad_norm": 0.257175087928772,
      "learning_rate": 7.093225246091488e-07,
      "loss": 1.3695,
      "step": 19990
    },
    {
      "epoch": 3.860258637328701,
      "grad_norm": 1.1166203022003174,
      "learning_rate": 6.996718780158271e-07,
      "loss": 0.6829,
      "step": 20000
    },
    {
      "epoch": 3.8621887666473653,
      "grad_norm": 0.20844879746437073,
      "learning_rate": 6.900212314225053e-07,
      "loss": 0.5088,
      "step": 20010
    },
    {
      "epoch": 3.8641188959660298,
      "grad_norm": 1.3482779264450073,
      "learning_rate": 6.803705848291836e-07,
      "loss": 0.9464,
      "step": 20020
    },
    {
      "epoch": 3.866049025284694,
      "grad_norm": 0.8209116458892822,
      "learning_rate": 6.707199382358618e-07,
      "loss": 0.4989,
      "step": 20030
    },
    {
      "epoch": 3.8679791546033586,
      "grad_norm": 0.7492874264717102,
      "learning_rate": 6.610692916425401e-07,
      "loss": 0.3709,
      "step": 20040
    },
    {
      "epoch": 3.8699092839220226,
      "grad_norm": 25.65230369567871,
      "learning_rate": 6.514186450492184e-07,
      "loss": 1.2641,
      "step": 20050
    },
    {
      "epoch": 3.871839413240687,
      "grad_norm": 9.031116485595703,
      "learning_rate": 6.417679984558966e-07,
      "loss": 0.3207,
      "step": 20060
    },
    {
      "epoch": 3.8737695425593515,
      "grad_norm": 1.2912002801895142,
      "learning_rate": 6.321173518625747e-07,
      "loss": 1.2075,
      "step": 20070
    },
    {
      "epoch": 3.875699671878016,
      "grad_norm": 8.286773681640625,
      "learning_rate": 6.224667052692531e-07,
      "loss": 1.051,
      "step": 20080
    },
    {
      "epoch": 3.8776298011966803,
      "grad_norm": 18.326358795166016,
      "learning_rate": 6.128160586759313e-07,
      "loss": 1.0655,
      "step": 20090
    },
    {
      "epoch": 3.8795599305153443,
      "grad_norm": 7.752805233001709,
      "learning_rate": 6.031654120826095e-07,
      "loss": 0.9567,
      "step": 20100
    },
    {
      "epoch": 3.8814900598340087,
      "grad_norm": 9.992464065551758,
      "learning_rate": 5.935147654892878e-07,
      "loss": 0.7322,
      "step": 20110
    },
    {
      "epoch": 3.883420189152673,
      "grad_norm": 18.621585845947266,
      "learning_rate": 5.83864118895966e-07,
      "loss": 1.6037,
      "step": 20120
    },
    {
      "epoch": 3.8853503184713376,
      "grad_norm": 11.846394538879395,
      "learning_rate": 5.742134723026443e-07,
      "loss": 0.8523,
      "step": 20130
    },
    {
      "epoch": 3.887280447790002,
      "grad_norm": 0.21621999144554138,
      "learning_rate": 5.645628257093225e-07,
      "loss": 0.7125,
      "step": 20140
    },
    {
      "epoch": 3.889210577108666,
      "grad_norm": 74.73300170898438,
      "learning_rate": 5.549121791160008e-07,
      "loss": 0.8495,
      "step": 20150
    },
    {
      "epoch": 3.891140706427331,
      "grad_norm": 18.290721893310547,
      "learning_rate": 5.45261532522679e-07,
      "loss": 0.6291,
      "step": 20160
    },
    {
      "epoch": 3.893070835745995,
      "grad_norm": 7.076394557952881,
      "learning_rate": 5.356108859293573e-07,
      "loss": 0.32,
      "step": 20170
    },
    {
      "epoch": 3.8950009650646593,
      "grad_norm": 5.2053117752075195,
      "learning_rate": 5.259602393360355e-07,
      "loss": 0.3348,
      "step": 20180
    },
    {
      "epoch": 3.8969310943833237,
      "grad_norm": 9.720684051513672,
      "learning_rate": 5.163095927427137e-07,
      "loss": 0.4626,
      "step": 20190
    },
    {
      "epoch": 3.898861223701988,
      "grad_norm": 77.83659362792969,
      "learning_rate": 5.06658946149392e-07,
      "loss": 0.8632,
      "step": 20200
    },
    {
      "epoch": 3.9007913530206526,
      "grad_norm": 29.169721603393555,
      "learning_rate": 4.970082995560704e-07,
      "loss": 1.5276,
      "step": 20210
    },
    {
      "epoch": 3.9027214823393166,
      "grad_norm": 3.860912322998047,
      "learning_rate": 4.873576529627486e-07,
      "loss": 0.7405,
      "step": 20220
    },
    {
      "epoch": 3.904651611657981,
      "grad_norm": 4.743465900421143,
      "learning_rate": 4.777070063694269e-07,
      "loss": 0.6322,
      "step": 20230
    },
    {
      "epoch": 3.9065817409766455,
      "grad_norm": 11.255173683166504,
      "learning_rate": 4.68056359776105e-07,
      "loss": 1.1359,
      "step": 20240
    },
    {
      "epoch": 3.90851187029531,
      "grad_norm": 32.95076370239258,
      "learning_rate": 4.5840571318278325e-07,
      "loss": 0.8166,
      "step": 20250
    },
    {
      "epoch": 3.9104419996139743,
      "grad_norm": 25.184818267822266,
      "learning_rate": 4.4875506658946156e-07,
      "loss": 0.8731,
      "step": 20260
    },
    {
      "epoch": 3.9123721289326383,
      "grad_norm": 4.363464832305908,
      "learning_rate": 4.391044199961398e-07,
      "loss": 0.6879,
      "step": 20270
    },
    {
      "epoch": 3.9143022582513027,
      "grad_norm": 2.007783889770508,
      "learning_rate": 4.29453773402818e-07,
      "loss": 0.5211,
      "step": 20280
    },
    {
      "epoch": 3.916232387569967,
      "grad_norm": 1.410831093788147,
      "learning_rate": 4.1980312680949626e-07,
      "loss": 0.6018,
      "step": 20290
    },
    {
      "epoch": 3.9181625168886316,
      "grad_norm": 5.896036624908447,
      "learning_rate": 4.101524802161745e-07,
      "loss": 0.6576,
      "step": 20300
    },
    {
      "epoch": 3.920092646207296,
      "grad_norm": 28.111536026000977,
      "learning_rate": 4.0050183362285276e-07,
      "loss": 0.6757,
      "step": 20310
    },
    {
      "epoch": 3.92202277552596,
      "grad_norm": 0.8595631718635559,
      "learning_rate": 3.9085118702953106e-07,
      "loss": 0.8666,
      "step": 20320
    },
    {
      "epoch": 3.9239529048446244,
      "grad_norm": 0.4340166449546814,
      "learning_rate": 3.812005404362092e-07,
      "loss": 0.8381,
      "step": 20330
    },
    {
      "epoch": 3.925883034163289,
      "grad_norm": 0.27447250485420227,
      "learning_rate": 3.715498938428875e-07,
      "loss": 0.457,
      "step": 20340
    },
    {
      "epoch": 3.9278131634819533,
      "grad_norm": 29.979454040527344,
      "learning_rate": 3.6189924724956576e-07,
      "loss": 1.0612,
      "step": 20350
    },
    {
      "epoch": 3.9297432928006177,
      "grad_norm": 3.6248016357421875,
      "learning_rate": 3.52248600656244e-07,
      "loss": 0.758,
      "step": 20360
    },
    {
      "epoch": 3.9316734221192817,
      "grad_norm": 0.3698723614215851,
      "learning_rate": 3.4259795406292227e-07,
      "loss": 1.3523,
      "step": 20370
    },
    {
      "epoch": 3.9336035514379466,
      "grad_norm": 5.74881649017334,
      "learning_rate": 3.3294730746960046e-07,
      "loss": 0.7507,
      "step": 20380
    },
    {
      "epoch": 3.9355336807566106,
      "grad_norm": 0.8851720690727234,
      "learning_rate": 3.232966608762787e-07,
      "loss": 0.4237,
      "step": 20390
    },
    {
      "epoch": 3.937463810075275,
      "grad_norm": 45.57938003540039,
      "learning_rate": 3.1364601428295697e-07,
      "loss": 0.9841,
      "step": 20400
    },
    {
      "epoch": 3.9393939393939394,
      "grad_norm": 0.22967681288719177,
      "learning_rate": 3.039953676896352e-07,
      "loss": 0.4462,
      "step": 20410
    },
    {
      "epoch": 3.941324068712604,
      "grad_norm": 11.568059921264648,
      "learning_rate": 2.9434472109631347e-07,
      "loss": 1.2102,
      "step": 20420
    },
    {
      "epoch": 3.9432541980312683,
      "grad_norm": 7.95089054107666,
      "learning_rate": 2.846940745029917e-07,
      "loss": 1.2109,
      "step": 20430
    },
    {
      "epoch": 3.9451843273499323,
      "grad_norm": 0.07309683412313461,
      "learning_rate": 2.7504342790966997e-07,
      "loss": 1.0473,
      "step": 20440
    },
    {
      "epoch": 3.9471144566685967,
      "grad_norm": 26.65619468688965,
      "learning_rate": 2.653927813163482e-07,
      "loss": 0.6755,
      "step": 20450
    },
    {
      "epoch": 3.949044585987261,
      "grad_norm": 0.8321160674095154,
      "learning_rate": 2.557421347230265e-07,
      "loss": 1.0789,
      "step": 20460
    },
    {
      "epoch": 3.9509747153059256,
      "grad_norm": 8.641109466552734,
      "learning_rate": 2.460914881297047e-07,
      "loss": 0.3023,
      "step": 20470
    },
    {
      "epoch": 3.95290484462459,
      "grad_norm": 5.257243633270264,
      "learning_rate": 2.3644084153638295e-07,
      "loss": 0.7568,
      "step": 20480
    },
    {
      "epoch": 3.954834973943254,
      "grad_norm": 15.756771087646484,
      "learning_rate": 2.267901949430612e-07,
      "loss": 1.4339,
      "step": 20490
    },
    {
      "epoch": 3.9567651032619184,
      "grad_norm": 1.6200454235076904,
      "learning_rate": 2.1713954834973945e-07,
      "loss": 0.4642,
      "step": 20500
    },
    {
      "epoch": 3.958695232580583,
      "grad_norm": 10.355348587036133,
      "learning_rate": 2.074889017564177e-07,
      "loss": 1.2047,
      "step": 20510
    },
    {
      "epoch": 3.9606253618992473,
      "grad_norm": 16.80997657775879,
      "learning_rate": 1.9783825516309596e-07,
      "loss": 0.6281,
      "step": 20520
    },
    {
      "epoch": 3.9625554912179117,
      "grad_norm": 83.82313537597656,
      "learning_rate": 1.8818760856977418e-07,
      "loss": 0.5577,
      "step": 20530
    },
    {
      "epoch": 3.9644856205365757,
      "grad_norm": 192.70399475097656,
      "learning_rate": 1.7853696197645243e-07,
      "loss": 0.6111,
      "step": 20540
    },
    {
      "epoch": 3.96641574985524,
      "grad_norm": 3.824324369430542,
      "learning_rate": 1.6888631538313066e-07,
      "loss": 0.6241,
      "step": 20550
    },
    {
      "epoch": 3.9683458791739046,
      "grad_norm": 8.759268760681152,
      "learning_rate": 1.5923566878980893e-07,
      "loss": 1.3014,
      "step": 20560
    },
    {
      "epoch": 3.970276008492569,
      "grad_norm": 45.034549713134766,
      "learning_rate": 1.4958502219648719e-07,
      "loss": 0.9416,
      "step": 20570
    },
    {
      "epoch": 3.9722061378112334,
      "grad_norm": 33.63987350463867,
      "learning_rate": 1.399343756031654e-07,
      "loss": 0.9698,
      "step": 20580
    },
    {
      "epoch": 3.9741362671298974,
      "grad_norm": 0.49536842107772827,
      "learning_rate": 1.3028372900984366e-07,
      "loss": 0.3718,
      "step": 20590
    },
    {
      "epoch": 3.9760663964485623,
      "grad_norm": 39.172664642333984,
      "learning_rate": 1.206330824165219e-07,
      "loss": 1.3716,
      "step": 20600
    },
    {
      "epoch": 3.9779965257672263,
      "grad_norm": 10.190185546875,
      "learning_rate": 1.1098243582320016e-07,
      "loss": 1.3494,
      "step": 20610
    },
    {
      "epoch": 3.9799266550858907,
      "grad_norm": 2.0090811252593994,
      "learning_rate": 1.0133178922987841e-07,
      "loss": 1.6267,
      "step": 20620
    },
    {
      "epoch": 3.981856784404555,
      "grad_norm": 16.943710327148438,
      "learning_rate": 9.168114263655665e-08,
      "loss": 0.7283,
      "step": 20630
    },
    {
      "epoch": 3.9837869137232196,
      "grad_norm": 0.19750624895095825,
      "learning_rate": 8.20304960432349e-08,
      "loss": 0.5912,
      "step": 20640
    },
    {
      "epoch": 3.985717043041884,
      "grad_norm": 6.118693828582764,
      "learning_rate": 7.237984944991316e-08,
      "loss": 0.4551,
      "step": 20650
    },
    {
      "epoch": 3.987647172360548,
      "grad_norm": 55.3673210144043,
      "learning_rate": 6.27292028565914e-08,
      "loss": 0.8301,
      "step": 20660
    },
    {
      "epoch": 3.9895773016792124,
      "grad_norm": 0.3739640414714813,
      "learning_rate": 5.307855626326964e-08,
      "loss": 0.2068,
      "step": 20670
    },
    {
      "epoch": 3.991507430997877,
      "grad_norm": 2.4458014965057373,
      "learning_rate": 4.3427909669947896e-08,
      "loss": 1.1584,
      "step": 20680
    },
    {
      "epoch": 3.9934375603165413,
      "grad_norm": 12.74885368347168,
      "learning_rate": 3.3777263076626134e-08,
      "loss": 1.0699,
      "step": 20690
    },
    {
      "epoch": 3.9953676896352057,
      "grad_norm": 9.537328720092773,
      "learning_rate": 2.4126616483304385e-08,
      "loss": 0.5417,
      "step": 20700
    },
    {
      "epoch": 3.9972978189538697,
      "grad_norm": 2.4514973163604736,
      "learning_rate": 1.447596988998263e-08,
      "loss": 0.6046,
      "step": 20710
    },
    {
      "epoch": 3.999227948272534,
      "grad_norm": 0.7446367740631104,
      "learning_rate": 4.825323296660877e-09,
      "loss": 1.0601,
      "step": 20720
    },
    {
      "epoch": 4.0,
      "eval_intent_acc": 0.8576388888888888,
      "eval_loss": 1.317680835723877,
      "eval_runtime": 22.7828,
      "eval_samples_per_second": 50.564,
      "eval_slot_f1": 0.6379383185466836,
      "eval_slot_precision": 0.6138211382113821,
      "eval_slot_recall": 0.6640281442392261,
      "eval_steps_per_second": 25.282,
      "step": 20724
    }
  ],
  "logging_steps": 10,
  "max_steps": 20724,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
